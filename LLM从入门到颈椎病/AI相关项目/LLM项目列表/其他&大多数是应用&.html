<html>
  
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../style.css">
    <base target="_parent">
    <title data-trilium-title>其他&amp;大多数是应用&amp;</title>
  </head>
  
  <body>
    <div class="content">
       <h1 data-trilium-h1>其他&amp;大多数是应用&amp;</h1>

      <div class="ck-content">
        <p>&nbsp;</p>
        <h3>GPT4All Python</h3>
        <blockquote>
          <p>GPT4All 在日常台式机和笔记本电脑上私下运行大型语言模型 (LLM).</p>
        </blockquote>
        <blockquote>
          <p>无需 API 调用或 GPU - 您只需下载应用程序并<a href="https://docs.gpt4all.io/gpt4all_desktop/quickstart.html#quickstart">开始使用即可</a>.</p>
        </blockquote>
        <p><a href="https://github.com/nomic-ai/gpt4all?tab=readme-ov-file">GitHub - nomic-ai/gpt4all: GPT4All: Run Local LLMs on Any Device. Open-source and available for commercial use.</a>
        </p>
        <h3>llama.cpp</h3>
        <p>llama.cpp是一个开源项目,旨在为Meta的LLaMA模型及其他大型语言模型提供高效的推理能力.该项目的主要目标是支持低配置硬件的高性能推理,确保在多种硬件架构上都能达到最佳性能.</p>
        <p><a href="https://mp.weixin.qq.com/s/4ETzVp1a46CJPzSb3U4saQ">理解llama.cpp怎么完成大模型推理的</a>
        </p>
        <p><a href="https://github.com/ggerganov/llama.cpp">GitHub - ggerganov/llama.cpp: LLM inference in C/C++</a>
        </p>
        <h3>轻松实现自托管LLM</h3>
        <p>OpenLLM 允许开发人员使用单个命令将任何开源 LLM(Llama 3.2、Qwen2.5、Phi3 等)或自定义模型 作为与 OpenAI
          兼容的 API运行.它具有内置聊天 UI、最先进的推理后端以及使用 Docker、Kubernetes 和BentoCloud创建企业级云部署的简化工作流程.</p>
        <p><a href="https://github.com/bentoml/OpenLLM">GitHub - bentoml/OpenLLM: Run any open-source LLMs, such as Llama, Mistral, as OpenAI compatible API endpoint in the cloud.</a>
        </p>
        <h3>LLM4Decompile 逆向工程: 使用LLM反编译二进制代码</h3>
        <p>借助llm来实现编译后的二进制还原到源代码</p>
        <p><a href="https://github.com/albertan017/LLM4Decompile">GitHub - albertan017/LLM4Decompile: Reverse Engineering: Decompiling Binary Code with Large Language Models</a>
        </p>
        <h3>Flowise - 轻松构建 LLM 应用程序</h3>
        <p>通过拖拽界面构建定制化的LLM流程</p>
        <p><a href="https://github.com/FlowiseAI/Flowise/blob/main/i18n/README-ZH.md">Flowise/i18n/README-ZH.md at main · FlowiseAI/Flowise</a>
        </p>
        <h3>开放式WebUI界面</h3>
        <blockquote>
          <p>Open WebUI 是一个<a href="https://github.com/open-webui/pipelines">可扩展</a>、功能丰富且用户友好的自托管
            WebUI,旨在完全离线运行.它支持各种 LLM 运行器,包括 Ollama 和 OpenAI 兼容 API.有关更多信息,请务必查看我们的
            <a
            href="https://docs.openwebui.com/">Open WebUI 文档</a>.</p>
        </blockquote>
        <p><a href="https://github.com/open-webui/open-webui">GitHub - open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)</a>
        </p>
        <ul>
          <li>姊妹项目</li>
        </ul>
        <p><a href="https://openwebui.com/">Open WebUI</a>
        </p>
        <h3>Langchain-Chatchat</h3>
        <blockquote>
          <p>基于 ChatGLM 等大语言模型与 Langchain 等应用框架实现,开源、可离线部署的 RAG 与 Agent 应用项目.</p>
        </blockquote>
        <p><a href="https://github.com/chatchat-space/Langchain-Chatchat">GitHub - chatchat-space/Langchain-Chatchat: Langchain-Chatchat（原Langchain-ChatGLM）基于 Langchain 与 ChatGLM, Qwen 与 Llama 等语言模型的 RAG 与 Agent 应用 | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM, Qwen and Llama) RAG and Agent app with langchain</a>
        </p>
        <h3>Cagliostro Forge 合作实验室</h3>
        <blockquote>
          <p>欢迎来到 Stable Diffusion 笔记本的下一个版本！这是<a href="https://colab.research.google.com/github/Linaqruf/sd-notebook-collection/blob/main/cagliostro-colab-ui.ipynb">Cagliostro Colab UI</a>的后继者,现在通过
            <a
            href="https://github.com/lllyasviel/stable-diffusion-webui-forge">lllyasviel/stable-diffusion-webui-forge</a>进行了增强.</p>
        </blockquote>
        <p><a href="https://github.com/cagliostrolab/forge-colab">GitHub - cagliostrolab/forge-colab</a>
        </p>
        <h3>Opik 开源端到端 LLM 开发平台</h3>
        <blockquote>
          <p>Opik 是一个用于评估、测试和监控 LLM 应用程序的开源平台.由<a href="https://www.comet.com/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=what_is_opik_link&amp;utm_campaign=opik">Comet</a>构建.</p>
        </blockquote>
        <p><a href="https://www.comet.com/docs/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=docs_button&amp;utm_campaign=opik">Opik by Comet | Opik Documentation</a>
        </p>
        <p><a href="https://github.com/comet-ml/opik">GitHub - comet-ml/opik: Open-source end-to-end LLM Development Platform</a>
        </p>
        <h3>prime - 大规模分散训练</h3>
        <blockquote>
          <p>prime(之前称为 ZeroBand)是一个通过互联网高效、全球分布式训练 AI 模型的框架.</p>
        </blockquote>
        <blockquote>
          <p>作为去中心化项目,开源了 基础模型,checkpoint模型,后训练模型,数据,Prime训练框架,技术报告等所有资源</p>
        </blockquote>
        <p><a href="https://github.com/PrimeIntellect-ai/prime">GitHub - PrimeIntellect-ai/prime: prime is a framework for efficient, globally distributed training of AI models over the internet.</a>
        </p>
        <h3>RAG-Retrieval @车中草同学</h3>
        <blockquote>
          <p>RAG-Retrieval 提供了全链路的RAG检索模型微调(train)和推理(infer)以及蒸馏(distill)代码.</p>
        </blockquote>
        <ul>
          <li>
            <blockquote>
              <p>对于微调,<strong>支持微调任意开源的RAG检索模型</strong>,包括向量模型(图a,bert-based,llm-based
                embedding)、迟交互式模型(图d,colbert)、重排序模型(图c,bert-based, llm-based reranker).</p>
            </blockquote>
          </li>
          <li>
            <blockquote>
              <p>对于推理,RAG-Retrieval专注于重排序(reranker),开发了一个轻量级的python库<a href="https://pypi.org/project/rag-retrieval/">rag-retrieval</a>,<strong>提供统一的方式调用任意不同的RAG排序模型</strong>.</p>
            </blockquote>
          </li>
          <li>
            <blockquote>
              <p>对于蒸馏,支持将基于LLM的reranker模型蒸馏到基于bert的reranker模型中.</p>
            </blockquote>
          </li>
        </ul>
        <p><a href="https://github.com/NLPJCL/RAG-Retrieval/blob/master/README_zh.md">RAG-Retrieval/README_zh.md at master · NLPJCL/RAG-Retrieval</a>
        </p>
        <p><a href="https://www.notion.so/RAG-Retrieval-Roadmap-c817257e3e8a484b8850cac40a3fcf88">交流群地址</a>
        </p>
        <p>微信: liruanyi</p>
        <h3>复现 O1模型</h3>
        <blockquote>
          <p>该项目的核心开发团队主要由上海交通大学 GAIR 研究组大三、大四本科生和大一博士生组成,并得到了纽约大学、穆罕默德·本·扎耶德人工智能大学等大型语言模型领域顶尖研究科学家的指导.</p>
        </blockquote>
        <p><a href="https://github.com/GAIR-NLP/O1-Journey#about-the-team%E3%80%82">GitHub - GAIR-NLP/O1-Journey: O1 Replication Journey: A Strategic Progress Report – Part I</a>
        </p>
        <h3>ai-llm-comparison</h3>
        <p>您可以在这个网站上比较每个 AI 模型、检查价格并找到最好的模型！</p>
        <p><a href="https://countless.dev/">Countless.dev | AI Model Comparison</a>
        </p>
        <p><a href="https://github.com/Ahmet-Dedeler/ai-llm-comparison">GitHub - Ahmet-Dedeler/ai-llm-comparison: A website where you can compare every AI Model ✨</a>
        </p>
        <h2>ChatALL</h2>
        <blockquote>
          <p>基于大型语言模型(LLMs)的 AI 机器人非常神奇.然而,它们的行为可能是随机的,不同的机器人在不同的任务上表现也有差异.如果你想获得最佳体验,不要一个一个尝试.ChatALL(中文名:
            齐叨)可以把一条指令同时发给多个 AI,帮助您发现最好的回答.你需要做的只是<a href="https://github.com/sunner/ChatALL/releases">下载、安装</a>和提问.</p>
        </blockquote>
        <p><a href="https://github.com/ai-shifu/ChatALL">GitHub - ai-shifu/ChatALL: Concurrently chat with ChatGPT, Bing Chat, Bard, Alpaca, Vicuna, Claude, ChatGLM, MOSS, 讯飞星火, 文心一言 and more, discover the best answers</a>
        </p>
        <h3>从llama2看LLM的基本知识</h3>
        <p>从开源项目fork的</p>
        <p><a href="https://github.com/Dynamicwang/llama">GitHub - Dynamicwang/llama: Inference code for Llama models</a>
        </p>
        <h3>AIBox</h3>
        <p>中国人民大学的团队</p>
        <p>有《大模型综述》-赵鑫</p>
        <p>Yulan-Mini 模型 （取自人民大学校花玉兰花）</p>
        <p>（DUT 的校花也是玉兰）</p>
        <p><a href="http://aibox.ruc.edu.cn/">欢迎访问AI Box小组</a>
        </p>
        <h3>LLMSurvey</h3>
        <blockquote>
          <p>🔥 大型语言模型（LLM）已经<s>NLP 社区</s>  <s>AI 社区</s>  <strong>席卷全球</strong>。这是一份关于大型语言模型的论文精选列表，尤其是与
            ChatGPT 相关的论文。它还包含 LLM 培训框架、部署 LLM 的工具、有关 LLM 的课程和教程以及所有公开可用的 LLM 检查点和
            API。</p>
        </blockquote>
        <p><a href="https://www.yuque.com/attachments/yuque/0/2024/pdf/42982692/1735104215690-16cdffdf-3f15-430a-89f1-b859d8c19095.pdf">LLMBook.pdf</a>
        </p>
        <p><a href="https://github.com/RUCAIBox/LLMSurvey">GitHub - RUCAIBox/LLMSurvey: The official GitHub page for the survey paper “A Survey of Large Language Models”.</a>
        </p>
        <h3>STILL: Slow Thinking with LLMs 类O1系统</h3>
        <p><a href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">GitHub - RUCAIBox/Slow_Thinking_with_LLMs: A series of technical report on Slow Thinking with LLM</a>
        </p>
        <p><a href="https://arxiv.org/pdf/2412.09413">https://arxiv.org/pdf/2412.09413</a>
        </p>
        <h3>Agenta</h3>
        <p><strong>Agenta</strong> 是一个开源平台，帮助开发者和产品团队构建基于大语言模型（LLM）的强大 AI 应用程序。它提供了用于提示管理和评估的所有工具。</p>
        <p>使用 <strong>Agenta</strong>，你可以：</p>
        <ul>
          <li><strong>快速实验和比较</strong> 在任何 LLM 工作流上的提示（例如链式提示、增强生成（RAG）、LLM 代理等）</li>
          <li><strong>快速创建测试集和黄金数据集</strong> 以进行评估</li>
          <li>使用 <strong>预设或自定义评估器</strong> 评估你的应用程序</li>
          <li>通过 <strong>人工反馈</strong> 标注和 A/B 测试你的应用程序</li>
          <li>与产品团队合作进行提示工程和评估</li>
          <li><strong>一键部署</strong> 你的应用程序，通过 UI、CLI 或 GitHub 工作流</li>
        </ul>
        <p><strong>Agenta</strong> 旨在通过加快实验的速度，提升 LLM 应用程序开发周期的速度。</p>
        <p><strong>Agenta 与其他平台的区别</strong>：</p>
        <ul>
          <li><strong>适用于任何 LLM 应用工作流</strong>
            <br>Agenta 使得在任何 LLM 应用架构（如链式提示、RAG 或 LLM 代理）上进行提示工程和评估成为可能。它与 Langchain、LlamaIndex
            等任何框架兼容，也支持 OpenAI、Cohere 或本地模型等任何模型提供商。</li>
        </ul>
        <p><a href="https://chatgpt.com/c/676cb8fd-99cc-8005-9b11-c9558efd0f9c#">点击此处查看如何使用你自己的自定义应用与 Agenta</a> 和
          <a
          href="https://chatgpt.com/c/676cb8fd-99cc-8005-9b11-c9558efd0f9c#">了解更多 Agenta 如何工作的内容</a>。</p>
        <ul>
          <li><strong>促进开发者与产品团队的协作</strong>
            <br>Agenta 使非开发者能够在任何自定义 LLM 应用程序的配置上进行迭代、评估、标注、A/B 测试，并部署，所有这些都可以通过用户界面完成。</li>
        </ul>
        <p>只需在应用代码中添加几行代码，你就可以创建一个提示游乐场，允许非开发者实验应用程序的提示，并使用 Agenta 中的所有工具。</p>
        <p><a href="https://github.com/Agenta-AI/agenta">GitHub - Agenta-AI/agenta: The open-source LLMOps platform: prompt playground, prompt management, LLM evaluation, and LLM Observability all in one place.</a>
        </p>
        <h3>Anything-LLM</h3>
        <p>👉 适用于桌面(Mac、Windows和Linux)的AnythingLLM！</p>
        <p>这是一个全栈应用程序,可以将任何文档、资源(如网址链接、音频、视频)或内容片段转换为上下文,以便任何大语言模型(LLM)在聊天期间作为参考使用.此应用程序允许您选择使用哪个LLM或向量数据库,同时支持多用户管理并设置不同权限.</p>
        <p><a href="https://github.com/Mintplex-Labs/anything-llm">GitHub - Mintplex-Labs/anything-llm: The all-in-one Desktop &amp; Docker AI application with built-in RAG, AI agents, and more.</a>
        </p>
        <h3>llm</h3>
        <blockquote>
          <p>一个 CLI 实用程序和 Python 库，用于与大型语言模型交互，通过远程 API 和可以在您自己的机器上安装和运行的模型。</p>
        </blockquote>
        <p><a href="https://github.com/simonw/llm">GitHub - simonw/llm: Access large language models from the command-line</a>
        </p>
        <h3>llmware</h3>
        <p><code>llmware</code>提供了构建基于 LLM 的应用程序（例如 RAG、Agents）的统一框架，使用小型、专门的模型，这些模型可以私下部署，安全地与企业知识源集成，并可以经济高效地调整和适应任何业务流程。</p>
        <p><code>llmware</code>有两个主要组成部分：</p>
        <ol>
          <li><strong>RAG Pipeline——</strong>将知识源连接到生成式 AI 模型的整个生命周期的集成组件；以及</li>
          <li><strong>50 多个小型、专门的模型</strong>针对企业流程自动化中的关键任务进行了微调，包括基于事实的问答、分类、总结和提取。</li>
        </ol>
        <blockquote>
          <p>通过将这两个组件结合在一起，并集成领先的开源模型和底层技术，<code>llmware</code>提供了一套全面的工具来快速构建基于知识的企业
            LLM 应用程序。</p>
        </blockquote>
        <blockquote>
          <p>我们的大多数示例无需 GPU 服务器即可运行 - 立即在您的笔记本电脑上开始运行。</p>
        </blockquote>
        <p><a href="https://github.com/llmware-ai/llmware">GitHub - llmware-ai/llmware: Unified framework for building enterprise RAG pipelines with small, specialized models</a>
        </p>
        <h2><a href="https://github.com/eugeneyan/open-llms"><strong>open-llms</strong></a></h2>
        <p>一个列出可商用开源模型的 List</p>
        <p><a href="https://github.com/eugeneyan/open-llms">GitHub - eugeneyan/open-llms: 📋 A list of open LLMs available for commercial use.</a>
        </p>
        <h2><a href="https://github.com/mlc-ai/web-llm"><strong>web-ll</strong></a>m</h2>
        <blockquote>
          <p>WebLLM 是一款高性能的浏览器内 LLM 推理引擎，它通过硬件加速将语言模型推理直接引入 Web 浏览器。所有内容都在浏览器内运行，无需服务器支持，并通过
            WebGPU 加速。</p>
        </blockquote>
        <blockquote>
          <p>WebLLM与<a href="https://platform.openai.com/docs/api-reference/chat"><strong>OpenAI API</strong></a><strong>完全兼容。</strong> 也就是说，您可以在本地对任何开源模型使用相同的
            OpenAI API ，其功能包括流式传输、JSON 模式、函数调用（WIP）等。</p>
        </blockquote>
        <p><a href="https://github.com/mlc-ai/web-llm">GitHub - mlc-ai/web-llm: High-performance In-browser LLM Inference Engine</a>
        </p>
        <h3>OpenLLM</h3>
        <blockquote>
          <p>OpenLLM 允许开发人员使用单个命令将任何开源 LLM（Llama 3.2、Qwen2.5、Phi3 等<a href="https://github.com/bentoml/OpenLLM#supported-models">）</a>或自定义模型
            作为与 <strong>OpenAI 兼容的 API运行。它具有</strong><a href="https://github.com/bentoml/OpenLLM#chat-ui">内置聊天 UI</a>、最先进的推理后端以及使用
            Docker、Kubernetes 和<a href="https://github.com/bentoml/OpenLLM#deploy-to-bentocloud">BentoCloud</a>创建企业级云部署的简化工作流程。</p>
        </blockquote>
        <p><a href="https://github.com/bentoml/OpenLLM">GitHub - bentoml/OpenLLM: Run any open-source LLMs, such as Llama, Mistral, as OpenAI compatible API endpoint in the cloud.</a>
        </p>
        <h3>SillyTavern</h3>
        <blockquote>
          <p>移动设备界面友好，多种人工智能服务或模型支持（KoboldAI/CPP, Horde, NovelAI, Ooba, OpenAI, OpenRouter,
            Claude, Scale），类似Galgame老魔模式，Horde SD，文本系统语音生成，世界信息（Lorebooks），可定制的界面，自动翻译，以及比你所需要的更多的提示。附带扩展服务，支持文本绘画生成和语音生成并基于支持数据库的聊天信息汇总。</p>
        </blockquote>
        <blockquote>
          <p>基于 TavernAI 1.2.8 的分叉版本</p>
        </blockquote>
        <p><a href="https://github.com/SillyTavern/SillyTavern/blob/release/.github/readme-zh_cn.md">SillyTavern/.github/readme-zh_cn.md at release · SillyTavern/SillyTavern</a>
        </p>
        <h3>LLMDataHub</h3>
        <p>用于 LLM 训练的优质数据集</p>
        <p><a href="https://github.com/Zjh-819/LLMDataHub">GitHub - Zjh-819/LLMDataHub: A quick guide (especially) for trending instruction finetuning datasets</a>
        </p>
        <h3>llm-examples</h3>
        <p>Streamlit + LLM 示例应用程序</p>
        <p><a href="https://github.com/streamlit/llm-examples">GitHub - streamlit/llm-examples: Streamlit LLM app examples for getting started</a>
        </p>
        <h3>llm-foundry</h3>
        <p><a href="https://github.com/mosaicml/composer">此存储库包含使用Composer</a>和
          <a
          href="https://forms.mosaicml.com/demo?utm_source=github.com&amp;utm_medium=referral&amp;utm_campaign=llm-foundry">MosaicML 平台</a>训练、微调、评估和部署 LLM 进行推理的代码。此代码库易于使用、高效_且_灵活，可快速试验最新技术。</p>
        <p><a href="https://github.com/mosaicml/llm-foundry">GitHub - mosaicml/llm-foundry: LLM training code for Databricks foundation models</a>
        </p>
        <h3>Awesome-LLMOps</h3>
        <blockquote>
          <p>一份为开发人员精心挑选的最佳 LLMOps 工具列表。</p>
        </blockquote>
        <p><a href="https://github.com/tensorchord/Awesome-LLMOps">GitHub - tensorchord/Awesome-LLMOps: An awesome &amp; curated list of best LLMOps tools for developers</a>
        </p>
        <h3>chatgpt-mirai-qq-bot</h3>
        <blockquote>
          <p>一键部署！真正的 AI 聊天机器人！支持ChatGPT、文心一言、讯飞星火、Bing、Bard、ChatGLM、POE，多账号</p>
        </blockquote>
        <p><a href="https://github.com/lss233/chatgpt-mirai-qq-bot">GitHub - lss233/chatgpt-mirai-qq-bot: 🚀 一键部署！真正的 AI 聊天机器人！支持ChatGPT、文心一言、讯飞星火、Bing、Bard、ChatGLM、POE，多账号，人设调教，虚拟女仆、图片渲染、语音发送 | 支持 QQ、Telegram、Discord、微信 等平台</a>
        </p>
        <h3>khoj</h3>
        <p><a href="https://khoj.dev/">Khoj</a>是一款个人 AI 应用，可帮助您扩展能力。它可从设备上的个人 AI
          顺利扩展到云级企业 AI。</p>
        <ul>
          <li>
            <blockquote>
              <p>与任何本地或在线 LLM 聊天（例如 llama3、qwen、gemma、mistral、gpt、claude、gemini）。</p>
            </blockquote>
          </li>
          <li>
            <blockquote>
              <p>从互联网和您的文档（包括图像、pdf、markdown、org-mode、word、notion 文件）获取答案。</p>
            </blockquote>
          </li>
          <li>
            <blockquote>
              <p>从您的浏览器、Obsidian、Emacs、桌面、电话或 Whatsapp 访问它。</p>
            </blockquote>
          </li>
          <li>
            <blockquote>
              <p>创建具有自定义知识、角色、聊天模型和工具的代理来承担任何角色。</p>
            </blockquote>
          </li>
          <li>
            <blockquote>
              <p>自动完成重复性研究。将个人新闻通讯和智能通知发送到您的收件箱。</p>
            </blockquote>
          </li>
          <li>
            <blockquote>
              <p>使用我们先进的语义搜索快速轻松地找到相关文档。</p>
            </blockquote>
          </li>
          <li>
            <blockquote>
              <p>生成图像、大声说话、播放您的信息。</p>
            </blockquote>
          </li>
          <li>
            <blockquote>
              <p>Khoj 是开源的，可自行托管。始终如此。</p>
            </blockquote>
          </li>
        </ul>
        <p><a href="https://github.com/khoj-ai/khoj">GitHub - khoj-ai/khoj: Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.</a>
        </p>
      </div>
    </div>
  </body>

</html>