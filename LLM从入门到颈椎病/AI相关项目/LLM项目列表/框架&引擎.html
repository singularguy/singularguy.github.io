<html>
  
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../style.css">
    <base target="_parent">
    <title data-trilium-title>框架&amp;引擎</title>
  </head>
  
  <body>
    <div class="content">
       <h1 data-trilium-h1>框架&amp;引擎</h1>

      <div class="ck-content">
        <p>&nbsp;</p>
        <h2>vLLM 🌟</h2>
        <blockquote>
          <p>vLLM是一个高吞吐量和内存高效的大型语言模型(LLMs)推理和服务引擎</p>
        </blockquote>
        <p><a href="https://mp.weixin.qq.com/s/KM-Z6FtVfaySewRTmvEc6w">vLLM CPU和GPU模式署和推理 Qwen2 等大语言模型详细教程</a>
        </p>
        <h2>SGLang</h2>
        <blockquote>
          <p>SGLang 是一个针对大型语言模型和视觉语言模型的快速服务框架。它通过共同设计后端运行时和前端语言，让您与模型的交互更快、更可控。核心功能包括：</p>
        </blockquote>
        <ul>
          <li><strong>快速后端运行时</strong>：通过 RadixAttention 提供高效的服务，用于前缀缓存、前跳约束解码、无开销 CPU
            调度程序、连续批处理、令牌注意（分页注意）、张量并行、FlashInfer 内核、分块预填充和量化（FP8/INT4/AWQ/GPTQ）。</li>
          <li><strong>灵活的前端语言</strong>：为编程 LLM 应用程序提供直观的界面，包括链式生成调用、高级提示、控制流、多模式输入、并行性和外部交互。</li>
          <li><strong>广泛的模型支持</strong>：支持广泛的生成模型（Llama、Gemma、Mistral、QWen、DeepSeek、LLaVA
            等）、嵌入模型（e5-mistral、gte、mcdse）和奖励模型（Skywork），并且易于扩展以集成新模型。</li>
          <li><strong>活跃的社区</strong>：SGLang 是开源的，并得到行业采用的活跃社区的支持。</li>
          <li>使用文档</li>
        </ul>
        <p><a href="https://sgl-project.github.io/">SGLang Documentation — SGLang</a>
        </p>
        <ul>
          <li>博客</li>
        </ul>
        <p><a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/">SGLang v0.4: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs | LMSYS Org</a>
        </p>
        <p><a href="https://github.com/sgl-project/sglang">GitHub - sgl-project/sglang: SGLang is a fast serving framework for large language models and vision language models.</a>
        </p>
      </div>
    </div>
  </body>

</html>