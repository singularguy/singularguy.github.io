# LN&和&BN&有什么区别
**BN (Batch Normalization)** 和 **LN (Layer Normalization)** 的核心区别在于 **归一化的维度不同**,这直接影响了它们的计算方式和适用场景.本文将 从 **计算维度**、**具体例子** 和 **直观理解** 三个方面重新解释.

### 1\. **核心区别：归一化的维度**

*   **BN**：在 **批次维度(batch dimension)** 上进行归一化.
*   **LN**：在 **特征维度(feature dimension)** 上进行归一化.

#### 数学公式对比

假设输入数据为$X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$,其中：

*   ( N )：批次大小
*   ( C )：通道数
*   ( H, W )：特征图的高度和宽度

**BN 的计算**：  
对每个通道 ( c ),在批次维度 ( N ) 上计算均值和方差：

$\\mu\_c = \\frac{1}{N \\cdot H \\cdot W} \\sum\_{n=1}^{N} \\sum\_{h=1}^{H} \\sum\_{w=1}^{W} X\_{n, c, h, w}$ $\\sigma\_c^2 = \\frac{1}{N \\cdot H \\cdot W} \\sum\_{n=1}^{N} \\sum\_{h=1}^{H} \\sum\_{w=1}^{W} (X\_{n, c, h, w} - \\mu\_c)^2 $

$\\hat{X}_{n,c,h,w} = \\frac{X_{n,c,h,w} - \\mu\_{c}}{\\sqrt{\\sigma\_{c}^{2} + \\epsilon}} $

**LN 的计算**：  
对每个样本 ( n ),在特征维度 ( C, H, W ) 上计算均值和方差：

$\\mu\_n = \\frac{1}{C \\cdot H \\cdot W} \\sum\_{c=1}^{C} \\sum\_{h=1}^{H} \\sum\_{w=1}^{W} X\_{n, c, h, w} $

$\\sigma\_n^2 = \\frac{1}{C \\cdot H \\cdot W} \\sum\_{c=1}^{C} \\sum\_{h=1}^{H} \\sum\_{w=1}^{W} (X\_{n, c, h, w} - \\mu\_n)^2 $

$\\hat{X}_{n,c,h,w} = \\frac{X_{n,c,h,w} - \\mu\_{n}}{\\sqrt{\\sigma\_{n}^{2} + \\epsilon}} $

**关键区别**：

*   BN 的均值和方差是 **跨样本** 计算的(对每个通道,跨批次).
*   LN 的均值和方差是 **跨特征** 计算的(对每个样本,跨通道和空间维度).

### 2\. **具体例子**

假设输入数据是一个批次图像,维度为$ \[N, C, H, W\] = \[2, 3, 2, 2\]$

*   ( N = 2 )：2 张图像
*   ( C = 3 )：3 个通道(如 RGB)
*   ( H = 2, W = 2 )：图像高度和宽度为 2

#### 输入数据

```python
图像 1 (n=1):
通道 1 (c=1): [[1, 2], [3, 4]]
通道 2 (c=2): [[5, 6], [7, 8]]
通道 3 (c=3): [[9, 10], [11, 12]]

图像 2 (n=2):
通道 1 (c=1): [[13, 14], [15, 16]]
通道 2 (c=2): [[17, 18], [19, 20]]
通道 3 (c=3): [[21, 22], [23, 24]]
```

#### BN 的计算

对每个通道 ( c ),跨批次计算均值和方差.例如,对通道 1：

$\\mu\_1 = \\frac{1 + 2 + 3 + 4 + 13 + 14 + 15 + 16}{8} = 8.5$

$\\sigma\_1^2 = \\frac{(1-8.5)^2 + (2-8.5)^2 + \\dots + (16-8.5)^2}{8} $

#### LN 的计算

对每个样本 ( n ),跨特征计算均值和方差.例如,对图像 1：

$\\mu\_1 = \\frac{1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 + 12}{12} = 6.5 $

$\\sigma\_1^2 = \\frac{(1-6.5)^2 + (2-6.5)^2 + \\dots + (12-6.5)^2}{12} $

**关键区别**：

*   BN 是对 **每个通道** 跨 **批次** 归一化.
*   LN 是对 **每个样本** 跨 **特征** 归一化.

### 对于样本是单列多维度向量的情况

假设输入数据是一个批次样本，维度为$\[N, D\]$，其中：

+$N$：批次大小 +$D$：特征维度

例如，输入数据为：

样本1 (n=1):$\[x\_{1,1}, x\_{1,2}, x\_{1,3}\] = \[1, 2, 3\]$

样本2 (n=2):$\[x\_{2,1}, x\_{2,2}, x\_{2,3}\] = \[4, 5, 6\]$

### BN 的计算

对于每个特征维度$d$，在批次维度$N$上计算均值和方差。

例如，对于特征维度1：

$\\mu\_1 = \\frac{1 + 4}{2} = 2.5$

$\\sigma\_1^2 = \\frac{(1-2.5)^2 + (4-2.5)^2}{2} = 2.25$

同理，可以计算特征维度2和3的均值和方差。

然后，对每个特征维度进行归一化：

$\\hat{x}\_{1,1} = \\frac{1 - 2.5}{\\sqrt{2.25}} = -1$

$\\hat{x}\_{1,2} = \\frac{2 - 3.5}{\\sqrt{2.25}} = -1$

$\\hat{x}\_{1,3} = \\frac{3 - 4.5}{\\sqrt{2.25}} = -1$

同理，可以计算样本2的归一化值。

### LN 的计算

对于每个样本$n$，在特征维度$D$上计算均值和方差。

例如，对于样本1：

$\\mu\_1 = \\frac{1 + 2 + 3}{3} = 2$

$\\sigma\_1^2 = \\frac{(1-2)^2 + (2-2)^2 + (3-2)^2}{3} = 0.6667$

同理，可以计算样本2的均值和方差。

然后，对每个样本的所有特征进行归一化：

$\\hat{x}\_{1,1} = \\frac{1 - 2}{\\sqrt{0.6667}} = -1.2247$

$\\hat{x}\_{1,2} = \\frac{2 - 2}{\\sqrt{0.6667}} = 0$

$\\hat{x}\_{1,3} = \\frac{3 - 2}{\\sqrt{0.6667}} = 1.2247$

同理，可以计算样本2的归一化值。

### 3\. **直观理解**

*   **BN**：假设输入是图像,BN 会对每个颜色通道(如 R、G、B)在所有图像上归一化.例如,对红色通道,BN 会计算所有图像的红色像素的均值和方差.
*   **LN**：假设输入是图像,LN 会对每张图像的所有像素(包括所有通道)归一化.例如,对一张图像,LN 会计算它的所有像素(R、G、B)的均值和方差.

### 4\. **适用场景**

*   **BN**：适合 **大批量数据**(如计算机视觉任务),但对小批量数据效果较差.
*   **LN**：适合 **小批量数据** 或 **序列数据**(如 NLP 任务),因为它不依赖批次大小.

### 5\. **总结**

| 特性  | BN (Batch Normalization) | LN (Layer Normalization) |
| --- | --- | --- |
| 归一化维度 | 对每个通道,跨批次归一化 | 对每个样本,跨特征归一化 |
| 计算方式 | 基于批次统计量 ($\\mu\_c, \\sigma\_c^2$) | 基于样本统计量 ($\\mu\_n , \\sigma\_c^2$) |
| 对批次的依赖 | 依赖批次大小,小批次时效果差 | 不依赖批次大小,适合小批次或单样本 |
| 适用场景 | 大批量数据(如计算机视觉) | 小批量或序列数据(如 NLP) |