# 手撕Transformer模型：分步骤数学实例解析
> **作者: 学姐带你玩AI**
> 
> **原文: https://mp.weixin.qq.com/s/DOQfm8tHoXKKSEvz9c8Nvg**

手撕Transformer：逐步带你了解Transformer是如何工作的。

**Stp1——定义数据集**

构建ChatGPT所依赖的数据集规模高达570GB。但是本文不用如此规模的data，所以我们将采用一个规模极小的数据集，通过可视化的手段直观地进行数值分析。

![](手撕Transformer模型：分步骤数学实例解析_640.)

虽然我们的数据集已经经过了清洗处理，但在实际场景中，比如创建像ChatGPT这样的模型时，清洗高达570GB的数据集是非常困难的任务，需要投入大量的时间和精力。

**stp2——确定词汇表大小**

词汇表的大小，即我们数据集中不同单词的总数量，是通过一个特定公式来确定的。在这个公式中，N代表数据集中所有单词的总数。

![](1_手撕Transformer模型：分步骤数学实例解析_640.)

为了找到N，我们需要将数据集拆分为单个单词。

![](3_手撕Transformer模型：分步骤数学实例解析_640.)

在得到N之后，我们执行集合操作以去除重复项，然后我们可以计算唯一单词的数量以确定词汇表的大小。

![](10_手撕Transformer模型：分步骤数学实例解析_640.)

因此，词汇表的大小是23，因为我们的数据集中有23个唯一单词。

**Stp3——编码**

我们需要为每个唯一单词分配一个唯一数字。

![](6_手撕Transformer模型：分步骤数学实例解析_640.)

由于我们将单个标记视为一个单词并为其分配了一个数字，而ChatGPT使用以下公式将单词的一部分视为一个标记：1个标记=0.75个单词。在对整个数据集进行编码后，接下来是选择输入并开始使用transformer架构进行处理。

**Stp4——计算嵌入**

现在让我们从语料库中选择一句话，这句话将在transformer架构中进行处理。

![](4_手撕Transformer模型：分步骤数学实例解析_640.)

我们已经选择了输入，并且需要为其找到一个嵌入向量。原始论文为每个输入单词使用了一个512维的嵌入向量。

![](5_手撕Transformer模型：分步骤数学实例解析_640.)

在我们的案例中，为了可视化计算过程，我们需要使用较小维度的嵌入向量。因此，我们将为嵌入向量使用6维的维度。

![](13_手撕Transformer模型：分步骤数学实例解析_640.)

嵌入向量的这些值介于0和1之间，并且在开始时是随机填充的。随着transformer开始理解单词之间的含义，它们将会被更新。

**Stp5——计算位置嵌入**

我们需要为输入数据确定位置嵌入。位置嵌入的计算方式依赖于两个公式，具体使用哪个公式取决于每个单词嵌入向量中第i个值所处的位置。

![](2_手撕Transformer模型：分步骤数学实例解析_640.)

显而易见的是，我们的输入句子是“when you play the game of thrones”，起始单词是“when”，起始索引（POS）值为0，维度（d）为6。对于i从0到5，我们计算输入句子的第一个单词的位置嵌入。

![](11_手撕Transformer模型：分步骤数学实例解析_640.)

同样的，我们可以计算输入句子中所有单词的位置嵌入。

![](12_手撕Transformer模型：分步骤数学实例解析_640.)

**Stp6——连接位置嵌入和词嵌入**

在计算完位置嵌入后，我们需要将词嵌入和位置嵌入相加。

![](26_手撕Transformer模型：分步骤数学实例解析_640.)

将这两个矩阵（词嵌入矩阵和位置嵌入矩阵）结合后得到的矩阵将作为编码器部分的输入。

**Stp7——多头注意力**

多头注意力由多个单头注意力组成。我们需要组合多少个单头取决于我们自己。例如，Meta的LLaMA大型语言模型在编码器架构中使用了32个单头。下面是一个单头注意力的示意图。

![](7_手撕Transformer模型：分步骤数学实例解析_640.)

有三个输入：查询（query）、键（key）和值（value）。这些矩阵是通过将我们之前计算的词嵌入和位置嵌入矩阵相加后得到的同一矩阵的转置，乘以不同的权重矩阵集而得到的。假设为了计算查询矩阵，权重矩阵集的行数必须与转置矩阵的列数相同，而权重矩阵的列数可以是任意的；例如，我们假设权重矩阵有4列。权重矩阵中的值在0到1之间随机选择，当transformer开始学习这些单词的含义时，这些值将会被更新。

![](24_手撕Transformer模型：分步骤数学实例解析_640.)

同样，我们可以使用相同的程序来计算键和值矩阵，但是权重矩阵中的值对于这两者来说必须是不同的。

![](33_手撕Transformer模型：分步骤数学实例解析_640.)

因此，在矩阵相乘后，我们得到了查询、键和值的结果：

![](23_手撕Transformer模型：分步骤数学实例解析_640.)

既然我们已经有了这三个矩阵，让我们开始逐步计算单头注意力。

![](21_手撕Transformer模型：分步骤数学实例解析_640.)

为了缩放结果矩阵，我们必须重用我们嵌入向量的维度，即6。

![](41_手撕Transformer模型：分步骤数学实例解析_640.)

接下来的掩码步骤是可选的，我们可以不进行计算。掩码就像是告诉模型只关注某个点之前发生的事情，在弄清楚句子中不同单词的重要性时不要偷窥未来。它有助于模型以逐步的方式理解事物，而不会通过提前查看来作弊。所以现在我们将在我们的缩放结果矩阵上应用softmax操作。

![](27_手撕Transformer模型：分步骤数学实例解析_640.)

执行最后的乘法步骤，从单头注意力中获得结果矩阵。

![](28_手撕Transformer模型：分步骤数学实例解析_640.)

我们已经计算了单头注意力，而如前所述，多头注意力由多个单头注意力组成。下面是一个可视化的示例：

![](18_手撕Transformer模型：分步骤数学实例解析_640.)

每个单头注意力都有三个输入：查询、键和值，并且每个输入都有一组不同的权重。一旦所有单头注意力输出了它们的结果矩阵，它们都会被连接起来，并且最终连接起来的矩阵将再次通过乘以一组用随机值初始化的权重矩阵来进行线性变换，这些权重在transformer开始训练后将会得到更新。在我们的案例中，我们考虑的是单头注意力，但如果我们使用的是多头注意力，那么它的外观就会像这样。

![](31_手撕Transformer模型：分步骤数学实例解析_640.)

无论是单头注意力还是多头注意力，结果矩阵都需要再次通过乘以一组权重矩阵来进行线性变换。

![](29_手撕Transformer模型：分步骤数学实例解析_640.)

确保线性权重矩阵的列数必须等于我们之前计算的矩阵（词嵌入+位置嵌入）的列数，因为在下一步中，我们将把结果归一化矩阵与（词嵌入+位置嵌入）矩阵相加。

![](17_手撕Transformer模型：分步骤数学实例解析_640.)

既然我们已经计算出了多头注意力的结果矩阵，接下来我们将进行相加和归一化的步骤。

**Stp8——相加和归一化**

一旦我们从多头注意力中获得了结果矩阵，我们就必须将其添加到我们的原始矩阵中。

![](30_手撕Transformer模型：分步骤数学实例解析_640.)

为了对上述矩阵进行归一化，我们需要计算每一行的均值和标准差。

![](25_手撕Transformer模型：分步骤数学实例解析_640.)

我们将矩阵中的每个值减去对应行的均值，然后除以对应行的标准差。

![](32_手撕Transformer模型：分步骤数学实例解析_640.)

添加一个小的误差值可以防止分母为零，并避免使整个项变为无穷大。

**Stp9——前馈网络**

矩阵归一化后，将通过前馈网络进行处理。我们将使用一个非常基本的网络，它只包含一个线性层和一个ReLU激活函数层。它的外观如下所示：

![](14_手撕Transformer模型：分步骤数学实例解析_640.)

首先，我们需要计算线性层，方法是将我们最后计算得到的矩阵与一组随机权重矩阵相乘（这些权重在transformer开始学习时会得到更新），并将结果矩阵加上一个同样包含随机值的偏置矩阵。

![](43_手撕Transformer模型：分步骤数学实例解析_640.)

计算完线性层后，我们需要将其通过ReLU层，并使用ReLU层的公式。

![](42_手撕Transformer模型：分步骤数学实例解析_640.)

**Stp10——再次相加和归一化**

一旦我们从前馈网络中获得结果矩阵，我们必须将其与之前相加和归一化步骤中得到的矩阵相加，然后使用行均值和标准差对其进行归一化。

![](22_手撕Transformer模型：分步骤数学实例解析_640.)

这个相加和归一化步骤的输出矩阵将作为解码器部分中存在的多头注意力机制之一的查询和键矩阵，这可以通过从相加和归一化步骤追溯到解码器部分来轻松理解。

**Stp11——解码器部分**

到现在为止，我们已经计算了编码器部分，从编码数据集到将矩阵通过前馈网络传递，我们所执行的所有步骤都是独一无二的。这意味着我们之前没有计算过它们。但是从现在开始，即将进行的所有步骤，即transformer的剩余架构（解码器部分），都将涉及类似类型的矩阵乘法。看看我们的transformer架构。我们到目前为止已经覆盖了哪些内容，以及我们还需要覆盖哪些内容：

![](36_手撕Transformer模型：分步骤数学实例解析_640.)

我们不会计算整个解码器，因为解码器的大部分内容都包含与我们在编码器中已经完成的类似计算。详细计算解码器只会因为重复步骤而使博客变得冗长。相反，我们只需要关注解码器的输入和输出的计算。在训练时，解码器有两个输入。一个来自编码器，其中最后一个相加和归一化层的输出矩阵作为解码器部分中第二个多头注意力层的查询和键。下面是它的可视化（由batool haider提供）：

![](15_手撕Transformer模型：分步骤数学实例解析_640.)

而值矩阵则来自解码器中的第一个相加和归一化步骤之后。解码器的第二个输入是预测文本。如果你还记得，我们对编码器的输入是“当你玩《权力的游戏》时”，那么对解码器的输入就是预测文本，在我们的例子中就是“你赢或你死”。但是，预测的输入文本需要遵循一个标准的标记包装，让transformer知道从哪里开始和结束。

![](37_手撕Transformer模型：分步骤数学实例解析_640.)

其中，和是两个新引入的标记。此外，解码器一次接受一个标记作为输入。这意味着将作为输入，而你必须为它预测文本。

![](38_手撕Transformer模型：分步骤数学实例解析_640.)

正如我们已经知道的，这些嵌入充满了随机值，这些值将在后续的训练过程中得到更新。按照我们之前在编码器部分计算的方式，计算解码器中剩余的部分。

![](35_手撕Transformer模型：分步骤数学实例解析_640.)

在深入探讨之前，我们需要通过一个简单的数学例子来理解什么是掩码多头注意力。

**Stp12——理解掩码多头注意力**

在Transformer中，掩码多头注意力就像模型用来聚焦句子不同部分的聚光灯。它的特别之处在于，它不允许模型通过查看句子中后面的单词来作弊。这有助于模型逐步理解和生成句子，这对于诸如对话或将单词翻译成另一种语言等任务非常重要。假设我们有以下输入矩阵，其中每一行代表序列中的一个位置，每一列代表一个特征：

![](8_手撕Transformer模型：分步骤数学实例解析_640.)

具有两个头的掩码多头注意力的组成部分：

*   线性投影（查询、键、值）：假设每个头的线性投影为：头1：Wq1、Wk1、Wv1 和 头2：Wq2、Wk2、Wv2。
    
*   计算注意力得分：对于每个头，使用查询和键的点积来计算注意力得分，并应用掩码以防止关注到未来的位置。
    
*   应用Softmax：应用Softmax函数以获得注意力权重。
    
*   加权求和（值）：将注意力权重与值相乘，得到每个头的加权和。
    
*   拼接和线性变换：将两个头的输出拼接起来，并应用线性变换。现在，让我们做一个简化的计算：
    

假设两个条件：

Wq1 = Wk1 = Wv1 = Wq2 = Wk2 = Wv2 = I，即单位矩阵。

Q = K = V = Input Matrix

![](9_手撕Transformer模型：分步骤数学实例解析_640.)

拼接步骤将两个注意力头的输出组合成一组信息。想象一下，你有两个朋友，他们分别就一个问题给你建议。拼接他们的建议意味着把两条建议放在一起，这样你就能更全面地了解他们的意见。在transformer模型的上下文中，这一步有助于从多个角度捕捉输入数据的不同方面，从而生成模型可以用于进一步处理的更丰富表示。

**Stp13——计算预测单词**

解码器最后一个相加和归一化块的输出矩阵必须包含与输入矩阵相同数量的行，而列的数量可以是任意的。在这里，我们假设有6列。

![](19_手撕Transformer模型：分步骤数学实例解析_640.)

解码器的最后一个相加和归一化块产生的矩阵必须被展平，以便与一个线性层匹配，从而找到数据集中（语料库中）每个唯一单词的预测概率。

![](39_手撕Transformer模型：分步骤数学实例解析_640.)

这个展平后的层将被传递给一个线性层，以计算我们数据集中每个唯一单词的logits（分数）。

![](34_手撕Transformer模型：分步骤数学实例解析_640.)

一旦我们获得了logits，就可以使用softmax函数对它们进行归一化，并找到包含最高概率的单词。

![](16_手撕Transformer模型：分步骤数学实例解析_640.)

所以根据我们的计算，解码器预测出的单词是“you”。

![](40_手撕Transformer模型：分步骤数学实例解析_640.)

这个预测的单词“you”将被视为解码器的输入单词，并且这个过程将持续进行，直到预测出标记。

**Important Points：**

上面的例子非常简单，因为它没有涉及到轮次（epochs）或任何其他只能使用像Python这样的编程语言才能可视化的重要参数。它只展示了训练过程，而评估或测试过程无法通过这种矩阵方法直观地看到。掩码多头注意力可以用来防止Transformer查看未来，从而有助于避免模型过拟合。

在本文中，我利用矩阵方法为你阐述了Transformer模型在数学层面的基本运作机制。并且，我们深入探讨了位置编码、softmax函数、前馈神经网络，以及最为关键的多头注意力机制等核心概念。