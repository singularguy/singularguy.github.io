# [公式要更新]大模型推理加速技术的学习路线是什么&
> **作者: 猛猿**
> 
>   
> **链接:** [**https://www.zhihu.com/question/591646269/answer/3309904882**](https://www.zhihu.com/question/591646269/answer/3309904882)

大家好哇,好久没有更新了,今天想来讲讲[**Flash Attention(V1)**](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=Flash+Attention%EF%BC%88V1%EF%BC%89&zhida_source=entity).

不知道你有没有和我一样的感受,第一次读Flash Attention的论文时,感觉头懵懵的: \*\*它不仅涉及了硬件和cuda的知识,还涉及到很多计算逻辑上的trick.\*\*我的痛点是不能在头脑中具象化整个流程,就更不要提对细节的推导了.

所以这篇文章我读了很久,也写了很久(一个月),最终决定按照如下方式对Flash Attention进行介绍:

*   本文一到三部分,介绍相关硬件知识及Flash Attention诞生背景.
*   本文四到五部分,通过**图解形式**介绍forward/backward中的分块计算过程.**所有的符号、公式都会给出详细的说明和推导过程**.我在阅读中发现论文的一些推导不太符合直觉(or写得可能不太对),所以这里我在遵从论文符号表达的基础上,部分内容按自己的理解重新顺了一遍.
*   本文第六到第八部分,量化介绍Flash attention在性能上的改进,包括计算量、显存和IO复杂度.

**【大模型计算加速系列】**

[**猛猿: 图解大模型计算加速系列: FlashAttention V1,从硬件到计算逻辑**](https://zhuanlan.zhihu.com/p/669926191)

[**猛猿: 图解大模型计算加速系列: Flash Attention V2,从原理到并行计算**](https://zhuanlan.zhihu.com/p/691067658)

[**猛猿: 图解Mixtral 8 \* 7b推理优化原理与源码实现**](https://zhuanlan.zhihu.com/p/691066049)

[**猛猿: 从啥也不会到CUDA GEMM优化**](https://zhuanlan.zhihu.com/p/703256080)

[**猛猿: 图解大模型计算加速系列之: vLLM核心技术PagedAttention原理**](https://zhuanlan.zhihu.com/p/691038809)

[**猛猿: 图解大模型计算加速系列: vLLM源码解析1,整体架构**](https://zhuanlan.zhihu.com/p/691045737)

[**猛猿: 图解大模型计算加速系列: vLLM源码解析2,调度器策略(Scheduler)**](https://zhuanlan.zhihu.com/p/692540949)

[**猛猿: 图解大模型计算加速系列: vLLM源码解析3,块管理器BlockManager(上篇)**](https://zhuanlan.zhihu.com/p/700780161)

[**猛猿: 图解大模型计算加速系列: vLLM源码解析3,Prefix Caching**](https://zhuanlan.zhihu.com/p/707228704)**(BlockManager下篇)**

[**猛猿: 图解大模型计算加速系列: 分离式推理架构1,从DistServe谈起**](https://zhuanlan.zhihu.com/p/706761664)

[**猛猿: 图解大模型计算加速系列: 分离式推理架构2,模糊分离与合并边界的chunked-prefills**](https://zhuanlan.zhihu.com/p/710165390)

**【历史文章汇总】**

[**猛猿: 【必看】历史技术文章导航**](https://zhuanlan.zhihu.com/p/654910335)

* * *

一、Flash attention在做一件什么事
------------------------

我们知道,对于Transformer类的模型,假设其输入序列长度为 NN ,那么其计算复杂度和消耗的存储空间都为 O(N2)O(N^{2}) .也就是说,随着输入序列的变长,将给计算和存储带来极大的压力.

因此,我们迫切需要一种办法,能解决Transformer模型的 O(N2)O(N^{2}) 复杂度问题.如果能降到 O(N)O(N) ,那是最好的,即使做不到,逼近 O(N)O(N) 那也是可以的.所以,Flash Attention就作为一种行之有效的解决方案出现了.

Flash Attention在做的事情,其实都包含在它的命名中了(**Fast and Memory Efficient Exact Attention with IO-Awareness**),我们逐一来看:

**(1)Fast(with IO-Awareness),计算快**.在Flash Attention之前,也出现过一些加速Transformer计算的方法,这些方法的着眼点是“减少计算量FLOPs”,例如用一个[稀疏attention](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=%E7%A8%80%E7%96%8Fattention&zhida_source=entity)做近似计算._但是Flash attention就不一样了,它并没有减少总的计算量,因为它发现: 计算慢的卡点不在运算能力,而是在读写速度上.**所以它通过降低对显存(HBM)的访问次数来加快整体运算速度,这种方法又被称为**O-Awareness_\*.在后文中,我们会详细来看Flash Attention是如何通过**分块计算(tiling)和核函数融合(**[**kernel fusion**](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=kernel+fusion&zhida_source=entity))来降低对显存的访问.

**(2)Memory Efficicent,节省显存**.在标准attention场景中,forward时我们会计算并保存N\*N大小的注意力矩阵；在backward时我们又会读取它做梯度计算,这就给硬件造成了 O(N2)O(N^{2}) 的存储压力.在Flash Attention中,则巧妙避开了这点,使得存储压力降至 O(N)O(N) .在后文中我们会详细看这个trick.

\*\*(3)Exact Attention,精准注意力.\*\*在(1)中我们说过,之前的办法会采用类似于“稀疏attention”的方法做近似.这样虽然能减少计算量,但算出来的结果并不完全等同于标准attention下的结果.但是Flash Attention却做到了完全等同于标准attention的实现方式,这也是后文我们讲述的要点.

二、计算限制与内存限制
-----------

在第一部分中我们提过,**Flash Attention一个很重要的改进点是: 由于它发现Transformer的计算瓶颈不在运算能力,而在读写速度上.因此它着手降低了对显存数据的访问次数,这才把整体计算效率提了上来.所以现在我们要问了: 它是怎么知道卡点在读写速度上的？**

为了解答这个问题,我们先来看几个重要概念:

*   π\\pi : **硬件算力上限**.指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数.单位是 FLOPS or FLOP/s.
*   β\\beta : **硬件带宽上限**.指的是一个计算平台倾尽全力每秒所能完成的内存交换量.单位是Byte/s.
*   πt\\pi\_{t} : **某个算法所需的总运算量**,单位是FLOPs.下标$t$表示total.
*   βt\\beta\_{t} : \*\*某个算法所需的总数据读取存储量,\*\*单位是Byte.下标$t$表示total.

这里再强调一下对FLOPS和FLOPs的解释:

*   FLOPS: 等同于FLOP/s,表示Floating Point Operations Per Second,即每秒执行的浮点数操作次数,用于衡量硬件计算性能.
*   FLOPs: 表示Floating Point Operations,表示某个算法的总计算量(即总浮点运算次数),用于衡量一个算法的复杂度.

**我们知道,在执行运算的过程中,时间不仅花在计算本身上,也花在数据读取存储上**,所以现在我们定义

*   TcalT\_{cal} : 对某个算法而言,计算所耗费的时间,单位为s,下标cal表示calculate.其满足 Tcal=πtπT\_{cal} =\\frac{\\pi\_{t}}{\\pi}
*   TloadT\_{load} : 对某个算法而言,读取存储数据所耗费的时间,单位为s.其满足 Tload=βtβT\_{load} = \\frac{\\beta\_{t}}{\\beta}

**我们知道,数据在读取的同时,可以计算；在计算的同时也可以读取**,所以我们有:

+$T$: 对某个算法而言,完成整个计算所耗费的总时间,单位为s.其满足 T=max(Tcal,Tload)T = max(T\_{cal}, T\_{load})

**也就是说,最终一个算法运行的总时间,取决于计算时间和数据读取时间中的最大值.**

### 2.1 计算限制

当 Tcal>TloadT\_{cal} > T\_{load} 时,算法运行的瓶颈在计算上,我们称这种情况为**计算限制(math-bound)**.此时我们有: πtπ>βtβ\\frac{\\pi\_{t}}{\\pi} > \\frac{\\beta\_{t}}{\\beta} ,即 πtβt>πβ\\frac{\\pi\_{t}}{\\beta\_{t}} > \\frac{\\pi}{\\beta}

### 2.2 内存限制

当 Tcal<TloadT\_{cal} < T\_{load} 时,算法运行的瓶颈在数据读取上,我们称这种情况为**内存限制(memory-bound)**.此时我们有 πtπ<βtβ\\frac{\\pi\_{t}}{\\pi} <\\frac{\\beta\_{t}}{\\beta} ,即 πtβt<πβ\\frac{\\pi\_{t}}{\\beta\_{t}} <\\frac{\\pi}{\\beta}

我们称 πtβt\\frac{\\pi\_{t}}{\\beta\_{t}} 为算法的**计算强度(Operational Intensity)**

### 2.3 Attention计算中的计算与内存限制

本节内容参考自: [回旋托马斯x: FlashAttention:加速计算,节省显存, IO感知的精确注意力](https://zhuanlan.zhihu.com/p/639228219)

有了2.1和2.2的前置知识,**现在我们可以来分析影响Transformer计算效率的因素到底是什么了.我们把目光聚焦到attention矩阵的计算上,其计算复杂度为$O(N^{2})$,是Transformer计算耗时的大头.**

假设我们现在采用的硬件为A100-40GB SXM,同时采用混合精度训练(可理解为训练过程中的计算和存储都是**fp16**形式的,一个元素占用2byte)

πβ=312∗10121555∗109=201FLOPs/Bytes\\frac{\\pi}{\\beta} = \\frac{312 \* 10^{12}}{1555 \* 10^{9}} = 201 FLOPs/Bytes

假定我们现在有矩阵 Q,K∈RN∗dQ, K \\in \\mathbb{R}^{N\*d} ,其中 NN 为序列长度, dd 为embedding dim.现在我们要计算 S=QKTS = QK^{T} ,则有(**对FLOPs要怎么算不了解的朋友,可以跳到6.1节进行阅读**):

πtβt=2N2d2Nd+2Nd+2N2=N2d2Nd+N2\\frac{\\pi\_{t}}{\\beta\_{t}} = \\frac{2N^{2}d}{2Nd + 2Nd + 2N^{2}} = \\frac{N^{2}d}{2Nd + N^{2}}

不同 N,dN, d 取值下的受限类型如下:

![]([公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

根据这个表格,我们可以来做下总结:

*   **计算限制(math-bound)**: 大矩阵乘法(N和d都非常大)、通道数很大的卷积运算.相对而言,**读得快,算得慢**.
*   **内存限制(memory-bound)**: 逐点运算操作.例如: 激活函数、dropout、mask、softmax、BN和LN.相对而言,**算得快,读得慢.**

**所以,我们第一部分中所说,“Transformer计算受限于数据读取”也不是绝对的,要综合硬件本身和模型大小来综合判断**.但从表中的结果我们可知,memory-bound的情况还是普遍存在的,所以Flash attention的改进思想在很多场景下依然适用.

在Flash attention中,**计算注意力矩阵时的softmax计算就受到了内存限制,这也是**[**flash attention**](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=flash+attention&zhida_source=entity)**的重点优化对象**,我们会在下文来详细看这一点.

### 2.4 [roof-line模型](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=roof-line%E6%A8%A1%E5%9E%8B&zhida_source=entity)

其实到2.3为止,我们对计算限制和内存限制的概念已经知道得很清楚了.在这一节中,我们更系统来做一个总结.

一个算法运行的效率是离不开硬件本身的.**我们往往想知道: 对于一个运算量为 πt\*\*\*\*\\pi\_{t}\*\*** ,数据读取存储量为 **β**t\*\*\*\*\\beta\_{t}\*\*\*\* 的算法,它在算力上限为 **π**\\pi\*\*\*\* ,带宽上限为 **β**\\beta\*\*\*\* 的硬件上,能达到的最大性能 **P**P\*\*\*\* (Attanable Performance)是多少？\*\*

这里最大性能 PP 指的是当前算法实际运行在硬件上时,每秒最多能达到的计算次数,单位是`FLOP/s`.

[**Roof-line模型**](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=Roof-line%E6%A8%A1%E5%9E%8B&zhida_source=entity)就是为了解答这一问题而提出的,它能直观帮我们看到算法在硬件上能跑得多快,模型见下图.

![](2_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

如图,横坐标 II 表示计算强度,满足 I=πtβtI = \\frac{\\pi\_{t}}{\\beta\_{t}} ；纵坐标 PP 表示算法运行在硬件上的性能.**算法的运行性能不会超过硬件本身的计算上限**,所以 PP 的最大值取到 π\\pi .根据我们之前的分析,当 I>πβI > \\frac{\\pi}{\\beta} 时,存在计算限制；当 I<πβI <\\frac{\\pi}{\\beta} 时,存在内存限制.

三、GPU上的存储与计算
------------

由于Flash attention的优化核心是减少数据读取的时间,而数据读取这块又离不开数据在硬件上的流转过程,所以这里我们简单介绍一些GPU上的存储与计算内容,作为Flash attention的背景知识.

### 3.1 GPU的存储分类

![](1_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

上图是Flash attention论文所绘制的硬件不同的存储类型、存储大小和带宽.一般来说,GPU上的存储分类,可以按照是否在芯片上分为**片上内存(on chip)和片下内存(off chip)**.

*   **片上内存**: 主要用于缓存(cache)及少量特殊存储单元(例如texture),其特点是\*\*“存储空间小,但带宽大”\*\*.对应到上图中,**SRAM**就属于片上内存,它的存储空间只有20MB,但是带宽可以达到19TB/s.
*   **片下内存**: 主要用于全局存储(global memory),即我们常说的**显存**,其特点是\*\*“存储空间大,但带宽小”\*\*,对应到上图中,**HBM就属于片下内存(也就是显存)**,它的存储空间有40GB(A100 40GB),但带宽相比于SRAM就小得多,只有1.5TB/s.

当硬件开始计算时,会先从显存(HBM)中把数据加载到片上(SRAM),在片上进行计算,然后将计算结果再写回显存中.**那么这个“片上”具体长什么样,它又是怎么计算数据的呢？**

### 3.2 GPU是如何做计算的

![](14_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

如图,负责GPU计算的一个核心组件叫**SM(**[**Streaming Multiprocessors**](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=Streaming+Multiprocessors&zhida_source=entity)**,流式多处理器),可以将其理解成GPU的计算单元,一个SM又可以由若干个SMP(SM Partition)组成**,例如图中就由4个SMP组成.SM就好比CPU中的一个核,但不同的是一个CPU核一般运行一个线程,但是一个SM却可以运行多个轻量级线程(由[Warp Scheduler](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=Warp+Scheduler&zhida_source=entity)控制,一个Warp Scheduler会抓一束线程(32个)放入cuda core(图中绿色小块)中进行计算).

现在,我们将GPU的计算核心SM及不同层级GPU存储结构综合起来,绘制一张简化图:

![](3_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

ref: [https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s33322/](https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s33322/)

*   **HBM2**: 即是我们的显存.
*   **L1缓存/**[**shared memory**](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=shared+memory&zhida_source=entity): 每个SM都有自己的L1缓存,用于存储SM内的数据,被SM内所有的cuda cores共享.SM间不能互相访问彼此的L1.NV Volta架构后(Volta架构前只有Kepler做过合并),L1和shared memory合并,目的是为了进一步降低延迟.合并过后,用户能写代码直接控制的依然是shared memory,同时可控制从L1中分配多少存储给shared memory.**Flash attention中SRAM指的就是L1 cache/shared memory.**
*   **L2缓存**: 所有SM共享L2缓存.L2缓存不直接由用户代码控制.L1/L2缓存的带宽都要比显存的带宽要大,也就是读写速度更快,但是它们的存储量更小.

**现在我们再理一遍GPU的计算流程: 将数据从显存(HBM)加载至on-chip的SRAM中,然后由SM读取并进行计算.计算结果再通过SRAM返回给显存.**

我们知道显存的带宽相比SRAM要小的多,读一次数据是很费时的,但是SRAM存储又太小,装不下太多数据.所以**我们就以SRAM的存储为上限,尽量保证每次加载数据都把SRAM给打满,节省数据读取时间**.

### 3.3 kernel融合

前面说过,由于从显存读一次数据是耗时的,因此**在SRAM存储容许的情况下,能合并的计算我们尽量合并在一起,避免重复从显存读取数据**.

举例来说,我现在要做计算A和计算B.在老方法里,我做完A后得到一个中间结果,写回显存,然后再从显存中把这个结果加载到SRAM,做计算B.但是现在我发现SRAM完全有能力存下我的中间结果,那我就可以把A和B放在一起做了,这样就能节省很多读取时间,我们管这样的操作叫**kernel融合**.

由于篇幅限制,我们无法详细解释**kernel**这个概念,\*\*在这里大家可以粗犷地理解成是“函数”,它包含对线程结构(grid-block-thread)的定义,以及结构中具体计算逻辑的定义.\*\*理解到这一层已不妨碍我们对flash attention的解读了,想要更近一步了解的朋友,推荐阅读这篇([小小将: CUDA编程入门极简教程](https://zhuanlan.zhihu.com/p/34587739%EF%BC%89%E6%96%87%E7%AB%A0%E3%80%82))

\*\*kernel融合和尽可能利用起SRAM,以减少数据读取时间,都是flash attention的重要优化点.\*\*在后文对伪代码的解读中我们会看到,分块之后flash attention将矩阵乘法、mask、softmax、dropout操作合并成一个kernel,做到了只读一次和只写回一次,节省了数据读取时间.

好！目前为止所有的背景知识我们都介绍完了,现在我们直入主题,看看flash attention到底是怎么巧妙解决memory-bound问题.

四、Forward运作流程
-------------

在后文相关的讲解中,我们遵循以下步骤:

**(1)先看Flash Attention做分块计算的整体流程.**

**(2)再看分块的计算细节.**

**(3)最后看Flash Attention是如何通过分块计算控制I/O,进而解决memory-bound的问题,提升整体运算速度.**

### 4.1 标准attention计算

这个大家应该都很熟悉了,假设一共有 NN 个token,每个token向量的维度为 dd ,则一个标准的attention计算如下图:

![](4_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

其中, S=QKT,P=softmax(S)S = QK^{T}, {P} = softmax(S) .在GPT类的模型中,还需要对 P{P} 做mask处理.**为了表达方便,诸如mask、dropout之类的操作,我们都忽略掉,下文也是同理**.

### 4.2 标准Safe softmax

这里我们需要额外强调 P=softmax(S){P} = softmax(S) 这一步.正常来说,假设 SS 中某一行向量为 \[x1,x2,...,xd\]\[x\_{1}, x\_{2}, ..., x\_{d}\] ,该行向量中的某一个元素为 xix\_{i} ,则对 SS 做softmax后,有:

softmax(xi)=exi∑j=1dexjsoftmax(x\_{i}) = \\frac{e^{x\_{i}}}{\\sum\_{j=1}^{d}e^{x\_{j}}}

**而如果 xix\_{i} 过大,那么在计算softmax的过程中,就可能出现数据上溢的情况**.为了解决这个问题,我们可以采用**safe softmax**方法:

m(x)=maxi⁡xim(x) = \\mathop{max}\\limits\_{i}x\_{i}

softmax(xi)=exi−m(x)∑j=1dexj−m(x)softmax(x\_{i}) = \\frac{e^{x\_{i}-m(x)}}{\\sum\_{j=1}^{d}e^{x\_{j}-m(x)}}

下图展示了safe softmax的过程,**这里 P~\*\*\*\*,P\\widetilde{P}, P\*\*** 分别表示做归一化前和做归一化后的结果.\*\*大家记住图中 m,lm, l 表达的含义,在后面的分块(Tiling)计算中,我们会用到这两个概念:

![](5_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

### 4.3 分块计算整体流程(Tiling)

我们知道Flash Attention的核心优化技术是采用了分块计算(Tiling),那么它是如何分块的？分块后的计算方式和不分块的计算方式又有哪些不同之处呢？

**我们先来了解分块计算的整体流程(帮助大家理解数据块是怎么流转的),然后我们再针对其中的细节做一一讲解.**

![](20_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

(1)首先,将 QQ 矩阵切为 TrT\_{r} 块(block),每块的长度为 BrB\_{r} .用 QiQ\_{i} 来表示切完后的某块矩阵,则$Q\_{i}$的维度为 (Br,d)(B\_{r}, d) .不难理解, QiQ\_{i} 中存储着某 BrB\_{r} 个token的query信息.

(2)然后,将 KTK^{T} 矩阵切为 TcT\_{c} 块,每块的长度为 BcB\_{c} .用 KjTK^{T}_{j} 表示切完后的某块矩阵,则 KjTK^{T}_{j} 的维度为 (d,Bc)(d, B\_{c}) .易知 KjTK^{T}_{j} 中存储着某 BcB_{c} 个token的key信息.

(3)同样,将 VV 矩阵也切为 TcT\_{c} 块,每块长度为 BcB\_{c} .用$V\_{j}$表示切完后的某块矩阵,则 VjV\_{j} 的维度为 (Bc,d)(B\_{c}, d) .易知 VjV\_{j} 中存储着某 BcB\_{c} 个token的value信息.

(4)理解了上面的定义后,我们就可以开始做**分块的attention计算**了.以上图为例:

*   **计算初始attention分数**: Sij=Qi∗KjT=(Br,d)∗(d,Bc)=(Br,Bc)S\_{ij} = Q\_{i} \* K^{T}_{j} = (B_{r}, d) \* (d, B\_{c}) = (B\_{r}, B\_{c}) ,图中的 SijS\_{ij} 表示前 BrB\_{r} 个token和前 BcB\_{c} 个token间的原始相关性分数.
*   **Safe softmax + mask + dropout**: 对 SijS\_{ij} 做safe softmax、mask和dropout操作,得到 P~ij\\widetilde{P}\_{ij} .\*\*你可能会有疑惑: 前面不是说, **P**~**i**j\*\*\*\*\\widetilde{P}_{ij}\*\*\*\* 是归一化前的结果, **P**i**j**P_{ij}\*\*\*\* 是归一化后的结果吗？那么这里是不是应该用 **P**i**j**P\_{ij}\*\*\*\* 呢？\*\*这里确实只用算到 P~ij\\widetilde{P}\_{ij} ,在后文对分块计算细节的讲解中,我们会详细说这点.目前为止,大家不用太纠结符号,只用大体知道 PP 代表的含义即可.
*   **计算output**: Oij=P~ij∗Vj=(Br,Bc)∗(Bc,d)=(Br,d)O\_{ij} = \\widetilde{P}_{ij} \* V_{j} = (B\_{r}, B\_{c}) \* (B\_{c}, d) = (B\_{r}, d) ,即可得到输出结果 OijO\_{ij} .\*_细心的你肯定又发现了,这个等式不太对劲,这个 **O**i**j**O\_{ij}_\*\*\* 不太对劲.\*\*想一想,在正常情况下,前 BrB\_{r} 个token过attention后的输出结果,应该是它和所有token都做注意力计算后的输出结果.可是这里, OijO\_{ij} 却只是前 BrB\_{r} 个token和前 BcB\_{c} 个token的结果.虽然 OijO\_{ij} 的shape对了,但其中的内容却不是我们最终想要的.所以,关于 OO 的计算,也是我们需要关注的细节,我们同样放在后文详说.

**在计算这些分块时,GPU是可以做并行计算的,这也提升了计算效率.**

好！现在你已经知道了单块的计算方式,现在让我们把整个流程流转起来把.在上图中,我们注明了 jj 是外循环, ii 是内循环,这个意思就是说,\*_对于每个 **j**j_\*\*\* ,我们都把所有的 **i**i\*\*\*\* 遍历一遍,得到相关结果.在论文里,又称为K,V是外循环,Q是内循环.\*\*写成代码就是:

```text-plain
# ---------------------
# Tc: K和V的分块数
# Tr: Q的分块数量
# ---------------------
for 1 <= j <= Tc:
    for 1 <= i <= Tr:
        do....
```

如果你还有疑惑,那么下面两张图可以更直观地解答你的疑惑.

j=0j = 0 ,遍历 ii :

![](7_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

j=1j = 1 ,遍历 ii :

![](6_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

\*\*【**⚠️**特别提醒】: 正如上文所说,这里的 **O**O\*\*\*\* 还需要经过一定的处理,才能和不分块场景下的 **O**O\*\*\*\* 完全等价.这里我们将每一块的 **O**O\*\*\*\* 单独画出,是为了帮助大家更好理解分块计算的整体流程,不代表它是最终的输出结果.\*\*

好！到这一步为止,我们已经掌握了使用Tiling计算attention的整体框架.但我们依然有很多细节问题没有解决:

*   **分块后,要如何正确计算attention score？(即**S,PS, P**的计算方法)**
*   **分块后,要如何正确计算输出**OO\*\*？\*\*
*   **分块后,是如何实现优化I/O,解决memory-bound的问题的？**

### 4.4 分块计算中的safe softmax

回顾之前绘制的标准safe softmax流程图,我们知道 m,lm, l 都是针对**完整的一行**做rowmax、rowsum后的结果,那么在分块场景下,会变成什么样呢？

![](17_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

以上图红圈内的数据为例,在标准场景下,我们是对红圈内的每一行做rowmax、rowsum后得到 P~\\widetilde{P} 的.

现在切换到分块场景,我们分别算出了 S00S\_{00} 和 S01S\_{01} ,然后我们再对它们分别做rowmax、rowsum,是不是也能得到和标准场景下一模一样的结果呢？

答案当然是否定的.举个简单的例子,\*\*标准场景下的 **m**(**x**)**m(x)** 是每行的全局最大值,可是分块后如果你也这么算,它就变成了局部最大值了.\*\*很明显,它不等同于标准场景下的结果.

所以,**Flash Attention的作者们,在这里使用了一种巧妙的计算方式.**

(1)我们假设标准场景下, SS 矩阵某一行的向量为 x=\[x1,x2,...,xd\]x = \[x\_{1}, x\_{2}, ..., x\_{d}\] ,因为分块的原因,它被我们切成了两部分 x=\[x(1),x(2)\]x = \[x^{(1)}, x^{(2)}\] .

(2)我们定义:

*   m(x)m(x) : 标准场景下,该行的全局最大值
*   m(x(1))m(x^{(1)}) : 分块1的全局最大值
*   m(x(2))m(x^{(2)}) : 分块2的全局最大值

那么易知: m(x)=m(\[x(1),x(2)\])=max(m(x(1)),m(x(2)))m(x) = m(\[x^{(1)}, x^{(2)}\]) = max(m(x^{(1)}), m(x^{(2)}))

(3)我们定义:

*   f(x)f(x) : 标准场景下, exp(x−m(x))exp(x - m(x)) 的结果
*   f(x(1))f(x^{(1)}) : 分块场景下, exp(x(1)−m(x(1)))exp(x^{(1)} - m(x^{(1)})) 的结果
*   f(x(2))f(x^{(2)}) : 分块场景下, exp(x(2)−m(x(2)))exp(x^{(2)} - m(x^{(2)})) 的结果

那么易知: f(x)=\[e(m(x(1))−m(x))f(x(1)),e(m(x(2))−m(x))f(x(2))\]f(x) = \[e^{(m(x^{(1)}) - m(x))}f(x^{(1)}), e^{(m(x^{(2)}) - m(x))}f(x^{(2)})\] .这个很好理解,详细的证明过程就不写了.

(4)我们定义:

*   l(x)l(x) : 标准场景下, rowsum\[f(x)\]rowsum\[f(x)\] 的结果
*   l(x(1))l(x^{(1)}) : 分块场景下, rowsum\[f(x(1))\]rowsum\[f(x^{(1)})\] 的结果
*   l(x(2))l(x^{(2)}) : 分块场景下, rowsum\[f(x(2))\]rowsum\[f(x^{(2)})\] 的结果

那么由(3)易知: l(x)=em(x(1))−m(x)l(x(1))+em(x(2))−m(x)l(x(2))l(x) = e^{m(x^{(1)}) - m(x)}l(x^{(1)}) + e^{m(x^{(2)}) - m(x)}l(x^{(2)})

(5)现在,我们就可以用分块计算的结果,来表示标准场景下safe softmax的结果了:

softmax(x)=f(x)l(x)=\[e(m(x(1))−m(x))f(x(1)),e(m(x(2))−m(x))f(x(2))\]em(x(1))−m(x)l(x(1))+em(x(2))−m(x)l(x(2))softmax(x) = \\frac{f(x)}{l(x)} = \\frac{\[e^{(m(x^{(1)}) - m(x))}f(x^{(1)}), e^{(m(x^{(2)}) - m(x))}f(x^{(2)})\]}{e^{m(x^{(1)}) - m(x)}l(x^{(1)}) + e^{m(x^{(2)}) - m(x)}l(x^{(2)})}

我们配合上面的图例和flash attention论文中的伪代码,再来进一步理解一下分块计算safe softmax的(1)～(5)步骤.

**这里我们需注意: 由于safe softmax是针对矩阵整行的计算,即相当于固定内圈 ii\*\*\*\* ,移动外圈 jj\*\*\*\* 的结果,所以在接下来的介绍中,我们都以这样的视角进行介绍.**

![](8_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

我们用 S00S\_{00} (图中浅绿色方块)替换掉(1)～(5)步骤中的 x(1)x^{(1)} ,用 S01S\_{01} (图中深绿色方块)替换掉 x(2)x^{(2)} .我们关注点在伪代码部分的5-11行.

由于伪代码中的表达符太多,容易阻碍大家的理解,因此我们先明确各个数学符号表达的含义:

*   SijS\_{ij} : 对应在我们的例子里,就是 S00S\_{00} 和 S01S\_{01} ,即 QiKjTQ\_{i}K^{T}\_{j} 的结果
*   m~ij\\widetilde{m}_{ij} : 对于当前分块 SijS_{ij} 来说,每行的局部最大值.相当于前面步骤(2)中对 m(x(1)),m(x(2))m(x^{(1)}), m(x^{(2)}) 的定义.
*   P~ij\\widetilde{P}\_{ij} : 分块场景下,各块的P矩阵(归一化前)结果.相当于步骤(3)中对 f(x(1)),f(x(2))f(x^{(1)}),f(x^{(2)}) 的定义.
*   lij~\\widetilde{l\_{ij}} : 分块场景下,rowsum的结果.相当于步骤(4)中对 l(x(1)),l(x(2))l(x^{(1)}),l(x^{(2)}) 的定义.
*   mm : 标准场景下,对 SS 矩阵而言,每行的最大值,这是全局最大值( mm 首次定义在伪代码第2行),相当于前面步骤(2)中对 m(x)m(x) 的定义
*   ll : 标准场景下,全局rowsum的结果($l$首次定义在伪代码第2行),相当于前面步骤(4)中对 l(x)l(x) 的定义.
*   mim\_{i} : 表示 max(m~i0,m~i1,...,m~i(j−1))max(\\widetilde{m}_{i0}, \\widetilde{m}_{i1}, ..., \\widetilde{m}_{i(j-1)}) .如果当前分块是 SijS_{ij} ,则 mim\_{i} 表示固定 ii 时,前 j−1j-1 个分块中的局部最大值.容易推知,当固定 ii ,遍历完 jj 后, mim\_{i} 的结果就是全局最大值了.例如图例中,我们遍历完 S00,S01S\_{00}, S\_{01} 后,就能得到全局最大值 m0m\_{0} .
*   minewm^{new}_{i} : 表示 max(m~i0,m~i1,...,m~i(j−1),m~ij)max(\\widetilde{m}_{i0}, \\widetilde{m}_{i1}, ..., \\widetilde{m}_{i(j-1)}, \\widetilde{m}_{ij}) .如果当前分块是 SijS_{ij} ,则 minewm^{new}\_{i} 表示固定 ii 时,截止到当前分块为止的局部最大值.
*   linewl^{new}_{i} : 和 minewm^{new}_{i} 对应,相当于步骤(4)中用分块更新 l(x)l(x) 的步骤.
*   lil\_{i} : 和 mim\_{i} 同理,**即当我们将 jj\*\*\*\* 遍历完后,我们就能得到针对 ii\*\*\*\* 的全局rowmax和全局rowsum**.而根据前面的定义, minewm^{new}_{i} 和 linewl^{new}_{i} 是遍历完最新的 SijS\_{ij} 后得到的rowmax和rowsum结果,所以每遍历完一块 SijS\_{ij} ,我们就执行伪代码的第13行,做一次更新.

**如果你被论文中这些数学符号乱花了眼,那再告诉大家一个理解它们的trick**:

*   **所有以 ijij 作为下标的,都表示当前分块的计算结果**
*   **所有以 ii\*\*\*\* 作为下标的,都表示截止到前一个分块(包含前一个分块)的计算结果**
*   **所有以 newnew\*\*\*\* 为上标的,都表示引入当前分块做更新后的结果**
*   **所有没有下标的,都表示全局结果**

![](10_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

相信通过上面对数学表发符的介绍,大家已经大致理解了分块计算safe softmax的过程,为了加深理解,现在我们再来读一遍伪代码,把整个流程串起来:

*   `伪代码第5～7行`: 从HBM(显存)上读取 Kj,VjK\_{j}, V\_{j} 到on-chip存储SRAM.注意,在代码处理逻辑上,这里是固定外圈 jj ,循环内圈 ii .但是由于整个safe softmax逻辑是对“行”而言的,所以在理解时大家需要想像成固定内圈 ii ,循环外圈 jj ,也就是我们图例中绘制的深浅绿/蓝/黄色块.
*   `伪代码第8行`: 从HBM(显存)上读取 Qi,Oi,li,miQ\_{i}, O\_{i}, l\_{i}, m\_{i} .**记住我们之前说的trick,下标带 ii\*\*\*\* 的都表示截止到前一个分块的计算结果.虽然我们前面没介绍过 OiO\_{i} (在后文会细说),但按这个trick你应该也能猜到, OiO\_{i} 也是随着分块的移动而逐步更新的.等移动到最后一个分块时,我们就能得到和标准场景下一模一样的输出结果 OiO\_{i} .在之前的图例中,为了方便大家对分块的整体流程有快速理解,我们画了很多个 OO 出来,现在你应该能猜到,对每个 ii\*\*\*\* ,我们只维持并不断更新一个OiO\_{i},直至遍历完毕(例如之前的图例中,我们画了6个 OO\*\*\*\* ,但实际我们要维护更新的,只有3个: O0\*\*\*\*,O1\*\*\*\*,O2O\_{0}, O\_{1}, O\_{2} )**
*   `伪代码第9行`: 正常计算 Sij=QiKjTS\_{ij} = Q\_{i}K^{T}\_{j}
*   `伪代码第10行`: 基于当前分块 SijS\_{ij} 计算 m~ij,P~ij,lij~\\widetilde{m}_{ij}, \\widetilde{P}_{ij}, \\widetilde{l\_{ij}} .
*   `伪代码第11行`: 引入当前分块,计算截止目前为止的rowmax和rowsum,分别用 minew,linewm^{new}_{i}, l^{new}_{i} 表示.
*   `伪代码第12行`: 更新 OiO\_{i} ,后文会详细解析这部分公式
*   `伪代码第13行`: 用 minew,linewm^{new}_{i}, l^{new}_{i} 去更新 mi,lim\_{i}, l\_{i}

讲完了分块safe softmax的伪代码,这时你可能发现一个问题了: **之前你是否一直以为,在这一顿操作后,分块计算得出的 S,P**~**S, \\widetilde{P} 应该要和标准场景下的完全一致(比如应该是我们步骤(1)**~**(5)介绍的那样)？但是现在看来,每个分块**Sij,P~ijS\_{ij}, \\widetilde{P}\_{ij}**依然是用自己局部的rowmax和rowsum做计算的,并没有达到我们理想中的效果呀！**

别急,还记得伪代码第12行我们说的更新 OiO\_{i} 的公式么？**分块计算的真正意义不在于得到正确的 S,P~S, \\widetilde{P} ,而在于得到正确的 OO\*\*\*\* .**

然后,你再来看伪代码5-13行,你会发现,在整个计算过程中,只有 mi,li,Oim\_{i}, l\_{i}, O\_{i} 被从on-chip的SRAM中写回到显存(HBM)中.**把 ii\*\*\*\* 都遍历完后,读写量也不过是 m,l,Om, l, O\*\*\*\* .相比于标准场景下,我们要读写的是 S,P,OS, P, O\*\*\*\* ,读写量是不是一下就少很多,这不就能解决memory-bound的问题了吗.**

所以,**分块计算safe softmax的意义,就是抹去对**S,PS,P**的读写.**

### 4.5 分块计算中的输出O

终于到翘首以盼的输出$O$的分析部分了,当你第一次看到伪代码12行更新$O\_{i}$的公式,是不是觉得两眼一黑？不要紧,这里我们依然通过图解的方式,帮助大家理解并推导这个公式.

![](9_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

之前我们说过,\*_上图中画的6个 **O**O_\*\*\* 并不是我们最终想要的结果.\*\*我们期望维护并更新 OiO\_{i} ,当该 ii 下的所有 jj 遍历完毕后,我们的 OiO\_{i} 就应该和标准场景下的 OiO\_{i} 完全相等.

回到图例中,图中的 OiO\_{i} 就应该等于被红框圈起来的 S,PS,P 部分和 VV 部分的乘积.但是别忘记之前说过,**这里各块**S,PS, P\*\*都是局部rowmax,rowsum计算出来的结果.\*\*所以我们必须对各块 S,PS, P 再做一些处理,才能让它们和V相乘,更新 OiO\_{i} .

那么要处理到什么程度为止呢？\*_第一想法可能是,只要让每块 **S**,**P**S,P_\*\*\* 结果和标准场景下的结果完全一致,不就行了吗？但是别忘了,你不计算到最后一块 **S**,**P**S,P\*\*\*\* ,你是拿不到全局的rowmax和rowsum的.\*\*而由于为了解决memory-bound的问题,我们只保留 m,l,Om,l,O 而不存各块 S,PS,P .因此等你遍历到最后一块时,虽然有了全局的rowmax和rowsum,但没有 S,PS, P ,你根本算不出最终的 OiO\_{i} .

所以这里我们**换个思路**: OiO\_{i} 不是每遍历一块就更新一次吗？那有没有一种办法,**不断用当前最新的rowmax和rowsum去更新**OiO\_{i}\*\*,直到遍历完最后一块,这时的 **O**i**O\_{i}** 不就和标准场景下的结果完全一致了吗？也就是我们想构造形如下面这样的更新等式: \*\*

Oi=Oi+当前最新结果O\_{i} = O\_{i} + 当前最新结果

沿着这个思路,我们来看伪代码第12行公式的诞生过程:

Oi(j+1)=Pi,:j+1V:j+1=softmax(Si,:j+1)V:j+1=diag(l(j+1))−1\[exp(\[Si,:j,Si(j+1)\]−m(j+1))\]\[V:jVj+1\]=diag(l(j+1))−1\[exp(Si,:j−m(j+1))V:j+exp(Si(j+1)−m(j+1))Vj+1)\]=diag(l(j+1))−1\[e−m(j+1)exp(Si,:j)V:j+e−m(j+1)exp(Si(j+1))Vj+1)\]=diag(l(j+1))−1\[diag(l(j))em(j)−m(j+1)diag(l(j))−1exp(Si,:j−m(j))V:j+e−m(j+1)exp(Si(j+1))Vj+1)\]=diag(l(j+1))−1\[diag(l(j))em(j)−m(j+1)Pi,:jV:j+e−m(j+1)exp(Si(j+1))Vj+1)\]=diag(l(j+1))−1\[diag(l(j))em(j)−m(j+1)Oi(j)+em~−m(j+1)exp(Si(j+1)−m~)Vj+1\]=diag(l(j+1))−1\[diag(l(j))em(j)−m(j+1)Oi(j)+em~−m(j+1)P~i(j+1)Vj+1\]\\begin{aligned} O^{(j+1)}_{i}&= P_{i,:j+1}V\_{:j+1}\\ &= softmax(S\_{i,:j+1})V\_{:j+1}\\ &= diag(l^{(j+1)})^{-1}\[exp(\[S\_{i,:j}, S\_{i(j+1)}\]-m^{(j+1)})\]\\begin{bmatrix} V\_{:j}\\V\_{j+1} \\end{bmatrix}\\ &= diag(l^{(j+1)})^{-1}\[exp(S\_{i,:j}-m^{(j+1)})V\_{:j} + exp(S\_{i(j+1)}-m^{(j+1)})V\_{j+1})\]\\ &= diag(l^{(j+1)})^{-1}\[e^{-m^{(j+1)}}exp(S\_{i,:j})V\_{:j} + e^{-m^{(j+1)}}exp(S\_{i(j+1)})V\_{j+1})\]\\ &= diag(l^{(j+1)})^{-1}\[diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}}diag(l^{(j)})^{-1}exp(S\_{i,:j}-m^{(j)})V\_{:j} + e^{-m^{(j+1)}}exp(S\_{i(j+1)})V\_{j+1})\]\\ &= diag(l^{(j+1)})^{-1}\[diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}}{P}_{i,:j}V_{:j} + e^{-m^{(j+1)}}exp(S\_{i(j+1)})V\_{j+1})\]\\ &= diag(l^{(j+1)})^{-1}\[diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}}O^{(j)}_{i} + e^{\\widetilde{m}-m^{(j+1)}}exp(S_{i(j+1)}-\\widetilde{m})V\_{j+1}\]\\ &= diag(l^{(j+1)})^{-1}\[diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}}O^{(j)}_{i} + e^{\\widetilde{m}-m^{(j+1)}}\\widetilde{P}_{i(j+1)}V\_{j+1}\] \\end{aligned}

初次看到这个推导过程,你可能有些懵圈,不要紧,我们一行一行来看.在讲解之前,我们先明确以上推导过程中符号**上下标**的含义:

*   ii : 这个大家应该很熟悉了.例如图例中, i=0,1,2i=0,1,2 分别对应着深浅绿、深浅蓝、深浅黄块.
*   i(j+1)i(j+1) : 表示**当前分块**的相关结果
*   i,:j+1i,:j+1 : 表示\*\*截止到当前分块(包含当前分块)**的相关结果. i,:ji, :j 表示**截止到前一分块(包含前一分块)\*\*的相关结果.

(1)第一行: **首先,我们期望的结果是,每遍历一个分块,就更新一次**OiO\_{i}**,遍历完全部的分块后,我们就能得到和标准场景下完全一致的**OiO\_{i} .基于此我们有 Oi(j+1)=Pi,:j+1V:j+1O^{(j+1)}_{i} = P_{i,:j+1}V\_{:j+1} .其中, Pi,:j+1P\_{i,:j+1} 表示从第0个分块到当前分块,我们用**当前最新**的rowmax,rowsum更新一次**所有分块**的 PP 结果(因为做过归一化了,所以是不带波浪号的 PP ). V:j+1V\_{:j+1} 则表示当前分块及之前所有分块所对应着的 VV 部分(例如图例中,若当前分块是浅绿色块,则其对应着浅灰色 VV ；若当前分块是深绿色块,则其对应着浅灰色+深灰色 VV ).

(2)第二行: 将 Pi,:j+1P\_{i,:j+1} 改写成 softmax(Si,:j+1)softmax(S\_{i, :j+1}) 的形式.**特别注意,这里 Si\*\*\*\*,\*\*:j+1S\_{i,:j+1}\*\* 所代表的各个分块间都是相互独立的**,你可以理解为,只有在做 softmaxsoftmax 这个操作时,才考虑对这些独立的 SS 用最新的rowmax,rowsum去更新 PP .

(3)第三行: 就是把(2)当中的 softmaxsoftmax 展开写了.即用当前最新的rowmax和rowsum去计算 PP .这里将 Si,:j+1S\_{i,:j+1} 拆成 \[Si,:j,Si(j+1)\]\[S\_{i,:j}, S\_{i(j+1)}\] 两部分(**\[之前所有的分块,当前分块\]**).同理拆 VV .

(4)～(5)第四～五行: 做简单的变式,不再赘述.

(6)第六行: 我们观察到,中括号式子里的前半部分,和之前所有分块的结果密切相关.**联想到我们最终的目标是不断更新**OiO\_{i}**,也就是在上一个**OiO\_{i}\*\*的基础上,引入当前分块的信息做更新.\*\*因此,能不能把上一个 OiO\_{i} (对应到我们的式子里就是 Oi(j)O^{(j)}\_{i} )表达出来呢？

基于这个思想做递推, Oi(j)O^{(j)}_{i} 当然就是**之前的所有分块**,用**上一分块**的rowmax、rowsum做更新后求得 PP ,再乘上对应的 VV 得到的结果呀,所以根据此我们攒出了 diag(l(j))−1exp(Si,:j−m(j))V:jdiag(l^{(j)})^{-1}exp(S_{i,:j}-m^{(j)})V\_{:j} 这一项(就是 Oi(j)O^{(j)}\_{i} ),然后再用 diag(l(j))em(j)−m(j+1)diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}} 去抵消我们在攒它的过程中引入的项.

(7)~(9): 第七～九行: 明确了(6)以后,剩下的部分就很好理解啦.这里额外说下,为什么要把 m~\\widetilde{m} 放进去呢(毕竟有了 Si(j+1),m(j+1)S\_{i(j+1)}, m^{(j+1)} 都是已知的,已经可以算了).因为我们在求解rowsum相关的数据时,还是要把数据从 SS 转为 P~\\widetilde{P} 才能求,因此避不开算 P~\\widetilde{P} .另外也是为了让表达起来更统一,因此这里引入 m~\\widetilde{m} ,进而引入 P~\\widetilde{P} 进行计算.

现在再回头看伪代码的第12行,是不是就很清楚了呢？**建议大家可以自行画图,动手推导,加深理解.**

五、Backward运作流程
--------------

### 5.1 softmax求导

在后文对分块计算backward中,我们会频繁接触到和softmax求导相关的知识,繁杂的数学符号可能会使很多朋友看得蒙圈,所以这里我们做个快速复习.

设

{y=softmax(z)L=f(y)\\left{\\begin{matrix} \\begin{aligned} y &= softmax(z)\\ L &= f(y) \\end{aligned} \\end{matrix}\\right.

其中, LL 表示Loss, f(.)f(.) 表示Loss函数, y=\[y1y2y3\]y = \\begin{bmatrix} y\_{1}&y\_{2}&y\_{3} \\end{bmatrix} , z=\[z1z2z3\]z = \\begin{bmatrix} z\_{1}&z\_{2}&z\_{3} \\end{bmatrix} ,若现在我们想求 ∂L∂zj\\frac{\\partial L}{\\partial z\_{j}} ,要怎么算呢？

根据链式法则,我们有 ∂L∂zj=∂L∂y∂y∂zj\\frac{\\partial L}{\\partial z\_{j}} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial z\_{j}} ,所以我们分别来看这两项.

(1) ∂L∂y\\frac{\\partial L}{\\partial y}

我们现在不考虑具体的Loss函数,直接假设这一项的结果为 \[m1m2m3\]\\begin{bmatrix} m\_{1}&m\_{2}&m\_{3} \\end{bmatrix}

(2) ∂y∂zj\\frac{\\partial y}{\\partial z\_{j}}

我们知道,对于某个 zjz\_{j} 来说,在softmax的操作下,它参与了 y1,y2,y3y\_{1}, y\_{2}, y\_{3} 三者的计算,因此它的偏导也和这三者密切相关,这里我们分成两种情况:

{∂yi∂zj=yi(1−yi),当i=j∂yi∂zj=−yiyj,当i≠j\\left{\\begin{matrix} \\begin{aligned} \\frac{\\partial y\_{i}}{\\partial z\_{j}} &= y\_{i}(1-y\_{i}),当i=j\\ \\frac{\\partial y\_{i}}{\\partial z\_{j}} &= -y\_{i}y\_{j},\\qquad当i\\neq j \\end{aligned} \\end{matrix}\\right.

根据这个结果,我们有: ∂y∂zj=∑i=1l∂yi∂zj\\frac{\\partial y}{\\partial z\_{j}} = \\sum\_{i=1}^{l}\\frac{\\partial y\_{i}}{\\partial z\_{j}}

这里 ll 代表向量中一共有几个要素,例如在本例中, l=3l=3 .假设我们现在要求 ∂y∂z1\\frac{\\partial y}{\\partial z\_{1}} ,则根据上述公式,我们有:

∂y∂z1=y1(1−y1)−y2y1−y3y1\\frac{\\partial y}{\\partial z\_{1}} = y\_{1}(1-y\_{1}) - y\_{2}y\_{1} - y\_{3}y\_{1}

这里不再赘述详细的推动过程,有需要的朋友可以参考[这篇文章](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/wuliytTaotao/p/10787510.html).

有了这个理解,我们再来谈谈基于 y=softmax(z)y= softmax(z) 的[Jacobian矩阵](https://zhida.zhihu.com/search?content_id=631880308&content_type=Answer&match_order=1&q=Jacobian%E7%9F%A9%E9%98%B5&zhida_source=entity)diag(y)−yTydiag(y) - y^{T}y :

diag(y)−yTy=\[y1000y2000y3\]−\[y1y2y3\]∗\[y1y2y3\]=\[y1−y12−y1y2−y1y3−y2y1y2−y22−y2y3−y3y1−y3y2y3−y32\]\\begin{aligned} diag(y) - y^{T}y &= \\begin{bmatrix} y\_{1}&0&0 \\ 0&y\_{2}&0 \\ 0&0&y\_{3} \\end{bmatrix}-\\begin{bmatrix} y\_{1}\\y\_{2}\\y\_{3} \\end{bmatrix}\*\\begin{bmatrix} y\_{1}&y\_{2}&y\_{3} \\end{bmatrix}\\ &=\\begin{bmatrix} y\_{1}-y\_{1}^{2}&-y\_{1}y\_{2}&-y\_{1}y\_{3} \\ -y\_{2}y\_{1}&y\_{2}-y\_{2}^{2}&-y\_{2}y\_{3} \\ -y\_{3}y\_{1}&-y\_{3}y\_{2}&y\_{3}-y\_{3}^{2} \\end{bmatrix} \\end{aligned}

很容易发现只要把每行/每列相加,就能得到对应$z$的偏导.别着急求和,我们继续往下看.

(3) ∂L∂zj=∂L∂y∂y∂zj\\frac{\\partial L}{\\partial z\_{j}} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial z\_{j}}

有了(1)(2)的结果,现在就可以来推导 ∂L∂zj\\frac{\\partial L}{\\partial z\_{j}} ,我们有:

∂L∂zj=∂L∂y∂y∂zj=∑i=1l∂L∂yi∂yi∂zj=yj(dyj−∑j=1lyjdyj)\\frac{\\partial L}{\\partial z\_{j}} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial z\_{j}} =\\sum\_{i=1}^{l}\\frac{\\partial L}{\\partial y\_{i}}\\frac{\\partial y\_{i}}{\\partial z\_{j}} = y\_{j}(\\mathrm{d}y\_{j}-\\sum\_{j=1}^{l}y\_{j}dy\_{j})

举个例子,若我们现在想求 ∂L∂z1\\frac{\\partial L}{\\partial z\_{1}} ,我们将 ∂L∂y=\[m1m2m3\]\\frac{\\partial L}{\\partial y} = \\begin{bmatrix} m\_{1}&m\_{2}&m\_{3} \\end{bmatrix} 代入上面公式,则有:

∂L∂z1=m1(y1−y12)−m2y1y2−m3y1y3\\frac{\\partial L}{\\partial z\_{1}} = m\_{1}(y\_{1}-y\_{1}^{2}) - m\_{2}y\_{1}y\_{2} - m\_{3}y\_{1}y\_{3}

现在,针对所有的 zz ,我们将 ∂L∂z\\frac{\\partial L}{\\partial z} 写成矩阵表达式有:

∂L∂z=∂L∂y∂y∂z=dy(diag(y)−yTy)=[m1m2m3](#root/alzFBwEmXrMu))=\[m1m2m3\]\[y1−y12−y1y2−y1y3−y2y1y2−y22−y2y3−y3y1−y3y2y3−y32\]\\begin{aligned} \\frac{\\partial L}{\\partial z} &=\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial z} =\\mathrm{d}y(diag(y) - y^{T}y)\\ &=\\begin{bmatrix} m\_{1}&m\_{2}&m\_{3} \\end{bmatrix}(\\begin{bmatrix} y\_{1}&0&0 \\ 0&y\_{2}&0\\ 0&0&y\_{3} \\end{bmatrix} - \\begin{bmatrix} y\_{1}\\y\_{2}\\y\_{3} \\end{bmatrix}\\begin{bmatrix} y\_{1}&y\_{2}&y\_{3} \\end{bmatrix}))\\ &=\\begin{bmatrix} m\_{1}&m\_{2}&m\_{3} \\end{bmatrix}\\begin{bmatrix} y\_{1}-y\_{1}^{2}&-y\_{1}y\_{2}&-y\_{1}y\_{3} \\ -y\_{2}y\_{1}&y\_{2}-y\_{2}^{2}&-y\_{2}y\_{3} \\ -y\_{3}y\_{1}&-y\_{3}y\_{2}&y\_{3}-y\_{3}^{2} \\end{bmatrix} \\end{aligned}

\*\*至此,大家记住这两个重要的结论: \*\*

∂L∂z=∂L∂y∂y∂z=dy(diag(y)−yTy)∂L∂zj=yj(dyj−∑j=1lyjdyj)\\begin{aligned} \\frac{\\partial L}{\\partial z} &=\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial z} =\\mathrm{d}y(diag(y) - y^{T}y)\\ \\frac{\\partial L}{\\partial z\_{j}} &= y\_{j}(\\mathrm{d}y\_{j}-\\sum\_{j=1}^{l}y\_{j}dy\_{j}) \\end{aligned}

### 5.2 标准backward计算

![](11_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

我们先来总结下forward中做的操作,为了表达简便,这里将mask、dropout等零碎操作省去,同时假设$f(.)$是损失函数:

S=QKTP=softmax(S)O=PVL=f(O)\\begin{matrix} \\begin{aligned} S &= QK^{T}\\ P &= softmax(S)\\ O &= PV \\ L & = f(O) \\end{aligned} \\end{matrix}

对于标准backward来说,在计算开始时,显存(HBM)上已经存放有 Q,K,V,O,S,PQ, K, V, O, S, P 这些数据.论文中的伪代码已经介绍得非常清楚,大家可以自行阅读,这里就不赘述了.对伪代码第3行求 dSijdS\_{ij} 有困惑的朋友,可见上文“softamx求导”部分.

### 5.3 分块backward计算

在讲解backward计算前,我们先来看看经过分块Forward计算后,显存(HBM)上都存了哪些数据:

*   mm : 全局rowmax
*   ll : 全局rowsum
*   Q,K,VQ, K, V : 等同于标准attention场景下的结果
*   OO : 等同于标准attention场景下的输出结果 OO
*   dO\\mathrm{d}O : 有了完整的 OO ,我们就可以按正常的backward步骤先求出它的梯度,也存放在显存上.然后我们就能按照链式法则,分块地去求别的矩阵的梯度了.

既然有了全局的 m,lm,l ,那么现在对于任意一块 SijS\_{ij} ,我们就能基于$m,l$算出和标准场景下完全一致的 PijP\_{ij} 了.因此,在backward的过程中,flash attention将采用**重计算**的方式,**重新算出 Sij,PijS\_{ij}, P\_{ij}\*\*\*\* ,并将它们运用到backward的计算中去,所以在接下来的讲解中,大家就可以把 S,PS, P\*\*\*\* 理解成完全等同于标准场景下的结果,而不是像分块计算forward中那样的 S,PS, P\*\*\*\* .**

\*\*另外需要注意的是,为了简化表达,在接下来的分析中,关于mask、dropout之类的步骤,我们在表述上都略去.\*\*现在让我们来看分块计算backward的伪代码:

![](15_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

**(1)求 VjV\_{j} 梯度**

由Forward过程我们知: O=PVO = PV ,因此有了 dOdO 后,我们就可以先来求 dPdP 和 dVdV 了.**观察下方的图,我们会发现此时所有的**PP**都是不带波浪号的,再强调一下,这是因为经过了重计算,此处**S,PS, P**的结果都等同于标准场景下的结果,而不是forward中所代表的含义.**

![](21_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

假设现在 j=0j=0 ,那我们要怎么求 dV0dV\_{0} 呢？

**我们先来看**V0V\_{0}**都参与了**OO**哪些部分的计算,以及是怎么参与的**: 由图可知, P00P\_{00} 和 V00V\_{00} 参与了 O0O\_{0} 的计算, P10P\_{10} 和 V00V\_{00} 参与了 O1O\_{1} 的计算, P20P\_{20} 和 V0V\_{0} 参与了 O2O\_{2} 的计算.所以我们有:

dV0=(P00)TdO0+(P10)TdO1+(P20)TdO2dV\_{0} = (P\_{00})^{T}dO\_{0} + (P\_{10})^{T}dO\_{1} + (P\_{20})^{T}dO\_{2}

进而推知:

dVj=∑i(Pij)TdOidV\_{j} = \\sum\_{i}(P\_{ij})^{T}dO\_{i}

在伪代码11～15行中,做的都是 S,PS, P**重计算**的过程,伪代码的第16行,就是在按这个方法分块计算并累积 dVjdV\_{j} .

**(2)求 PijP\_{ij}\*\*\*\* 梯度**

![](16_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

观察上图,可以发现 PijP\_{ij} 只与 Vj,OiV\_{j}, O\_{i} 相关,例如 P10P\_{10} 只与 V0,O1V\_{0}, O\_{1} 相关.因此我们有:

dP\_{ij} = dO\_{i}V\_{j}^{T}

这就是伪代码第17行做的事情.

**(3)求 S\_{ij} 梯度**

这一块是令许多人感到迷惑的,我们先来\*\*回顾下“softmax求导”部分让大家记住的一个重要结论: \*\*

\\frac{\\partial L}{\\partial z} =\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial z} =\\mathrm{d}y(diag(y) - y^{T}y)

我们假设 s\_{i}, p\_{i}, o\_{i} 分别为矩阵 S,P,O 的某一行(注意这里 i 不是表示第 i 块的意思,是表示第 i 行,所以我们用小写的 s, p, o 表示),那么根据这个结论,我们有:

\\begin{aligned} ds\_{i} &= dp\_{i}(diag(p\_{i}) - p\_{i}^{T}p\_{i})\\ &= dp\_{i}diag(p\_{i})-dp\_{i}p\_{i}^{T}p\_{i}\\ &=dp\_{i}diag(p\_{i})-do\_{i}V^{T}p\_{i}^{T}p\_{i}\\ &= dp\_{i}diag(p\_{i}) - do\_{i}o\_{i}^{T}p\_{i}\\ &= p\_{i}\\circ \[dp\_{i} - rowsum(do\_{i}\\circ o\_{i})\] \\end{aligned}

**你可能对这个推导的最后一步有疑惑: 为什么要大费周章,将**ds\_{i}\*\*改写成这么复杂的形式呢？因为在最后一步之前,我们都是针对“某一行”来求导,而引入最后一步的目的,是为了延展至对“某一块(多行)”的求导,\*\*也就是说针对某一块 dS\_{i} (注意这里是大写的 S , i 的含义也回归至“第几块”),我们有:

dS\_{i} = P\_{i}\\circ\[dP\_{i} - rowsum(dO\_{i}\\circ O\_{i})\]

如果实在难以理解推导过程,建议大家可以带一些具体的值进去,就能理解我们为什么要写成这种形式了.进而,我们可以推知:

dS\_{ij} = P\_{ij}\\circ\[dP\_{ij} - rowsum(dO\_{i}\\circ O\_{i})\]

这就是伪代码第19～20行做的事情.

**(4)求 Q\_{i} 梯度**

![]([公式要更新]大模型推理加速技术的学习路线是什么&_imag.svg)

到目前为止,我们已经知道 dS\_{ij} ,那么现在就可以根据链式法则继续求 dQ\_{i} 了.

对照上图,我们把目光聚焦在 Q\_{0} 身上,由forward过程可知:

\\begin{matrix} S\_{00}&= Q\_{0}K\_{0}^{T}\\ S\_{01} &= Q\_{0}K\_{1}^{T} \\end{matrix}

因此,针对 Q\_{0} ,我们有: dQ\_{0} = dS\_{00}K\_{0} + dS\_{01}K\_{1}

推广到任意 Q\_{i} ,我们有: dQ\_{i} = \\sum\_{j}dS\_{ij}K\_{j}

这就是伪代码第21行做的事情.

**(5)求 K\_{j} 梯度**

![](1_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.svg)

这一步就很简单啦,**如果你被复杂的分块推导弄懵了脑袋,那不妨再复习一下我们前面提过的trick**: 对照上图,取出某一块 K\_{j} .由于我们是从 dS\_{ij} 链式推向 K\_{j} ,所以这里只要搞明白这块 K\_{j} 和哪些 Q 一起计算出了哪些 S 再把相关结果相加即可.

只要看了流程图,就不难得知: 某块 K\_{j} 和对应的 Q\_{i} 共同计算出了对应的 S\_{ij} ,因此有:

dK\_{j} = \\sum\_{i}dS\_{ij}^{T}Q\_{i}

**这就是伪代码第22行做的事情.**

好！现在我们就把分块backward的细节讲完了,**当大家感到迷茫时,一定记得画图**；在碰到需要做累加才能计算出梯度的步骤中,画图也可以帮助我们快速理解是按 i 维度还是按 j 维度进行累加.

六、计算量和显存需求
----------

### 6.1 矩阵相乘的计算量

我们先来看一个前置知识: **两个矩阵相乘,要怎么统计它们的计算量？**

我们一般用**FLOPs(floating point operations,浮点运算次数)来表示运算量的大小.对于“两矩阵相乘”这个操作而言,其运算量 = 乘法运算的次数 + 加法运算的次数.**

来看一个具体例子:

![](13_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

两矩阵相乘,**为了获取图中深橘色部分的元素,我们一共需要进行n次乘法运算和n-1次加法运算**.

那么现在结果矩阵中,一共有m\*p个橘色方块,则意味着我们需要进行: `m*p*(n + n - 1)`次浮点计算.

再进一步,假设此时在蓝色和绿色的矩阵外,我们还有一个**bias矩阵**,意味着计算单个橘色方块时我们需要进行n次乘法和n-1+1次加法运算,那么此时总计算量为: `m*p*(n+n) = 2mnp`.当然,即使不加这个bias,我们也可以把-1项给忽略,得到相同的结果.

**所以这里我们总结下,假设有两个矩阵A和B,它们的维度分别为(m, n)和(n, p),则这两矩阵相乘的运算量为**`**2mnp**`**.**

一般在矩阵运算中,**乘法运算的时间要高于加法运算的时间,因此有时在统计运算量时,我们只考虑乘法运算的次数,则此时两矩阵相乘的运算量可近似为mnp**

### 6.2 Flash Attention的计算量

有了前置知识,我们就能分析flash attention的计算量了,我们以forward过程为例(为了大家阅读方便,我们再把forward的伪代码放一遍):

![](12_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

我们知道矩阵相乘运算占据了运算量的大头,因此我们把分析目光集中到所有的矩阵运算上来.

(1)在代码第9行,我们有 Sij=QiKjTS\_{ij} = Q\_{i}K\_{j}^{T} ,其中 Qi∈RBr∗d,KjT∈Rd∗BcQ\_{i}\\in\\mathbb{R}^{B\_{r}_d}, K\_{j}^{T} \\in \\mathbb{R}^{d_B\_{c}} .根据前置知识,求 SijS\_{ij} 的计算量为 O(BrBcd)O(B\_{r}B\_{c}d) .

(2)在代码第12行,我们有 P~ijVj\\widetilde{P}~_~{ij}V~_~{j} ,其中 P~ij∈RBr∗Bc,Vj∈RBc∗d\\widetilde{P}_{ij} \\in \\mathbb{R}^{B_{r}\*B\_{c}}, V\_{j} \\in \\mathbb{R}^{B\_{c}\*d} .则这里的计算量同样为 O(BrBcd)O(B\_{r}B\_{c}d)

(3)接下来我们看一共计算了多少次(1)和(2),也就是执行了多少次内循环: TcTr=NBcNBrT\_{c}T\_{r} = \\frac{N}{B\_{c}}\\frac{N}{B\_{r}}

(4)\*\*综合以上三点,flash attention的forward计算量为: \*\*O(N2BcBrBrBcd)=O(N2d)O(\\frac{N^{2}}{B\_{c}B\_{r}}B\_{r}B\_{c}d) = O(N^{2}d) ,注意,因为计算量是用大O阶表示的,所以这里我们把常数项都省略了.

同理大家可以自行推一下backward中的计算量,在论文里给出的结论是 O(N2)O(N^{2}) ,d远小于N,因此 dd 也可以略去不表达.

### 6.3 Flash Attention的显存需求

和标准attention相比,如果不考虑 OO 的话,Flash Attention只需要存储 m,lm,l ,其显存需求为 O(N)O(N) .

而标准attention需要存储 S,PS,P ,其显存需求为 O(N2)O(N^{2}) .

**可以发现相比于标准attention,flash attention明显降低了对显存的需求.**

七、IO复杂度
-------

之前我们强调过,flash attention相比于标准attention的最大优势,就是其减少了对显存(HBM)的访问次数,一定程度上解决了memory bound的问题.所以这一节我们就来具体分析这两者对显存的访问次数(同样都是以forward为例,backward部分论文中也有给出相关推导过程,大家可以类比forward自行阅读).

### 7.1 标准attention的IO复杂度

![](18_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

(1)从HBM中读取 Q,K∈RN∗dQ, K \\in \\mathbb{R}^{N \* d} ,计算 S=QKT,S∈RN∗NS = QK^{T}, S \\in \\mathbb{R}^{N\*N} 并将 SS 写回HBM.一读一写的IO复杂度为: O(Nd+N2)O(Nd + N^{2}) ,在表示大O阶时我们忽略常数项.

(2)从HBM中读取 S∈RN∗NS \\in \\mathbb{R}^{N_N} ,同时计算 P∈RN∗NP \\in \\mathbb{R}^{N_N} 并将其写回HBM.一读一写的IO复杂度为: O(N2)O(N^{2})

(3)从HBM中读取 P∈RN∗N,V∈RN∗dP \\in \\mathbb{R}^{N_N}, V \\in \\mathbb{R}^{N_d} ,计算 O=PV,O∈RN∗dO=PV, O \\in \\mathbb{R}^{N\*d} 并将 OO 写回HBM.一读一写的IO复杂度为: O(Nd+N2)O(Nd + N^{2})

所以,\*\*总体来说标准attention的IO复杂度为: \*\*O(Nd+N2)O(Nd + N^{2})

### 7.2 Flash attention的IO复杂度

(1)我们来看伪代码的第6行,在每个外循环中,我们都会加载 K,VK, V 的block.所有外循环结束后,相当于我们加载了完整的 K,V∈RN∗dK, V \\in \\mathbb{R}^{N\*d} ,因此这里的IO复杂度为: O(Nd)O(Nd)

(2)再看伪代码第8行,在每个内循环中,我们都加载了部分 Q,O,m,lQ, O, m, l block,由于 m,lm, l 本身比较小(IO复杂度是 O(N)O(N) ),因此我们暂时忽略它们,只考虑 Q,OQ, O (原论文也是这么分析的).固定某个外循环,所有内循环结束后,我们相当于完整遍历了 Q,O∈RN∗dQ, O \\in \\mathbb{R}^{N\*d} .同时我们会经历 TcT\_{c} 次外循环.因此这里最终的IO复杂度为: O(TcNd)O(T\_{c}Nd) .

(3)将 O,m,lO, m, l 写回HBM,这里近似后IO复杂度为: O(Nd)O(Nd) .不过在原论文的分析中并没有考虑写回的复杂度,不过省略一些常数项不会影响我们最终的分析.

所以,\*\*总体来说flash attention的IO复杂度为: \*\*

O(TcNd)=O(NBcNd)=O(4NdMNd)=O(N2d2M)O(T\_{c}Nd) = O(\\frac{N}{B\_{c}}Nd) = O(\\frac{4Nd}{M}Nd) = O(\\frac{N^{2}d^{2}}{M})

论文中提过,一般d的取值在64～128,M的取值在100KB左右,因此有 d2M<<1\\frac{d^{2}}{M} << 1 .**因此可以看出,Flash attention的IO复杂度是要显著小于标准attention的IO复杂度的.**

八、实验效果
------

![](19_[公式要更新]大模型推理加速技术的学习路线是什么&_imag.webp)

Flash attention的作者将 N=1024,d=64,B=64N=1024, d = 64, B = 64 的GPT2-medium部署在A100 GPU上,来观测采用flash attention前后的模型的计算性能.

我们先看最左侧图表,标准attention下,计算强度 I=66.640.3=1.6<201I = \\frac{66.6}{40.3} = 1.6 < 201 ,说明GPT2在A100上的训练是受到内存限制的.而在采用flash attention后得到了明显改善,runtime也呈现了显著下降.

我们再来看中间的图表,它表示在使用flash attention的前提下,以forward过程为例,每个数据块的大小对HBM读写次数(绿色)和耗时(蓝色)的影响.可以发现,数据块越大,读写次数越少,而随着读写次数的减少,runtime也整体下降了(复习一下,读写复杂度为 O(TcNd)O(T\_{c}Nd) ,数据块越大意味着 TcT\_{c} 越小).**但有意思的是,当数据块大小>256后,runtime的下降不明显了,这是因为随着矩阵的变大,计算耗时也更大了,会抹平读写节省下来的时间.**

九、参考
----

1、[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2205.14135)

2、[https://leimao.github.io/blog/Math-Bound-VS-Memory-Bound-Operations/](https://link.zhihu.com/?target=https%3A//leimao.github.io/blog/Math-Bound-VS-Memory-Bound-Operations/)

3、[回旋托马斯x: FlashAttention:加速计算,节省显存, IO感知的精确注意力](https://zhuanlan.zhihu.com/p/639228219)

4、[紫气东来: NLP(十七): 从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能](https://zhuanlan.zhihu.com/p/638468472)

5、[暧暧内含光: GPU 内存概念浅析](https://zhuanlan.zhihu.com/p/651179378)

6、[kaiyuan: GPU内存(显存)的理解与基本使用](https://zhuanlan.zhihu.com/p/462191421)

7、[小小将: CUDA编程入门极简教程](https://zhuanlan.zhihu.com/p/34587739)

8、[Michael Yuan: Roofline Model与深度学习模型的性能分析](https://zhuanlan.zhihu.com/p/34204282)