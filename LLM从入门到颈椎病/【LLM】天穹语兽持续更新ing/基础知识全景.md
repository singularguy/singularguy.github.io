# 基础知识全景
今天刷到了 Mr. Hand 的小红书，发现我原来整的东西太偏实践了，以《从零到 1 手搓出一个大模型》为主题，忘记了其实还有很多知识点（八股文）要熟练掌握的。但这两部分其实是交错在一起的，所以这里就放基础知识的一个 list，到时候引用就好了

大模型基础知识一览
---------

第一部分：Transformer 结构
-------------------

与 LLM 相关的面试都会问到 Transformer，比如手撕多头注意力，自注意力缩放，参数计算等等。 Transformer 相关内容可以看 Transformer

### 1\. 分词器 tokenizer & Embedding 层

(但是这一块应该放在训练里的，或许也可以放在理论基础里 anyway doesn't matters)

*   BPE，BBPE，WordPiece 等算法
*   了解各类模型的分词方法
*   感兴趣的同学可以看一下 tokenizer 在预训练过程具体如何处理
*   Embedding

### 2\. 注意力模块

*   Self-attention，cross-attention 的原理
*   MHA、MQA、GQA、MLA、DCA 等多种注意力机制优化策略（可能会考手撕）
*   线性注意力，稀疏注意力，kvcache 等等
*   推荐看苏神的科学空间，原理推导写的很清楚

### 3\. 前馈神经网络 FFN & 残差连接 & 归一化

感觉这部分主要是八股文呢

*   这几个模块的作用是什么
*   LN 和 BN 的区别
*   Pre-norm 和 post-norm
*   SwiGLU 等激活函数
*   RMSNorm 等归一化

第二部分：主流大模型
----------

大模型发展脉络

*   BERT 系列
*   GPT
*   Llama
*   Qwen
*   GLM
*   Baichuan
*   DeepSeek 等等
*   注意关注一下发展脉络，每一代做了哪些优化，不要只看最新版

第三部分：预训练 Pre-train 过程
---------------------

*   预训练任务有哪些  
    预训练
*   数据配比
*   数据筛选过滤方法
*   合成数据
*   推荐阅读主流大模型开源的技术报告
*   数据工程

第四部分：后训练 Post-train 过程
----------------------

这部分是面试过程中考察的第二个重点，一般会联合实习项目一起深挖。

*   微调
*   对齐

### 1\. 全量微调/高效微调

*   微调数据构造
*   数据配比
*   全参微调
*   冻结微调 LoRA
*   PEFT 高效微调（prompt tuning、p-tuning v 2、prefix-tuning、adapter-tuning、LoRA 及其变体）
*   CoT，Reasoning 等 o 1 系列策略（暂且放在这里了）
*   建议做一个完整项目，加深对每一个步骤的理解

### 2\. RLHF & Aligning

*   为什么有 SFT 还需要 RLHF，两者有何区别
*   RLAIF，ReFT，OpenAI 做 RLHF 的过程
*   里面的几个模型分别是怎么运作的
*   DPO 的原理和实现
*   PPO 和 DPO 对比
*   DPO 有哪些问题以及如何优化
*   SimPO，KTO，ORPO，GRPO 等等

第五部分：模型压缩与量化
------------

*   了解各种量化方式
*   GPTQ，AWQ 等
*   各种量化精度
*   训练显存计算

### 4\. 位置编码 PE

*   为什么需要位置编码
*   常见的位置编码有哪些，如正余弦、可学习、RoPE、ALiBi、YARN 等
*   绝对位置编码和相对位置编码
*   长度外推策略
*   了解各大模型长上下文处理方式

### 5\. 代表模型

*   Encoder-only
*   Decoder-only
*   Encoder-decoder
*   Prefix-decoder 等结构及其代表模型
*   分别适用于哪些任务
*   为什么现在的大模型都是 decoder-only 结构

### 6\. 解码策略

*   Top-k、top-p、temperature 等参数含义
*   Greedy search、beam search 等解码策略
*   投机解码及其优化算法

第六部分：专家模型 MoE
-------------

*   了解 MoE 结构及其原理
*   训练思路
*   推荐看知乎上关于 MoE 的万字长文介绍

第七部分：RAG & Agent
----------------

*   借助 LangChain 框架学习 RAG 流程
*   了解文档分块，向量模型训练，多种检索策略等关键步骤
*   Agent 的几个框架，如 ReAct，Reflexion 等
*   了解基本思路

第八部分：部署 & 分布式训练 & 推理加速
----------------------

*   Windows、ios、Android 多端部署框架
*   Flash attention，vllm，accelerate 等推理加速框架
*   Deepspeed，Megatron 等分布式训练框架

第九部分：模型评估
---------

*   阅读理解、问答、代码生成、数学等多个维度评估 Benchmark
*   检测 LLM 真实性、流畅度、幻觉等
*   如何利用 LLM 对其他模型/任务做评估

第十部分：其他结构
---------

*   Mamba
*   RWKV 等