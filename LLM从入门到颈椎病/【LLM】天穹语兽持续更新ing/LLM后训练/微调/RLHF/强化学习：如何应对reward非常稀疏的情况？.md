# 强化学习：如何应对reward非常稀疏的情况？
> 作者: 强化学徒
> 
> 原文: [https://www.zhihu.com/question/53667251/answer/3037067648](https://www.zhihu.com/question/53667251/answer/3037067648)  
>   
>  

今天给大家分享的一篇我们最近做的有意思的文章，文章标题是：[D2SR: Transferring Dense Reward Function to Sparse by Network Resetting](https://link.zhihu.com/?target=https%3A//ieeexplore.ieee.org/document/10249999)。

**欢迎大家引用！为了让大家更方便的阅读和理解，我不仅写了这篇博客，还花时间翻译了原文。**

**大家有什么具体问题，也欢迎评论区讨论！**

我逐句翻译校对的中文版：[D2SR\_中文版](https://link.zhihu.com/?target=https%3A//github.com/kaixindelele/DRLib/blob/main/D2SR_RCAR_%25E4%25B8%25AD%25E6%2596%2587%25E7%2589%2588.pdf)

讲解视频在B站：[https://b23.tv/xbq88Dq](https://link.zhihu.com/?target=https%3A//b23.tv/xbq88Dq)

大家一般知道dense奖励学的快，但是很难设计，而且比较容易受到噪声的干扰。而sparse设计简单，对噪声不敏感，但学的慢。

那能不能设计一个方案，兼顾二者优势呢？这个活儿我从19年底，就在尝试了。

我们将从五个部分介绍这篇工作，和大家探讨一下在强化学习中，如何利用不同[奖励函数](https://zhida.zhihu.com/search?content_id=582279457&content_type=Answer&match_order=1&q=%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0&zhida_source=entity)，兼顾二者的优势，实现效率和性能的综合提升。

首先我们简要回顾一下强化学习的基本概念，强化学习一般是有一个agent在环境中通过交互，利用环境的反馈奖励，以最大化累计未来期望回报为目标，调整自身策略，努力获得一个最优策略。

![](4_强化学习：如何应对reward非常稀疏的情况？_image.)

![](10_强化学习：如何应对reward非常稀疏的情况？_image.)

对于我们机器人领域来说，强化学习也取得了很多进展，比如伯克利的[Sergey Levine](https://zhida.zhihu.com/search?content_id=582279457&content_type=Answer&match_order=1&q=Sergey+Levine&zhida_source=entity)教授，他们做的OPT系列工作，第一次将深度强化学习应用到机械臂操作任务当中。

![](18_强化学习：如何应对reward非常稀疏的情况？_image.)

![](19_强化学习：如何应对reward非常稀疏的情况？_image.)

然而DRL在操作任务中仍然存在奖励函数难以设计的问题，一般来说，设计一个dense奖励函数，同时强化学习和机器人领域的专家知识，而且还需要和环境不断的交互调参，才可能最终得到一个好的奖励函数。而这种奖励函数由于对每个数值都需要有准确的反馈评估，当奖励信号存在噪声的时候，比如目标物体的位置信息存在观测误差的情况下，有噪声的奖励值经过[贝尔曼方程](https://zhida.zhihu.com/search?content_id=582279457&content_type=Answer&match_order=1&q=%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B&zhida_source=entity)的放大，就会极大的影响策略的最终性能。

![](22_强化学习：如何应对reward非常稀疏的情况？_image.)

![](3_强化学习：如何应对reward非常稀疏的情况？_image.)

而简单的稀疏奖励函数，只需要考虑最终任务有没有完成，仅需要一个简单的传感器就能判别。但agent在这种奖励函数的引导下，在探索到成功的样本前，它所有的action都是同样负奖励，策略更新时，根本无法获得哪个action更好的信息，因此策略也不会有提升，因而更难探索到成功的奖励。稀疏奖励在前期探索的过程中，经常会面临这种困境。

这两种奖励函数的优缺点是如此的突出和互补。密集奖励函数在早期勘探中具有较高的效率，但设计全局最优密集奖励函数较为困难，且容易受到噪声的影响。另一方面，稀疏奖励函数在早期探索时效率较低，但如果提供有价值的样本，稀疏奖励引导的智能体原则上与任务的一致性更高，对噪声的[鲁棒性](https://zhida.zhihu.com/search?content_id=582279457&content_type=Answer&match_order=1&q=%E9%B2%81%E6%A3%92%E6%80%A7&zhida_source=entity)更强。

**我们能不能设计一个方案，兼顾二者的优势，避免二者的缺点？**

这个非常直观的idea，我从2019年底就开始探索，当时提出来的方案是dense2sparse，在用dense奖励学习一段时间后，直接将奖励函数替换成sparse奖励函数。但只是在简单的有噪声的reach任务中取得了好的效果，在操作任务中，相比dense奖励函数并不能有大幅度的提升。后来尝试了很多种调参方案，都没有取得好的效果，最终将那篇文章发表在[AIM2022](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2003.02740)上，虽然没有做出好的结果，但这个点选的好，到现在还是有7个引用。

这个项目花了我很长的时间，也让我一直在思考，究竟问题出现在哪儿？

直到后来有朋友给我推荐了一篇2022年ICML的文章：The Primacy Bias in Deep Reinforcement Learning。

这篇文章的观点非常有趣，它通过严谨的实验证明，在深度强化学习的学习过程中存在一种primacy bias，即当策略不够好时，探索到的那些数据，用于更新网络，会让网络陷入某种局部最优，且丧失对新的高价值数据的学习能力。这种能力的丧失，仅仅通过将网络的参数重新初始化即可！初始化后，同样的数据，新的网络可以快速的学到更好的策略！

![](15_强化学习：如何应对reward非常稀疏的情况？_image.)

![](7_强化学习：如何应对reward非常稀疏的情况？_image.)

受到这篇文章的启发，我们认为，我们过去的dense2sparse方案，也存在这个问题！

可惜我当时没有尝试重置网络这个操作！错失一篇A！哈哈

在此方案的指导下，我们在dense2sparse的基础上，做了两处修改，一个是在转变的过程中，将网络参数重新随机初始化，另外一个是将经验池中的数据，利用sparse奖励函数重新计算，避免对后面的更新有影响。

![](13_强化学习：如何应对reward非常稀疏的情况？_image.)

![](强化学习：如何应对reward非常稀疏的情况？_image.)

我们这次在经典的多目标操作任务平台，FetchPush和FetchPickAndPlace任务中进行评估，我们设计了四个不同的dense奖励函数，和一个标准的sparse函数。

![](9_强化学习：如何应对reward非常稀疏的情况？_image.)

![](2_强化学习：如何应对reward非常稀疏的情况？_image.)

由于我们的任务是多目标任务，即policy的state space包含了object的goal position。因此我们的backbone算法采用的是DDPG+HER算法，HER算法的原理如图所示，当给定的目标没有完成时，只需要将目标换成achieved goal，就可以获得一个成功轨迹，因此可以极大的提高稀疏奖励任务下的样本效率。

![](16_强化学习：如何应对reward非常稀疏的情况？_image.)

![](6_强化学习：如何应对reward非常稀疏的情况？_image.)

我们首先将D2SR算法和几个标准奖励函数设置进行对比，这里的dense奖励函数我们选择的是D3，即同时考虑了夹爪和物体的距离，也考虑了物体到目标的距离。

![](17_强化学习：如何应对reward非常稀疏的情况？_image.)

![](11_强化学习：如何应对reward非常稀疏的情况？_image.)

三种不同的奖励设置，

有全程使用dense奖励函数的dense设置

有全程使用sparse奖励函数的sparse设置，

以及我们的过去方案dense2sparse，它是在150epoch的学习后，将奖励函数切换成sparse奖励函数。

我们从图中可以看出来，效果非常的amazing，我们的d2sr方案，几乎是瞬间拔高，相比于过去的方案，有轻微的提升之外，D2SR简直可以说是起飞。

更为关键的是在两个任务中，D2SR的性能都是一致的，且在五个随机种子的实验中，可以看出方差极低，足见方案的稳健性。

D2SR方案，相比D2S，多了两步操作，接下来的实验中，我们逐个验证每个操作对性能的影响。

![](12_强化学习：如何应对reward非常稀疏的情况？_image.)

![](8_强化学习：如何应对reward非常稀疏的情况？_image.)

其中D2DS，是为了验证网络重置对dense奖励函数有没有提升，结果表明几乎没有提升。

D2S+sparse reward指的是，在d2s的基础上，更新了经验池中的数据，相比于D2S方案，确实有明显的提升。

D2S+Reset，在D2S的基础上，重置了网络，但没有更新经验池的数据，也有提升，但不大。

而我们的D2SR方案，效果在两个任务中都是最好的。

所有的实验仍然是五个随机种子，实验结果非常理想。

既然D2SR方案如此稳健，那么整个方案对dense奖励的要求高不高，是我们也很好奇的一个事情。

![](23_强化学习：如何应对reward非常稀疏的情况？_image.)

![](5_强化学习：如何应对reward非常稀疏的情况？_image.)

我们在四个奖励函数中做了测试，发现，对每个奖励函数的提升都非常明显。但如果dense奖励函数设计的太差，一点高价值数据都没有采集到，那D2SR的转变也无能为力。

这个实验结果表明，利用我们的方案可以极大的降低对奖励函数设计的要求，只需要原则上符合要求，能够采集到一部分高价值数据，就可以获得很好的样本效率和性能。

最后我们在奖励函数有噪声的情况下做了测试，即dense的奖励信号来源，物体的坐标位置是有误差的，误差范围在1cm范围之内随机采样。

![](20_强化学习：如何应对reward非常稀疏的情况？_image.)

![](1_强化学习：如何应对reward非常稀疏的情况？_image.)

实验结果表明，我们的方案，仍然具有非常高的性能，和没有噪声的设置几乎一致。而D2S方案就几乎没有提升。

最后我们还发现了一个非常有意思的结果，就是HER原文中，提到他们的实验结果中，最好的dense奖励函数设计，都没有sparse奖励函数的样本效率高。

![](14_强化学习：如何应对reward非常稀疏的情况？_image.)

![](21_强化学习：如何应对reward非常稀疏的情况？_image.)

而我们的实验却有不一样的结果。

他们展示的最好的结果是我们的D1 设置，而我们的实验结果表明，D2设置远好于D1.在他们原文的脚注中，他们说他们也尝试过将夹爪和物体的距离纳入奖励函数中，但他们没有获得成功的训练结果，而我们的结果显示，D2至少可以让FetchPush可以很快的学会。

2017年NIPS非常经典且高价值的文章，也有类似的细节可以挖掘，也是非常有意思的事情。

最后我们做出如下总结：

首先，本文介绍了一种简洁高效的奖励函数转换方法，D2SR，它结合了密集和稀疏奖励函数的优点。

其次，D2SR可以有效突破原有dense奖励函数的限制，解决固定sparse奖励函数带来的探索困境。

然后我们发现， D2SR可以极大的降低对dense奖励函数的设计要求。

最后我们验证了D2SR可以利用稀疏奖励函数的鲁棒性，从而在具有噪声奖励的任务中获得稳定和改进的性能。

总之，通过一系列有意思的实验，**我们为大家提供了一个新的利用多种奖励函数的方案**。

后面还有一些工作需要补充，比如说，到底什么时候转换？对sparse奖励有什么要求？

谢谢大家，希望这样一篇有意思的工作能对大家有帮助，欢迎大家点赞和评论区讨论。

这篇文章目前是被RCAR2023会议接收，后面等正式录用，也欢迎大家引用。

最最最后，我博士期间一直在做的工作是强化学习在稀疏奖励下的高效学习，这也是一个非常有意思的领域，欢迎大家一起讨论学习