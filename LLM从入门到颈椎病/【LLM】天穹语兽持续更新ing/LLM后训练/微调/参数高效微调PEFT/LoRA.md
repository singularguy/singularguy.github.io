# LoRA
**LoRA 的核心假设:** 增量矩阵是低秩的，既然是低秩的，就可能包含了很多冗余信息，所以可以用两个矩阵对增量矩阵做低秩近似，去除冗余信息。 对于任何矩阵，低秩近似的一个最大好处就是参数量大幅降低，这也是 LoRA 可以实现“参数高效”的主要原因。但是，低秩+近似的副作用是可能带来**效果损失**。当然，从论文实验结果来看，似乎并没有多大的损失，这可能是因为有针对性地选择了数据集，也可能是因为在去除冗余信息的同时去除了一些噪声。 如果训练数据很多，增量矩阵很可能不再是低秩的，这时全量微调效果理论上应该优于 LoRA。

*   [说透 Lora](https://mp.weixin.qq.com/s/4_R3hmFeNATDtj6L3P7HFA) 大模型微调技术进展与方法汇总（不能错过的论文）
*   [https://zhuanlan.zhihu.com/p/653497613](https://zhuanlan.zhihu.com/p/653497613)