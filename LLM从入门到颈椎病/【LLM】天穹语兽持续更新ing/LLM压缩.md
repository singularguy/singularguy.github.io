# LLM压缩
> 近年来，随着Transformer、MOE架构的提出，使得深度学习模型轻松突破上万亿规模参数，从而导致模型变得越来越大，因此，我们需要一些大模型压缩技术来降低模型部署的成本，并提升模型的推理性能。 模型压缩主要分为如下几类：

*   模型剪枝（Pruning）
*   知识蒸馏（Knowledge Distillation）
*   模型量化（Quantization）
*   低秩分解（Low-Rank Factorization）