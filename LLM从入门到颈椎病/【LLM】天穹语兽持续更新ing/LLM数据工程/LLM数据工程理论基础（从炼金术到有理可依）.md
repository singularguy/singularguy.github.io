# LLM数据工程理论基础（从炼金术到有理可依）
> **原文:** [**https://yaofu.notion.site/An-Initial-Exploration-of-Theoretical-Support-for-Language-Model-Data-Engineering-Part-1-Pretraini-dc480d9bf7ff4659afd8c9fb738086eb#4646e4fbc81c4e018c395f24cafaf2bd**](https://yaofu.notion.site/An-Initial-Exploration-of-Theoretical-Support-for-Language-Model-Data-Engineering-Part-1-Pretraini-dc480d9bf7ff4659afd8c9fb738086eb#4646e4fbc81c4e018c395f24cafaf2bd)
> 
> **翻译:**[**https://mp.weixin.qq.com/s/iXrU-TeBGSHqPCjdq1UCYA**](https://mp.weixin.qq.com/s/iXrU-TeBGSHqPCjdq1UCYA)

近年来,研究和开源社区的焦点逐渐从模型工程转向数据工程,意识到据质量的重要性. \*\*然而,当我们说“我们想要更好的数据”时,“更好的数据”到底意味着什么？当我们说“我们优化数据构成”时,我们优化的目标是什么？ \*\*

\*\*我们想研究语言模型数据工程的理论支持. 我们相信,对问题的深入理解与开发解决问题的方法同样重要,理论分析将引导我们实现可预测的扩展: 在实际进行实验之前预测每项任务的最终性能. \*\*

在这篇文章中,我们汇总了最近关于数据工程的见解,并给出了数据优化的问题表述——也就是说,我们不提出优化数据的具体方法,但我们讨论优化数据时应该解决的问题是什么,以及指导我们的基本原则. 具体来说,我们讨论预训练和 SFT 数据优化的以下目标:

*   \*\*预训练数据优化: \*\*找到最佳的数据混合比例+数据格式+数据课程(mix ratio + data format + data curriculum ),使学习速度最大化.
*   \*\*有监督微调/指令调优数据优化: \*\*找到查询-响应对的最小组合(minimal combination of query-response pairs),使其对用户偏好分布的覆盖范围最大化.
*   \*\*可预测的扩展(predictable scaling): \*\*找到缩放法则,其中给定输入为预训练数据混合 + 模型大小(scale) + 微调数据混合 + 训练算法,以便可以在所有实验之前预测预训练损失、下游性能、人类偏好.

结合预训练(pretraining)和微调(finetuning)的见解,我们将进一步找到连接两者的方法以实现可预测的扩展(predictable scaling),也就是说,给定预训练和微调数据,给定模型架构和训练超参数,我们希望在执行之前预测所有结果实验.

1 预训练数据优化问题
-----------

首先澄清,虽然最终的下一个单词预测损失通常用于测量预训练,但这并不是我们优化预训练数据混合时的目标. 我们不是直接优化最终的损失,而是优化损失减少的速度——我们希望学习曲线的导数更大,从而曲线更陡. \*\*为了衡量学习速度,我们不使用下一个单词预测损失(考虑到它与无损压缩的联系,信息量很大),而是考虑摸索速度(grokking-模型获得特定技能的速度)可能是一个不错的替代指标. \*\*稍后我们将回到摸索速度(grokking)和下一个单词预测损失的联系.

2 使用摸索速度( speed of grokking )作为测量
---------------------------------

2.1 什么是摸索(grokking)
-------------------

![](2_LLM数据工程理论基础（从炼金术到有理可依）_image.j)  
摸索是指在训练的某个步骤,模型突然学会了一项技能,从记忆过渡到泛化

摸索(grokking)是指在训练的某个步骤,模型突然学会了一项技能,从记忆过渡到泛化. 对于特定的“技能”,例如模加法,我们引用 **Pearce et. al. 2023**的典型学习曲线. 我们可以看到,在训练开始时,模型记忆了训练数据,测试性能没有变化. 随着训练的进行,从第 35k 步到第 45k 步有一个相变期,模型突然从记忆过渡到泛化,在测试集上显示出 100% 的准确率. 学习过程中的这种阶段性变化被称为“grokking”.

2.2 摸索(grokking)速度
------------------

![](5_LLM数据工程理论基础（从炼金术到有理可依）_image.j)不同超参数下,模型具有不同的摸索速度.

假设我有两个超参数设置,设置1时模型在1000个优化步骤后达到90+测试精度,设置2时模型需要2000步,那么我们说超参数设置1给模型提供了比设置更快的速度2.

正如我们在上图中看到的,我们引用了原始的grokking论文(Power et. al. 2022),多种因素影响摸索(grokking)的速度: 权重衰减使得grokking时间更早(从而速度更快),而太小/太大学习率使得摸索时间更晚(因此速度更慢)——同样,这里的目标不是优化最终性能,目标是优化速度,即性能曲线的导数.

\*\*理想情况下,我们想要一个陡峭的学习曲线,使摸索速度最大化. \*\*

2.3 单一技能摸索、聚合技能和下游表现
--------------------

我们注意到存在技能粒度的概念. 例如:

*   单一技能: 如两位数加法
*   聚合技能: 一位数加法+两位数加法+两位数减法+……
*   下游表现: GSM8k 数学作业题表现

通常,一项任务需要多种技能的结合. 假设下游任务是 GSM8k 式的数学推理,那么所需的技能可能是两位数加法、两位数减法、理解数字之间的顺序(如 1 小于 2)、理解 1 周意味着 7 天等. ——这些技能在训练过程中可能会有不同的摸索点(这也解释了为什么需要训练模型足够长的时间),只有将它们全部结合起来,模型才能进行 GSM8k 推理.

2.4 聚合摸索(grokking)的具体示例
-----------------------

![](6_LLM数据工程理论基础（从炼金术到有理可依）_image.j)左: 语言技能列表；右: 这些技能在训练期间出现的位置. 加快摸索意味着我们希望将这些点向左移动——以使这些技能在早期训练阶段出现.

上图是来自Evanson et. al. 2023的研究,左边部分是模型获得的特定语言技能的列表,右边部分显示了在训练过程中何时获得这些技能. 显然,不同的技能有不同的学习速度,并且出现的时间也不同. 在这种情况下,加快摸索意味着我们想要向左移动点——这意味着我们可以在训练过程中更早地观察它们(这样我们就可以在后期训练阶段为更高级的技能留出“空间”).

\*\*一个重要的推论是中间模型检查点对于研究学习动态非常重要. 我们呼吁现有模型开源中间检查点！ \*\*

3 影响学习速度的数据因素
-------------

从上面的部分可以看出,摸索(grokking)似乎不是使用下一个单词预测损失作为衡量标准,而是更接近泛化,并且可以更好地与特定的技能集联系起来,因此是一个很好的替代学习指标. 在本节中,我们讨论观察到的影响学习速度的数据因素,重点关注数据格式、混合和课程.

3.1 数据格式
--------

![](4_LLM数据工程理论基础（从炼金术到有理可依）_image.j)不同的数据格式会带来不同的学习速度. 另请注意,它们是相同的数据 相同的方程相同的答案,只是格式不同.

在我们之前的工作中,已经证明,对思维链格式化数据进行训练可以赋予模型 CoT 推理能力. 在上图中,我们进一步可以看到,CoT/暂存器(scratchpad)数据有不同的格式,并且暂存器(scratchpad)越详细,模型学习的速度就越快.

\*\*注意细微差别,因为细节决定成败: \*\*

*   \*\*模型可以从所有格式的 CoT 中学习——最终的结果是相似的,并不是模型不能学习——只是速度不同,模型对于某些格式学习得更快. \*\*
*   数据的内容/含义/语义是相同的——它们是相同的三位数加法问题. 这是 CoT 的格式不同——一种比另一种更详细.

3.2 数据课程
--------

要理解数据课程(data curriculum),请注意以下几点:

*   我们希望向模型传授许多技能,例如技能 1 到 3.
*   对于不同的技能,存在不同的训练数据源可以帮助该技能(例如,MMLU 类型问题的网页、编码问题的 GitHub).

\*\*假设我们想要教授模型文本和代码能力,我们有 10B 文本和 10B 代码,我们的计算只允许我们训练 10B 数据. 我们希望最大化编码能力. 以下是三种可能的解决方案: \*\*

*   方法1(仅限代码): 直接馈送10B代码数据
*   方法2(均匀混合): 将5B文本和5B代码数据均匀混合,然后将它们同时输入模型
*   方法3(数据课程): 先输入5B文本,然后输入5B代码

哪一个能表现得最好？

*   \*\*如果模型从文本数据中学习的技能对代码数据没有帮助,那么我们可以直接执行方法1,仅代码,就像StarCoder和AlphaCode的情况一样. \*\*
*   **如果模型从文本数据中学习的技能可以转移到代码数据中,那么我们可能想做方法2,均匀混合**
*   \*\*如果学习代码技能需要模型先有文本技能,也就是说文本和代码之间有依赖关系,并且文本必须先有,那么我们需要做方法3,数据课程. Codex 和 CodeLLaMA 就是这种情况(尽管他们可能不是故意选择这样做的). \*\*

但哪种设置可能是预训练的“正确方法”？我们注意到 Chen et. al. 2023 的以下观察结果:

![](LLM数据工程理论基础（从炼金术到有理可依）_image.j)  
不同的数据源会产生不同的技能. 对特定数据顺序进行训练可以比对特定技能数据进行训练提供更快的学习速度.

在这里,作者表明,\*\*在合成和简化的设置中,学习数据课程安排技能 1、2、3 的数据可以实现对技能 3 的数据进行更快的学习速度. \*\*

\*\*  
\*\*

3.3 混合比例
--------

混合比是指我们如何对不同来源的数据进行加权. 例如,LLaMA 使用以下权重:

![](3_LLM数据工程理论基础（从炼金术到有理可依）_image.j)  
LLaMA 数据混合比. 这个比率降低了 Github 和 StackExchange 等代码数据的权重,也降低了 Arxiv 等论文数据的权重.

显然 LLaMA 降低的数据权重:

*   代码数据类似于Github,因此LLaMA的编码性能不是很高. 相比之下,主要在代码上进行训练的 starcoder 在编码任务上表现良好.
*   像Arxiv这样的论文数据,因此LLaMA的科学推理似乎不是很高. 相比之下,主要以论文训练的Galactica在科学推理方面表现良好.

不同的混合比例可能会导致不同的学习速度,正如 DoReMi 论文所示:

![](1_LLM数据工程理论基础（从炼金术到有理可依）_image.j)不同的数据混合比例提高了学习速度.

正如Xie et. al. 2023表明,更好的数据混合比例可以通过更高的学习曲线提高模型的性能,使模型更快地从数据中学习.

3.4 模型缩放
--------

尽管数据格式/课程/混合比例对于学习速度很重要,但一个非常重要的警告是模型缩放,因为大模型比小模型学习得快得多. 小于30B模型规模的数据工程结果无法迁移到大于70B的模型是很常见的. 这里的一个例子是我们验证代码数据是否真正提高了推理能力,我们观察到:

代码数据似乎有所改善:

*   符号推理,如 BABI 和 BBH-Algorithmic
*   符号数据和语言数据之间的翻译,例如结构到文本或文本到sql

**但是,上述观察仅发生在 7B 模型上** 当模型为 70B时,我们无法观察到 BBH 算法的改进.

此外,代码数据无法改进:

*   自然语言推理,如 BBH-Language
*   数学推理,如 GSM8K

请注意,代码数据仅改进了7B模型的BBH-Algo,而没有改进70B. 构建语言模型的各个地方也做出了类似的观察. \*\*事实上,我有点担心数据工程的一些子方向可能会被论文出版物淹没,这些论文出版物发出很大的声音,但无法推广到更大的模型规模. \*\*

* * *

因此,**所有来自数据格式/课程/混合比例的改进可能很容易被模型和数据扩展所淹没——它们仅适用于小模型和简单任务,当模型/数据变大时,可能会变得毫无用处**\*\*. 如果是这样的话,我们应该把更多的精力放在扎实的数据清理上,而不是无用的花哨课程上. \*\*我们不确定情况是否如此,但我们希望不是.

4 关于衡量学习速度的进一步讨论
----------------

在本节中,我们讨论不同粒度的技能及其学习曲线,我们还考虑如何衡量**单一技能指标和损失**之外的学习速度.

4.1 从微观层面的技能到宏观层面的缩放曲线
----------------------

假设在训练过程中,模型从小到大学习多粒度的技能,那么我们可以从以下几个角度考虑技能:

*   微观层面: 从相关文本中快速获得的单个微小技能,可能无法解释.
*   中观层面: 可解释的技能,由微观技能聚合而成,并成为人类可解释的技能,例如: 三位数加法.
*   宏观层面: 继续聚合中观层面的技能,最终显示出统计行为,特别是我们可以观察到的
    *   幂律(对数线性缩放曲线): 随着模型逐渐积累微观和中观层面的技能,整体宏观层面的技能平稳提升
    *   涌现能力(相变缩放曲线): 一些宏观技能需要中观技能通过一定的门槛. 例如,多步推理能力要求模型具有一定的单步准确率,比如5步推理,如果单步准确率为95%,那么累计准确率为77%,而80% %单步准确率只有32%的累积准确率,因为误差呈指数级增长.

\*\*上述观点与统计物理学非常相似: 在微观层面(例如气体分子),事物很难精确建模和预测,但如果我们将许多微观碎片收集到宏观层面(一罐理想气体),我们可以统计地描述它们的宏观特性. \*\*

\*\*  
\*\*

4.2 将单技能摸索(grokking)聚合为缩放法则和涌现能力
--------------------------------

![](7_LLM数据工程理论基础（从炼金术到有理可依）_image.j)不同技能的聚合曲线会产生总体损失曲线.

当我最初读到“量子模型”时,我觉得这又是一派胡言. 通常在人工智能中,任何以“量子”命名的东西要么非常有洞察力,要么完全是胡说八道. 乍一看,我觉得这篇论文可能非常有洞察力.

如上面的曲线所示,我们看到:

*   单技能(“量子”)曲线通常表现出相变形状.
*   模型以不同的速度学习不同的技能.
*   当将多种技能聚合在一起时,我们获得平滑的对数线性形状损失曲线.

4.3 我们可能想要损失和单技能准确性之外的东西
------------------------

我们认为:

*   损失很好,它是可预测的,与整体性能非常一致,并且可以解释为压缩比. 唯一的问题是损失不能直接转化为下游任务性能(我们的目标是在下一篇博客文章中使用迁移缩放定律来弥补这一点).
*   单技能准确度(无论是中观还是宏观层面)很好,但它只衡量单技能.
*   我们能否找到一些衡量介于两者之间的指标？
    *   与能力方向很好地结合(比如推理)
    *   可以从第一原理推导出来(如信息论)
    *   衡量我们与“真正的生成过程”的接近程度,例如某种相互信息
*   我不确定是否存在这样一个更好的指标,但非常愿意探索.

5 无损压缩、柯尔莫哥洛夫(Kolmogorov)复杂性和生成过程
---------------------------------

在本节中,我们解释模型之所以能够学习(进行“摸索”并表现出涌现能力)是因为模型学习了真正的生成过程. 我们从无损压缩的角度解释这一点.

最近,有一些讨论**将预训练过程解释为训练数据的无损压缩**. 我发现这个观点非常鼓舞人心. 具体来说:

*   训练语言模型相当于无损压缩训练数据.
*   请注意,压缩是无损的——有人认为模型会记住训练数据,而模型权重是数据的有损压缩. 压缩观点倾向于认为\[模型权重是有损压缩\]是错误的.
*   **损失曲线的积分**——损失曲线与x轴(以数据为索引)之间的面积,可以解释为算术编码下的压缩比.
*   模型压缩数据的效果越好,损失越低,模型就越有可能了解数据的底层生成过程.
*   柯尔莫哥洛夫(Kolmogorov)复杂度是生成数据的生成器的最小描述长度. 该语言模型近似于柯尔莫哥洛夫压缩器,即**生成数据(人类语言)的最短(随机)算法**.
*   **模型权重编码的不是数据,而是人类语言的底层生成过程(它是一个随机过程)**(大脑中关于如何生成语言的过程).
*   模型权重是无损压缩的副产品. 它编码了人类语言的生成过程. 我们将这种生成过程称为“智能”. 从这个角度来看,智力是人类大脑中产生人类语言的生成过程.

5.1 随机数生成器示例
------------

为了了解为什么无损压缩揭示了真实的生成过程,我们给出随机数的无损压缩示例:

*   假设您有一个用 1000 行代码编写的随机数生成算法. 例如,Python随机数生成算法.
*   假设您将随机数生成算法和初始随机种子存储在文件中. 这个文件并没有那么大. 例如,Python 随机数生成算法需要 31.4 kb 磁盘内存.
*   假设您使用此算法生成 100B 伪随机数.
*   你将这100B随机数发送给朋友并要求朋友对其进行压缩.
*   在不知道底层生成过程(算法)的情况下,如果您的朋友使用一些常见的压缩软件(例如 gzip),他们最终可能会得到 50B 或类似缩放的文件,压缩率很低.
*   然而,如果你的朋友以某种方式弄清楚了随机数生成算法和初始种子,**只需存储算法和种子只需要 31.4kb 磁盘内存,这是极高的压缩比**.
*   大型神经网络及其学习算法,类似于上述过程 使用大量观察来恢复底层生成过程(数据生成算法). 更高的压缩比/训练损失越底,他们恢复潜在生成过程的可能性就越大.

这听起来很神奇,但\*\*训练大语言模型确实感觉就像从大量生成的随机数中恢复随机数生成算法. \*\*

\*\*  
\*\*

5.2 柯尔莫哥洛夫(Kolmogorov)复杂度
-------------------------

柯尔莫哥洛夫复杂度是指生成数据的算法的最小长度. 从无损压缩的角度来看,语言模型被视为(不可计算的)柯尔莫哥洛夫压缩器的近似.

也就是说,**语言模型的权重可能编码了一个生成过程**,即生成其训练数据的算法. 请注意,**模型的权重不是对数据进行编码,而是对生成数据的程序进行编码,并优化权重,将其推向生成数据的最短程序**,这正是 Kolmogorov 压缩器的定义. 实际上,已经有一些工作展示了 RNN 和 Transformer 如何编码 CFG(生成上下文无关语言的生成过程).

6 总结以及与指令调整(instruction tuning)的联系
----------------------------------

在这篇文章中,我们回顾了语言模型学习动态的多项实证观察(**摸索、对数线性缩放定律、涌现能力**)、影响学习速度的数据因素(**数据格式、混合比例和课程**).

我们讨论了经验现象的可能解释,并将这些观察结果与最近的无损压缩理论联系起来.

我们的最终目标是建立一种理论并指导我们做数据(以及其他重要的学习因素),以便我们可以在实验前预测每项任务的最终表现(而不仅仅是预训练损失),而无需编写一行代码.

已经存在预测损失的缩放定律,我们上面讨论的理论进一步考虑了**摸索(grokking)和涌现能力**.

我们进一步解释说,模型之所以能够学习,是因为模型学习在无损压缩的角度下近似真实的生成过程/柯尔莫哥洛夫压缩器.

\*\*OpenAI 报道称,在 GPT-4 的开发过程中,他们在实验前预测了 HumanEval 的性能. 我们相信这种预测可以通过一个未知定理来统一,该定理能够预先预测所有下游性能. \*\*

\*\*通过探索和建立这样的定理,我们可以将深度学习和语言建模从炼金术转向经验科学. \*\*

\*\*  
\*\*

**附录: 缩放法则(Scaling Laws)**

*   Hestness et. al. Dec 2017. Deep Learning Scaling is Predictable, Empirically
    *   Baidu’s pioneer study on scaling law of neural networks
*   Kaplan et. al. Jan 2020. Scaling Laws for Neural Language Models
    *   Initial OpenAI LM scaling law
*   Henighan et. al. Nov 2020. Scaling Laws for Autoregressive Generative Modeling
    *   Extension of the initial OpenAI LM scaling law to multimodal
*   Hernandez et. al. Feb 2021. Scaling Laws for Transfer
    *   OpenAI scaling law for SFT
*   Bahri et. a. Feb 2021. Expythoning Neural Scaling Laws
*   Clark et. al. Feb 2022. Unified Scaling Laws for Routed Language Models
    *   DeepMind scaling law for MoE
*   Hoffmann et. al. Mar 2022. Training Compute-Optimal Large Language Models
    *   DeepMind Chinchilla scaling law
*   Gao et. al. Oct 2022. Scaling Laws for Reward Model Overoptimization
    *   OpenAI scaling law for RL
*   Aghajanyan et. al. Jan 2023. Scaling Laws for Generative Mixed-Modal Language Models
    *   Meta FAIR Chinchilla scaling law for multimodal