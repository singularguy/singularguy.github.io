<html>
  
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../../style.css">
    <base target="_parent">
    <title data-trilium-title>多模态-核心模型</title>
  </head>
  
  <body>
    <div class="content">
       <h1 data-trilium-h1>多模态-核心模型</h1>

      <div class="ck-content">
        <hr />
        
<h2>created: 2025-01-25T00:41
updated: 2025-01-25T13:25</h2>

        
<h2>目录</h2>

        <ul>
          <li><a href="#1.BLIP%E7%9A%84%E5%8E%9F%E7%90%86?">1.BLIP的原理?</a>
          </li>
          <li><a href="#2.CLIP%E7%9A%84%E5%8E%9F%E7%90%86?">2.CLIP的原理?</a>
          </li>
          <li><a href="#3.%E4%B8%BA%E4%BB%80%E4%B9%88StableDiffusion%E4%BD%BF%E7%94%A8CLIP%E8%80%8C%E4%B8%8D%E4%BD%BF%E7%94%A8BLIP?">3.为什么StableDiffusion使用CLIP而不使用BLIP?</a>
          </li>
          <li><a href="#4.BLIP2%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%89%E5%93%AA%E4%BA%9B%E5%88%9B%E6%96%B0%E7%82%B9?">4.BLIP2的工作有哪些创新点?</a>
          </li>
          <li><a href="#5.%E8%A7%A3%E9%87%8A%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%8C%E5%B9%B6%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E%E5%85%B6%E5%9C%A8%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8?">5.解释自监督学习中对比学习的原理，并举例说明其在图像特征提取中的应用?</a>
          </li>
          <li><a href="#6.%E8%AF%B4%E6%98%8EBLIP-2%E7%9A%84%E6%9F%A5%E8%AF%A2Transformer%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%A8%A1%E6%80%81%E5%B7%AE%E8%B7%9D%E9%97%AE%E9%A2%98?">6.说明BLIP-2的查询Transformer如何解决模态差距问题?</a>
          </li>
          <li><a href="#7.LLaMA-AdapterV2%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%97%A9%E6%9C%9F%E8%9E%8D%E5%90%88%E7%AD%96%E7%95%A5%E6%8F%90%E9%AB%98%E8%A7%86%E8%A7%89%E6%8C%87%E4%BB%A4%E8%B7%9F%E9%9A%8F%E8%83%BD%E5%8A%9B?">7.LLaMA-AdapterV2如何通过早期融合策略提高视觉指令跟随能力?</a>
          </li>
          <li><a href="#8.SAM%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%8F%AF%E6%8F%90%E7%A4%BA%E7%9A%84%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E5%AE%9E%E7%8E%B0%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B?">8.SAM如何通过可提示的分割任务实现强大的泛化能力?</a>
          </li>
          <li><a href="#9.PaLM-E%E5%A6%82%E4%BD%95%E5%B0%86%E8%BF%9E%E7%BB%AD%E4%BC%A0%E6%84%9F%E5%99%A8%E6%A8%A1%E6%80%81%E7%9B%B4%E6%8E%A5%E8%9E%8D%E5%85%A5%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%EF%BC%8C%E5%AE%9E%E7%8E%B0%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD?">9.PaLM-E如何将连续传感器模态直接融入语言模型中，实现具身智能?</a>
          </li>
        </ul>
        
<h2>1.BLIP的原理?</h2>

        <p>BLIP是一种统一视觉语言理解和生成的预训练模型。BLIP的特点在于它采用了一种编码器-解码器混合架构（MED)，并且引入了CapFilt机制来提高数据质量和模型性能。BLIP的主要组成部分包括：</p>
        <ol>
          <li>MED架构：包括单模态编码器、图像引导的文本编码器和图像引导的文本解码器，这使得BLIP能够同时处理理解和生成任务。</li>
          <li>预训练目标：BLIP在预训练期间联合优化了三个目标，包括图文对比学习、图文匹配和图像条件语言建模。</li>
          <li>CapFilt机制：包括Captioner和Filter两个模块，Captioner用于生成图像的文本描述，而Filter用于从生成的描述中去除噪声，从而提高数据集的质量。</li>
        </ol>
        <p>
          <img src="api/images/mFuC7gdCKwzt/BLIP.png" />
        </p>
        
<h2>2.CLIP的原理?</h2>

        <p>CLIP是由OpenAI提出的一种多模态预训练模型，它通过对比学习的方式，使用大规模的图像和文本数据对来进行预训练。CLIP模型包括两个主要部分：</p>
        <p>Text Encoder：用于提取文本的特征，通常采用基于Transformer的模型。</p>
        <p>Image Encoder：用于提取图像的特征，可以采用CNN或基于Transformer的Vision Transformer。
          <img
          src="api/images/fjHwZxd1q9hw/CLIP.png" />CLIP的训练过程涉及将文本特征和图像特征进行对比学习，使得模型能够学习到文本和图像之间的匹配关系。CLIP能够实现zero-shot分类，即在没有特定任务的训练数据的情况下，通过对图像进行分类预测其对应的文本描述。</p>
        
<h2>3.为什么StableDiffusion使用CLIP而不使用BLIP? </h2>

        <p>CLIP是通过对比学习的方式训练图像和文本的编码器，使得图像和文本之间的语义空间能够对齐。CLIP的架构和训练方式可能更适合Stable Diffusion模型的目标，即生成与文本描述相匹配的高质量图像。</p>
        <p>BLIP由于其图像特征受到了图文匹配（ITM)和图像条件语言建模(LM)的影响，可以理解为其图像特征和文本特征在语义空间不算对齐的。</p>
        <p>最大区别：损失函数，CLIP和BLIP针对任务不同，不同任务不同损失函数。</p>
        
<h2>4.BLIP2的工作有哪些创新点?</h2>

        <p>BLIP-2 使用 Q-Former 作为可训练的模块，用于连接冻结的图像编码器和冻结的 LLM。它从图像编码器中提取固定数量的输出特征，这些特征与输入图像的分辨率无关。Q-Former
          由两个 Transformer 子模块组成，它们共享相同的自注意力层。</p>
        <ul>
          <li>（1）图像 Transformer 与冻结的图像编码器进行交互，进行视觉特征提取。</li>
          <li>（2）文本 Transformer 既可以作为文本编码器，也可以作为文本解码器。</li>
        </ul>
        <p>BLIP-2 创建了一组可学习的输入到图像 Transformer 的查询嵌入。查询通过自注意力层交互，并通过交叉注意力层（每隔一个 Transformer
          块插入一个）与冻结的图像特征进行交互，还可以通过相同的自注意力层与文本进行交互。根据不同的预训练任务， BLIP-2 应用不同的自注意力掩码控制查询-文本交互。</p>
        <p>
          <img src="api/images/LyAFS5rdx7fN/BLIP2-1.png" />
        </p>
        <p>在生成学习阶段， BLIP-2 将带有冻结的图像编码器的 Q-Former 连接到冻结的 LLM，以利用 LLM 的生成能力。BLIP-2
          先使用全连接层将输出查询表示 Z 线性投影到与 LLM 的文本表示相同的维度。然后，在输入文本表示之前添加投影的查询表示。它们作为软性的视觉提示，将
          LLM 置于由 Q-Former 提取的视觉特征上。由于 Q-Former 已经被预训练以提取语言信息的视觉特征，有效地充当了信息瓶颈，馈送最有用的信息给
          LLM，同时删除不相关的视觉信息。这减轻了 LLM 学习视觉-语言对齐的负担，缓解了灾难性遗忘问题。</p>
        <p>对于基于解码器的 LLM， BLIP-2 使用语言建模损失进行预训练，冻结 LLM 的任务是在Q-Former 提取的视觉特征的条件下生成文本。对于基于编码器-解码器的
          LLM， BLIP-2 使用前缀语言建模损失进行预训练，将文本分为两部分：前缀文本与视觉特征连接在一起，作为 LLM编码器的输入；后缀文本作为
          LLM 解码器的生成目标。</p>
        <p>
          <img src="api/images/YJ5v731IGVSU/BLIP2-2.png" />
        </p>
        
<h2>5.解释自监督学习中对比学习的原理，并举例说明其在图像特征提取中的应用?</h2>

        <p>对比学习的原理，其核心在于通过优化一个目标函数，来促使模型学会区分数据中的相似与不同。具体来说，这一过程涉及两个关键步骤：首先是最大化正样本间的相似度，正样本通常指的是来自同一数据点的不同视角或变换，例如，在图像领域，可以通过对同一张图片进行不同的裁剪、旋转或颜色调整来生成正样本。这样做的好处是，模型能够学会忽略那些不重要的变化，专注于学习数据本质的特征。</p>
        <p>接下来是最小化负样本间的相似度。负样本通常是指来自不同数据点的样本。在对比学习中，通过确保模型能够区分这些不同的数据点，模型能够学会为每个数据点生成独特的特征表示。这种方法的有效性在于，它迫使模型去关注数据中的关键差异，而不是表面的、无关紧要的变化。</p>
        <p>以图像特征提取为例，对比学习特别有效。在这一领域，对比学习通过将图像与其经过增强的版本视为正样本，而将其他图像的增强版本视为负样本，来训练模型。这种方法使得模型能够学习到具有高度区分度的特征表示。例如，SimCLR（Simple
          Contrastive Learning of Representations）模型是一个典型的对比学习框架。它首先使用数据增强技术生成图像的多个副本，然后通过对比损失函数来训练模型，使模型能够学会区分不同的图像。</p>
        
<h2>6.说明解释BLIP-2的查询Transformer如何解决模态差距问题?</h2>

        <p>BLIP-2（Bridging the Language-Image Pre-training Gap）的查询Transformer是一种多模态预训练模型，旨在解决语言和图像之间的模态差距问题。该模型通过结合视觉和文本信息来提高跨模态的理解和生成能力。以下是BLIP-2的查询Transformer如何解决模态差距问题的详细解释：</p>
        <p>(1). 多模态预训练 BLIP-2采用多模态预训练的方法，通过同时处理图像和文本数据来学习跨模态的表示。具体来说，模型在预训练阶段使用大规模的图像-文本对，通过对比学习和生成任务来学习通用的跨模态表示。</p>
        <p>(2). 查询Transformer架构 BLIP-2的核心是一个查询Transformer模型，该模型结合了视觉和文本信息。查询Transformer的架构允许模型在处理图像和文本时动态地关注相关的信息。</p>
        <p>(2.1) 视觉编码器 视觉编码器负责将图像转换为视觉特征向量。BLIP-2使用预训练的视觉模型（如ResNet或ViT）来提取图像特征。</p>
        <p>(2.2) 文本编码器 文本编码器负责将文本转换为文本特征向量。BLIP-2使用BERT或RoBERTa等预训练的语言模型来提取文本特征。</p>
        <p>(2.3) 查询Transformer 查询Transformer结合了视觉和文本特征，通过自注意力机制和跨模态注意力机制来动态地关注相关的信息。具体来说，模型在处理每个查询时，可以同时考虑图像和文本中的相关信息，从而减少模态之间的差距。</p>
        <p>(3). 对比学习 BLIP-2使用对比学习来进一步减少模态差距。具体来说，模型通过比较正样本（匹配的图像-文本对）和负样本（不匹配的图像-文本对）来学习跨模态的表示。这种方法有助于模型学习到更鲁棒的跨模态特征。</p>
        <p>(4). 生成任务 BLIP-2还包括生成任务，如图像描述生成和文本条件下的图像生成。这些生成任务有助于模型更好地理解图像和文本之间的关系，从而进一步减少模态差距。</p>
        <p>(5). 解决模态差距的具体方法</p>
        <ul>
          <li><strong>跨模态注意力机制</strong>：查询Transformer通过跨模态注意力机制，使得模型在处理图像和文本时可以动态地关注相关的信息，从而减少模态之间的差距。</li>
          <li><strong>对比学习</strong>：通过比较正样本和负样本，模型学习到更鲁棒的跨模态特征，从而减少模态差距。</li>
          <li><strong>生成任务</strong>：生成任务帮助模型更好地理解图像和文本之间的关系，从而进一步减少模态差距。</li>
        </ul>
        
<h2>7.LLaMA-AdapterV2如何通过早期融合策略提高视觉指令跟随能力?</h2>

        <p>早期融合策略 ●策略描述：早期融合策略指的是在模型的早期层就融合视觉信息，这样可以在模型的训练过程中充分利用视觉知识，提高对视觉指令的理解和响应能力。
          ●对视觉指令跟随能力的影响：通过在模型的早期层融合视觉信息，LLaMA-Adapter V2 能够更有效地处理视觉指令，从而提高其视觉指令跟随能力。
          LLaMA-Adapter V2 的其他改进 ●偏差调整：通过解锁更多可学习的参数，如范数、偏差和比例，来增强 LLaMA Adapter 的性能，这些参数将指令遵循能力分布到整个
          LLaMA 模型中。 ●联合训练范式：优化不相交的可学习参数组，引入图像-文本对和指令跟随数据的联合训练范式，缓解图文对齐和指令跟随任务之间的干扰。</p>
        
<h2>8.SAM如何通过可提示的分割任务实现强大的泛化能力?</h2>

        <p>SAM（Segment Anything Model） 是一种基于提示的可分割模型，它通过以下方式实现强大的泛化能力： ●通用掩码预测器：SAM
          采用一个通用的掩码预测器，可以处理任何类别的分割任务，而不需要为每个类别单独训练模型。 ●提示机制：SAM 允许用户通过简单的点击或框选来提供分割提示，这使得模型能够适应各种复杂的分割场景。
          ●大规模预训练：SAM 在大规模数据集上进行预训练，学习到丰富的视觉特征和上下文信息，从而提高了模型的泛化能力。 ●微调能力：尽管 SAM 在预训练阶段已经具备了强大的泛化能力，但用户还可以根据特定任务对模型进行微调，以进一步提高性能。</p>
        
<h2>9.PaLM-E如何将连续传感器模态直接融入语言模型中，实现具身智能?</h2>

        <p>PaLM-E（Pathways Language Model with Embodied） 是一种将连续传感器模态直接融入语言模型的具身智能模型，其实现方式如下：
          ●多模态表示：PaLM-E 将视觉、听觉等连续传感器模态的信息编码成向量表示，并与文本信息一起输入到模型中。 ●统一编码空间：通过统一的编码空间，PaLME
          能够在文本和传感器模态之间建立联系，使得模型能够理解和生成与传感器数据相关的文本。 ●强化学习：PaLM-E 使用强化学习算法来优化模型在具身环境中的行为，从而实现更智能的交互和决策。</p>
      </div>
    </div>
  </body>

</html>