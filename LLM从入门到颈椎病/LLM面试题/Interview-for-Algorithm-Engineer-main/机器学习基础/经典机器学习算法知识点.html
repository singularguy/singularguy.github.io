<html>
  
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../../style.css">
    <base target="_parent">
    <title data-trilium-title>经典机器学习算法知识点</title>
  </head>
  
  <body>
    <div class="content">
       <h1 data-trilium-h1>经典机器学习算法知识点</h1>

      <div class="ck-content">
        <hr />
        
<h2>created: 2025-01-25T00:41
updated: 2025-01-26T02:20</h2>

        
<h2>目录</h2>

        <ul>
          <li><a href="#user-content-1.k-means%E7%AE%97%E6%B3%95%E9%80%BB%E8%BE%91%EF%BC%9F">1.K-means算法逻辑？</a>
          </li>
          <li><a href="#user-content-2.k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E9%80%BB%E8%BE%91%EF%BC%9F">2.K近邻算法逻辑？</a>
          </li>
          <li><a href="#user-content-3.%E4%BB%80%E4%B9%88%E6%98%AFNeRF%EF%BC%88Neural-Radiance-Fields%EF%BC%89%E6%8A%80%E6%9C%AF%EF%BC%9F">3.什么是NeRF（Neural-Radiance-Fields）技术？</a>
          </li>
          <li><a href="#user-content-4.%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A6%82%E5%BF%B5">4.介绍一下自回归模型的概念</a>
          </li>
          <li><a href="#user-content-5.%E4%BB%80%E4%B9%88%E6%98%AFK%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%9F">5.什么是K最近邻算法？</a>
          </li>
          <li><a href="#user-content-6.%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%9F">6.什么是朴素贝叶斯？</a>
          </li>
          <li><a href="#user-content-7.%E4%BB%80%E4%B9%88%E6%98%AF%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9F">7.什么是决策树？</a>
          </li>
          <li><a href="#user-content-8.%E4%BB%80%E4%B9%88%E6%98%AF%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%9F">8.什么是支持向量机？</a>
          </li>
          <li><a href="#user-content-9.%E4%BB%80%E4%B9%88%E6%98%AF%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%9F">9.什么是逻辑回归？</a>
          </li>
          <li><a href="#user-content-10.%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Nucleus%E9%87%87%E6%A0%B7%E5%8E%9F%E7%90%86">10.介绍一下机器学习中的Nucleus采样原理</a>
          </li>
          <li><a href="#user-content-11.%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E4%B8%8D%E5%90%8C%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E7%89%B9%E7%82%B9">11.介绍一下机器学习中不同聚类算法的性能特点</a>
          </li>
          <li><a href="#user-content-12.%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84SVD%EF%BC%88Singular-Value-Decomposition%EF%BC%89%E6%8A%80%E6%9C%AF">12.介绍一下机器学习中的SVD（Singular Value Decomposition）技术</a>
          </li>
          <li><a href="#user-content-13.%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86">13.介绍一下机器学习中的聚类算法原理</a>
          </li>
          <li><a href="#user-content-14.%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E9%A2%9C%E8%89%B2%E9%87%8F%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86">14.介绍一下颜色量化算法的原理</a>
          </li>
          <li><a href="#user-content-15.%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8BK-Means++%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86">15.介绍一下K-Means++算法的原理</a>
          </li>
        </ul>
        
<h2>1.K-means算法逻辑？</h2>

        <p>K-means算法是一个实用的无监督聚类算法，其聚类逻辑依托欧式距离，当两个目标的距离越近，相似度越大。对于给定的样本集，按照样本之间的距离大小，将样本集划分为
          $K$ 个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。</p>
        <p><strong>K-means的主要算法步骤</strong>：</p>
        <ol>
          <li>选择初始化的 $k$ 个样本作为初始聚类中心 $D = { D_{1}, D_{2}, D_{3}, ..., D_{k} }$ 。</li>
          <li>针对数据集中每个样本 $x_{i}$ ，计算它到 $k$ 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中.</li>
          <li>针对每个类别 $D_{j}$ ，重新计算它的聚类中心 $D_{j} = \frac{1}{|c_{j}|}\sum_{x\in c_{j}}x$
            。（即属于该类的所有样本的质心）；</li>
          <li>重复上面2和3两步的操作，直到达到设定的中止条件（迭代次数、最小误差变化等）。</li>
        </ol>
        <p><strong>K-Means的主要优点</strong>：</p>
        <ol>
          <li>原理简单，实现容易，收敛速度快。</li>
          <li>聚类效果较优。</li>
          <li>算法的可解释度比较强。</li>
          <li>主要需要调参的参数仅仅是簇数k。</li>
        </ol>
        <p><strong>K-Means的主要缺点</strong>：</p>
        <ol>
          <li>K值需要人为设定，不好把握。</li>
          <li>对初始的簇中心敏感，不同选取方式会得到不同结果。</li>
          <li>对于不是凸的数据集比较难收敛。</li>
          <li>如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。</li>
          <li>迭代结果只是局部最优。</li>
          <li>对噪音和异常点比较的敏感。</li>
        </ol>
        
<h2>2.K近邻算法逻辑？</h2>

        <p>K近邻（K-NN）算法计算不同数据特征值之间的距离进行分类。存在一个样本数据集合，也称作训练数据集，并且数据集中每个数据都存在标签，即我们知道每一个数据与所属分类的映射关系。接着输入没有标签的新数据后，在训练数据集中找到与该新数据最邻近的K个数据，然后提取这K个数据中占多数的标签作为新数据的标签（少数服从多数逻辑）。</p>
        <p><strong>K近邻算法的主要步骤</strong>：</p>
        <ol>
          <li>计算新数据与各个训练数据之间的距离。</li>
          <li>按照距离的递增关系进行排序。</li>
          <li>选取距离最小的K个点。</li>
          <li>确定前K个点所在类别的出现频率。</li>
          <li>返回前K个点中出现频率最高的类别作为新数据的预测分类。</li>
        </ol>
        <p>
          <img src="经典机器学习算法知识点_1923b502-8c67-491e.png" />
        </p>
        <p>K近邻算法的结果很大程度取决于K的选择。其距离计算一般使用欧氏距离或曼哈顿距离等经典距离度量。</p>
        <p><strong>K近邻算法的主要优点</strong>：</p>
        <ol>
          <li>理论成熟，思想简单，既可以用来做分类又可以做回归。</li>
          <li>可以用于非线性分类。</li>
          <li>对数据没有假设，准确度高，对异常点不敏感。</li>
          <li>比较适用于数据量比较大的场景，而那些数据量比较小的场景采用K近邻算法算法比较容易产生误分类情况。</li>
        </ol>
        <p><strong>K近邻算法的主要缺点</strong>：</p>
        <ol>
          <li>计算复杂性高；空间复杂性高。</li>
          <li>样本不平衡的时候，对稀有类别的预测准确率低。</li>
          <li>是慵懒散学习方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢。</li>
          <li>可解释性不强。</li>
        </ol>
        
<h2>3.什么是NeRF（Neural-Radiance-Fields）技术？</h2>

        <p><strong>NeRF（Neural Radiance Fields）技术主要用于生成高质量的三维场景渲染</strong>。NeRF技术通过使用神经网络来表示场景中的颜色和密度分布，从而能够生成从不同视角看到的高质量图像，在三维重建和新视图合成领域取得了显著的进展。以下是对
          NeRF 技术的详细讲解：</p>
        
<h3>基本概念</h3>

        <ol>
          <li>
            <p><strong>辐射场（Radiance Field）</strong>：</p>
            <ul>
              <li>NeRF 表示场景的辐射场，这是一种三维空间中每个点的颜色和密度的函数。具体来说，辐射场定义了从某个点在某个方向上发出的光的颜色和强度。</li>
            </ul>
          </li>
          <li>
            <p><strong>核心模型</strong>：</p>
            <ul>
              <li>NeRF 使用一个多层感知器（MLP）作为神经网络。这个网络接受三维空间中的位置（x, y, z）和视角方向（θ, φ）作为输入，并输出该位置的颜色（RGB）和密度（σ）。</li>
            </ul>
          </li>
          <li>
            <p><strong>体积渲染</strong>：</p>
            <ul>
              <li>通过体积渲染算法，从神经网络中采样不同位置的颜色和密度，合成最终的图像。体积渲染过程模拟了光线在三维场景中的传输和散射。</li>
            </ul>
          </li>
        </ol>
        
<h3>工作原理</h3>

        <p>NeRF 的工作原理可以分为以下几个步骤：</p>
        <ol>
          <li>
            <p><strong>输入编码</strong>：</p>
            <ul>
              <li>NeRF 使用傅里叶特征编码（Fourier Feature Encoding）来对输入的三维坐标和视角方向进行高频特征变换，从而提高模型的表示能力。这种编码将低频的空间信息转换为高频特征，使得神经网络可以更好地学习到细节。</li>
            </ul>
          </li>
          <li>
            <p><strong>核心模型训练</strong>：</p>
            <ul>
              <li>神经网络接受编码后的三维坐标和视角方向，输出该位置的颜色和密度。通过对比生成图像与实际图像之间的误差（如均方误差损失），调整神经网络的参数。</li>
              <li>训练数据通常是从多个视角拍摄的二维图像，这些图像包含了场景的不同视角信息。</li>
            </ul>
          </li>
          <li>
            <p><strong>体积渲染</strong>：</p>
            <ul>
              <li>对每条光线，从神经网络中采样多个点的颜色和密度，并通过体积渲染公式将这些值组合起来，生成光线上的像素颜色。</li>
              <li>具体的体积渲染公式计算光线在经过场景中的每个点时的颜色贡献，并将这些贡献累加起来，形成最终的像素值。</li>
            </ul>
          </li>
        </ol>
        
<h3>应用场景</h3>

        <ol>
          <li>
            <p><strong>新视图合成</strong>：</p>
            <ul>
              <li>NeRF 可以从给定的几张二维图像生成新的视图，非常适合用于虚拟现实（VR）和增强现实（AR）应用。</li>
            </ul>
          </li>
          <li>
            <p><strong>3D 重建</strong>：</p>
            <ul>
              <li>NeRF 可以用于从二维图像重建高质量的三维模型，应用于影视、游戏和数字文化遗产保护等领域。</li>
            </ul>
          </li>
          <li>
            <p><strong>计算机图形学</strong>：</p>
            <ul>
              <li>由于其生成高质量图像的能力，NeRF 在计算机图形学中也具有重要的应用价值。</li>
            </ul>
          </li>
        </ol>
        
<h3>优缺点</h3>

        
<h4>优点</h4>

        <ul>
          <li><strong>高质量渲染</strong>：NeRF 可以生成非常逼真的图像，捕捉到细节和复杂的光照效果。</li>
          <li><strong>少量数据需求</strong>：与传统的3D重建方法相比，NeRF只需要较少的输入图像即可生成高质量的三维场景。</li>
        </ul>
        
<h4>缺点</h4>

        <ul>
          <li><strong>计算成本高</strong>：训练NeRF模型需要大量的计算资源和时间，尤其是对于高分辨率的场景。</li>
          <li><strong>实时性问题</strong>：目前，NeRF的实时渲染仍然是一个挑战，需要更多的优化和硬件支持。</li>
        </ul>
        
<h2>4.介绍一下自回归模型的概念</h2>

        <p><strong>自回归模型思想很早就被提出，在AIGC时代因为应用于ChatGPT系列中而再次成为机器学习领域的“明星”</strong>。接下来我们就详细介绍一下自回归模型。</p>
        <p>自回归模型（Autoregressive Model, AR）是时间序列分析中的一种经典模型，用于表示当前值是过去若干值的线性组合。自回归模型假设时间序列的数据点可以用其自身的历史数据来解释，即通过过去的观测值预测当前和未来的观测值。以下是详细讲解自回归模型的原理、公式、假设、应用以及示例。</p>
        
<h3>1. 原理</h3>

        <p>自回归模型通过回归分析的方法，利用时间序列的过去值对当前值进行预测。其核心思想是，时间序列的当前值与其前几个时间点的值之间存在某种线性关系。</p>
        
<h3>2. 公式</h3>

        <p>自回归模型的数学表达式为：</p>
        <p>$$ y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p}
          + \epsilon_t $$</p>
        <p>其中：</p>
        <ul>
          <li>$y_t$ 是时间 $t$ 的观测值。</li>
          <li>$c$ 是常数项。</li>
          <li>$\phi_1, \phi_2, \ldots, \phi_p$ 是模型的系数。</li>
          <li>$p$ 是模型的阶数，表示回顾的时间步数。</li>
          <li>$\epsilon_t$ 是误差项，假设其为白噪声（即期望为零、方差为 $\sigma^2$ 的独立同分布随机变量）。</li>
        </ul>
        
<h3>3. 模型的假设</h3>

        <ul>
          <li><strong>线性关系</strong>：时间序列的当前值与过去 $p$ 个时间点的值之间存在线性关系。</li>
          <li><strong>平稳性</strong>：时间序列应是平稳的，即其统计特性（如均值和方差）随时间不变。</li>
          <li><strong>白噪声误差</strong>：误差项 $\epsilon_t$ 是白噪声。</li>
        </ul>
        
<h3>4. 应用</h3>

        <p>自回归模型广泛应用于经济学、金融学、气象学、工程学等领域，用于预测和分析时间序列数据。例如：</p>
        <ul>
          <li>经济数据中的 GDP 增长率、失业率等的预测。</li>
          <li>金融市场中的股票价格、利率等的预测。</li>
          <li>气象学中的气温、降雨量等的预测。</li>
        </ul>
        
<h3>5. 自回归模型的阶数选择</h3>

        <p>选择自回归模型的阶数 $p$ 是一个重要步骤。常用的方法包括：</p>
        <ul>
          <li><strong>AIC（Akaike 信息准则）</strong>：通过最小化 AIC 选择最佳阶数。</li>
          <li><strong>BIC（贝叶斯信息准则）</strong>：通过最小化 BIC 选择最佳阶数。</li>
        </ul>
        
<h3>6. 示例</h3>

        <p>下面是一个使用 Python 及 <code>statsmodels</code> 库来拟合和预测自回归模型的示例：</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.ar_model import AutoReg

# 生成一个模拟的自回归时间序列数据
np.random.seed(42)
n = 100
phi = [0.5, -0.3, 0.2]  # AR(3) 模型的系数
y = np.zeros(n)
y[0], y[1], y[2] = np.random.normal(size=3)  # 初始化前3个值
for t in range(3, n):
    y[t] = phi[0] * y[t-1] + phi[1] * y[t-2] + phi[2] * y[t-3] + np.random.normal()

# 拟合 AR 模型
model = AutoReg(y, lags=3)
model_fit = model.fit()

# 模型系数
print("模型系数:", model_fit.params)

# 预测未来10个时间点的值
y_forecast = model_fit.predict(start=n, end=n+9)
print("预测值:", y_forecast)

# 绘制原始数据与预测值
plt.figure(figsize=(10, 6))
plt.plot(y, label='原始数据')
plt.plot(range(n, n+10), y_forecast, label='预测值', color='red')
plt.legend()
plt.show()
</code></pre>

        
<h3>解释</h3>

        <ol>
          <li>
            <p><strong>数据生成</strong>：</p>
            <ul>
              <li>使用一个已知的 AR(3) 模型生成模拟数据，其中系数为 $[0.5, -0.3, 0.2]$。</li>
            </ul>
          </li>
          <li>
            <p><strong>拟合 AR 模型</strong>：</p>
            <ul>
              <li>使用 <code>AutoReg</code> 类拟合 AR 模型，并指定滞后阶数为 3。</li>
            </ul>
          </li>
          <li>
            <p><strong>输出模型系数</strong>：</p>
            <ul>
              <li>使用 <code>model_fit.params</code> 获取拟合模型的系数。</li>
            </ul>
          </li>
          <li>
            <p><strong>预测未来值</strong>：</p>
            <ul>
              <li>使用 <code>model_fit.predict</code> 方法预测未来 10 个时间点的值。</li>
            </ul>
          </li>
          <li>
            <p><strong>绘制图形</strong>：</p>
            <ul>
              <li>使用 <code>matplotlib</code> 库绘制原始数据和预测值的图形，以可视化效果展示预测结果。</li>
            </ul>
          </li>
        </ol>
        
<h2>5.什么是K最近邻算法？（K-nearest neighbor， KNN）</h2>

        
<h3>（1）K最近邻算法介绍（K-nearest neighbor， KNN）</h3>

        <p>KNN算法是一种用于分类任务的非参数统计方法。</p>
        <ul>
          <li>核心思想：当预测一个新样本的标签时，<strong>根据它距离最近的 $k$ 个样本点是什么标签来判断该新样本属于哪个标签</strong>（多数投票）。</li>
          <li>输入输出：输入为特征空间中的一个点，输出为该点所对应的类别标签。</li>
        </ul>
        
<h3>（2）KNN的算法流程</h3>

        <p>假设一个样本数据集 ${(x_i, y_i)}$ , $x_i$ 是一个多维向量，$y_i$ 是该向量的标签，对于未知向量 $x_j$，预测其对应的标签
          $y_j$</p>
        <ul>
          <li>计算 $x_j$ 到每一个 $x_i$ 的距离；</li>
          <li>对距离进行排序；</li>
          <li>选择最接近 $x_j$ 的 $k$ 个样本（也可通过kd树搜索）；</li>
          <li>根据多数投票原则，预测 $x_j$ 的标签。</li>
        </ul>
        
<h3>（3）核心参数</h3>

        
<h4>距离度量</h4>

        <p>两个向量 $x_i= (x_i^1,x_i^2,x_i^3...x_i^n)$，$x_j=(x_j^1,x_j^2,x_j^3...x_j^n)$
          的距离 → 两个向量的相似程度，其公式为： $$L_p(x_i,x_j)=(\sum_{j=1}^{n}{|x_{i}^{l}-x_{j}^{l}|^p})^{\frac{1}{p}}$$
          当p=1，为曼哈顿距离； 当p=2，为欧氏距离； 当p= $\infty$ ，为向量分量的最大距离差。</p>
        
<h4>$k$值选取</h4>

        <ul>
          <li>过小的 $k$ 值分类器：未知样本对邻近的样本十分敏感，易受到噪声干扰；</li>
          <li>过大的 $k$ 值分类器：未知样本易被预测为占比较大的标签类型。</li>
        </ul>
        <p><strong>常用的方法</strong>：
          <br />（1）从 $k=1$ 开始，使用交叉验证法从样本数据集中分出检验集估计分类器的误差率。
          <br />（2）重复该过程，每次 $k$ 增值1，允许增加一个近邻。
          <br />（3）选取产生最小误差率的 $k$ 。 （4）一般 $k$ 值不超过20，上限为 $n$ 的开方。</p>
        
<h4>分类决策规则</h4>

        <ul>
          <li><strong>KNN的分类决策规则</strong>：对未知样本的最邻近 $k$ 个样本进行标签统计，采用多数投票进行分类预测。</li>
        </ul>
        
<h3>（4）Python实现</h3>

        <p>使用<strong>sklearn.neighbors.KNeighborsClassifier</strong>即可创建KNN分类器，参数包括：</p>
        <ul>
          <li><strong>n_neighbors</strong>：设定k值，默认为5；</li>
          <li><strong>weights</strong>：设定k个邻近样本对型统计的权重，默认为平均权重；</li>
          <li><strong>algorithm</strong>：设定搜索邻近样本的方法，包括ball tree， kd tree和 brute。</li>
        </ul>
        
<h3>（5）算法优劣</h3>

        <ul>
          <li><strong>优点</strong>：简单易用，无需训练；对异常值不敏感。</li>
          <li><strong>缺点</strong>：惰性算法，计算量大。</li>
        </ul>
        
<h3>（6）算法应用场景</h3>

        <ul>
          <li>人脸识别，文字识别，医学图像处理等。 （毋雪雁,王水花,张煜东.K最近邻算法理论与应用综述[J].计算机工程与应用,2017,53(21):1-7.）</li>
        </ul>
        
<h3>（7）KNN用于回归问题</h3>

        <ul>
          <li>对于k个邻近样本的标签，采用平均值作为未知样本标签的预测值。</li>
        </ul>
        
<h2>6.什么是朴素贝叶斯？</h2>

        
<h3>（1） 贝叶斯定理</h3>

        <p><strong>先验概率</strong> - Prior probability</p>
        <ul>
          <li><strong>定义</strong>：在观测数据前，表达不确定量的不确定性的概率分布，记为 $P(A)$ 。</li>
          <li><strong>释义</strong>：根据已知的经验和分析得到的概率，即由因求果。</li>
        </ul>
        <p><strong>后验概率</strong> - Posterior probability</p>
        <ul>
          <li><strong>定义</strong>： 考虑和给出相关证据或数据后所得到的条件概率，记为 $P(B|A)$ 。</li>
          <li><strong>释义</strong>：依据得到的结果所计算出的事件发生的概率，即由果溯因。</li>
        </ul>
        <p><strong>联合概率</strong> - Joint probability</p>
        <ul>
          <li><strong>定义</strong>：两个事件共同发生的概率，记为 $P(AB)$ 。</li>
        </ul>
        <p><strong>条件概率</strong> - Conditional probability</p>
        <ul>
          <li><strong>定义</strong>：事件 $A$ 在事件 $B$ 发生的条件下发生的概率。</li>
          <li><strong>公式</strong>： $$P(A|B)=\frac{P(AB)}{P(B)}$$</li>
        </ul>
        <p><strong>全概率公式</strong> - Law of total probability</p>
        <ul>
          <li><strong>定义</strong>：将一复杂事件$A$的概率求解问题转化为不同独立条件（ $P({B_1}\bigcap{B_2}...)=0$
            &amp; $P({B_1}\bigcup{B_2}...)=1$ ）下发生的事件概率的求和问题。</li>
          <li><strong>公式</strong>： $$P(A)=\sum_{i=1}^n{{P(A|B_i)}\times{P(B_i)}} $$</li>
        </ul>
        <p><strong>贝叶斯定理</strong> - Bayes' theorem</p>
        <ul>
          <li><strong>定义</strong>：描述条件概率和后验概率的乘法关系。</li>
          <li><strong>公式</strong>: $$P(B_j|A)=\frac{{P(A|B_j)}\times{P(B_j)}}{P(A)}
            $$ $$P(B_j|A)=\frac{{P(A|B_j)}\times{P(B_j)}}{\sum_{i=1}^n{{P(A|B_i)}\times{P(B_i)}}}
            （独立事件）$$</li>
        </ul>
        <p><strong>后验概率与条件概率的联系</strong>
        </p>
        <ul>
          <li>后验概率是一种条件概率，用于描述在给定观测结果的情况下，因变量取某个值的概率。</li>
        </ul>
        
<h3>（2）案例释义（以信号发射为例）</h3>

        <p><strong>案例</strong>：有一个信号的发射端和接收端。发射端只发射A、B两种信号，其中发射信号A的概率为0.6，发射信号B的概率为0.4。当发射信号A时，接收端接收到信号A的概率是0.9，接收到信号B的概率是0.1。当发射信号B时，接收端接收到信号B的概率为0.8，接收到信号A的概率为0.2。求当接收到信号A时，发射信号为A的概率。</p>
        <p><strong>概率</strong>
        </p>
        <ul>
          <li><strong>先验概率</strong>：</li>
          <li>$P(sendA)$ = 0.6， $P(sendB)$ = 0.4</li>
          <li><strong>条件概率</strong>：</li>
          <li>$P(receiveA|sendA)$ = 0.9， $P(receiveB|sendB)$ = 0.8， $P(receiveA|sendB)$
            = 0.2</li>
          <li><strong>后验概率</strong>： $P(sendA|receiveA)$ = ?</li>
        </ul>
        
<h3>（3）朴素贝叶斯 - Naive bayes</h3>

        <p><strong>基本假设</strong>
        </p>
        <ul>
          <li>给定数据样本的每个特征相互独立。</li>
        </ul>
        <p><strong>简要推导</strong>
给定训练数据集，其中每个样本 $x$ 都包含 $n$ 维特征，即 $x=(x_1, x_2 ...,
          x_n)$ ，标签集合 有$k$ 种标签，即 $y=(y_1, y_2 ..., y_k)$ 。对于新样本 $x$ ，判断其标签 $y$ ，根据贝叶斯定理，可以到
          $x$ 属于 $y_k$ 标签的概率为： $$P(y_k|x)=\frac{{P(x|y_k)}\times{P(y_k)}}{\sum_{i=1}^k{{P(x|y_i)}\times{P(y_i)}}}$$</p>
        <ul>
          <li>后验概率最大的标签则为对新样本 $x$ 的预测标签。</li>
        </ul>
        <p>由于朴素贝叶斯的基本假设，所以条件概率 $P(x|y_k)$ 可以转化为： $$P(x|y_k)=P(x_1,x_2 ..., x_n|y_k)=\prod_{j=1}^n{P(x_j|y_k)}$$
          整合上式，朴素贝叶斯算法可以表示为： $$f(x)={\mathop{argmax}\limits_{y_1, y_2 ..., y_k}}{\frac{\prod_{j=1}^n{P(x_j|y_k)}{P(y_k)}}{\sum_{i=1}^k{{\prod_{j=1}^n{P(x_j|y_i)}}\times{P(y_i)}}}}$$
          由于对于所有的标签，分母一样，因此忽略分母部分，将朴素贝叶斯化简为： $$f(x)={\mathop{argmax}\limits_{y_1,
          y_2 ..., y_k}}{\prod_{j=1}^n{P(x_j|y_k)}{P(y_k)}}$$</p>
        
<h3>（4）细分模型（以文本分类为例）</h3>

        <p><strong>高斯模型</strong>
        </p>
        <ul>
          <li><strong>连续型变量特征</strong>
          </li>
          <li><strong>条件概率</strong>
$$P(x_i|y_k) = \frac{1}{\sqrt{2\pi\sigma_{y_k,i}^2}}\times{e^{-\frac{(x_i-u_{y_k,i})^2}{2\sigma_{y_k,i}^2}}}$$</li>
        </ul>
        <p>$u_{y_k,i}$ - $y_k$ 类中，第i维特征的均值； $\sigma_{y_k,i}^2$ - $y_k$ 类中，第i维特征的方差。</p>
        <p>在文本分类场景下，样本 $x$ 就是文档，特征 $x_i$ 就是单词，标签就 $y$ 就是文档类别，对于新样本 $x$ 判断其类别。</p>
        <p><strong>多项式模型</strong>
        </p>
        <ul>
          <li>以<strong>单词的频次</strong>参与统计计算。</li>
          <li><strong>先验概率</strong>： $$P(y_k)=\frac{y_k类文档的所有单词}{所有文档的所有单词}$$</li>
          <li><strong>条件概率</strong>
$$P(x_i|y_k)=\frac{单词x_i在y_k类文档中出现的次数之和+1}{y_k类文档的所有单词+所有文档的单词种类}$$</li>
        </ul>
        <p><strong>伯努利模型</strong>
        </p>
        <ul>
          <li>以<strong>是否出现</strong>参与统计计算。</li>
          <li><strong>先验概率</strong>： $$P(y_k)=\frac{y_k类文档的个数}{所有文档的个数}$$</li>
          <li><strong>条件概率</strong>
$$P(x_i|y_k)=\frac{y_k类文档中包含单词x_i的文档个数+1}{y_k类文档的个数+2}$$</li>
        </ul>
        
<h3>（5）拉普拉斯平滑</h3>

        <p>由于 $P(y_k)\times{\prod_{j=1}^n{P(x_j|y_k)}{P(y_k)}}$ 为多项连乘，其中一项为0，则整个公式为0(<strong>零概率事件</strong>)。因此，假定训练样本很大时，每个特征
          $x_i$ 和样本 $x$ 的计数加1造成的估计概率变化可以忽略不计，但可以方便有效的避免零概率问题。</p>
        
<h3>（6）Python代码</h3>

<pre><code>sklearn.naive_bayes.MultinomialNB()
</code></pre>

        
<h3>（7）算法优劣</h3>

        <p><strong>优点</strong>
        </p>
        <ul>
          <li>分类稳定，可以处理多分类任务；</li>
          <li>对确实数据不敏感，且可以进行增量训练。</li>
        </ul>
        <p><strong>缺点</strong>
        </p>
        <ul>
          <li>需要知道事件发生的先验概率。</li>
        </ul>
        
<h2>7.什么是决策树？</h2>

        
<h3>（1）决策树（Decision Tree）</h3>

        <p>决策树是一种树形结构模型，由一个根节点，若干个内部节点和叶节点组成。其中，根节点和内部结点表示一个特征或属性，叶结点表示一个类别。西瓜分类的一颗决策树如下所示：</p>
        <blockquote>
          <p>
            <img src="api/images/BBsoqrJtytW5/DT-1.png" alt="输入图片描述" />
          </p>
        </blockquote>
        
<h3>（2）基本流程</h3>

        <p>决策树一个由根到叶的递归过程，通过根节点和内部节点划分属性，直到只剩单一类型/无可用属性后停止。其伪代码如下所示：</p>
        <blockquote>
          <p>
            <img src="api/images/QpDFRGgkkFJL/DT-2.png" alt="输入图片描述" />
          </p>
        </blockquote>
        <p><strong>具体停止条件</strong>
        </p>
        <ul>
          <li>当前节点包含的样本属于同一类别，视为叶节点，无需划分；</li>
          <li>无可用属性进行后续划分，视为同一类别（叶节点），无需划分；</li>
          <li>当点节点不包含样本，删除该节点，回退至父节点重新划分。</li>
        </ul>
        
<h3>（3）算法分类</h3>

        <ul>
          <li><strong>ID3</strong>： 在决策树的内部节点和根节点上，使用信息增益方法作为划分属性的选择标准，信息增益越大越好。</li>
          <li><strong>C4.5</strong>：使用信息增益率来选择节点属性，增益率越高越好。</li>
          <li><strong>CART</strong>：使用基尼指数选择划分属性，基尼指数越小越好。</li>
        </ul>
        
<h3>（4）划分属性的选择标准</h3>

        
<h4>信息增益</h4>

        
<h2>假定离散属性 $a$ 有 $V$ 个可能的取值 ${a^1, a^2, ..., a^n}$ ，若使用 $a$ 来对样本集 $D$ 进行划分，则会产生 $V$ 个内部节点，其中第 $v$ 个内部节点包含了 $D$ 中所有在属性 $a$ 上取值为 ${a^n}$ 的样本，记为 $D_v$ ，根据式（1）计算出 $D_v$ 的信息熵，并给予该节点权重 $\frac{|D^v|}{|D|}$ 。再考虑到其他内部节点，计算出属性 $a$ 对样本集进行划分所获得的“信息增益”，如式（2）所示。
$$Ent(D) = - \sum_1^n{p_klog_2p_k}$$
 -  $Ent(D)$ - 信息熵；
 - $n$ - 类别数目；
 - $p_k$ - 样本集中第$k$类样本所占的比例
 $$Gain(D,a) = Ent(D) - \sum_1^V\frac{|D^v|}{|D|}Ent(D^v)$$
 - $Gain(D, a)$ - 信息增益。</h2>

        
<h4>信息增益率</h4>

        <p>$$Gain_ratio(D) =\frac{Gain(D,a)}{-\sum_1^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}}$$</p>
        
<h4>基尼指数</h4>

        <p>$$Gini(D) = - \sum_{k_1=1}^n\sum_{k_2!=k_1}{p_{k_1}}{p_{k_2}}$$ $$Gini_index(D,a)
          = \sum_{1}^V\frac{|D^v|}{|D|}Gini(D^v)$$</p>
        
<h3>（5）Python代码</h3>

<pre><code>from sklearn import tree #导入需要的模块
clf = tree.DecisionTreeClassifier() #实例化模型对象
class sklearn.tree.DecisionTreeClassifier (criterion=’gini’/'entropy', splitter=’best’, max_depth=None,min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None,random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,class_weight=None, presort=False)
</code></pre>

        
<h3>（6）算法优劣</h3>

        <p><strong>优点</strong>
        </p>
        <ul>
          <li>易于理解和实现；</li>
          <li>数据准备简单；</li>
        </ul>
        <p><strong>缺点</strong>
        </p>
        <ul>
          <li>类别过多时，分类困难且时间较长。</li>
        </ul>
        
<h2>8.什么是支持向量机？</h2>

        
<h3>（1）支持向量机（Support vector machine）</h3>

        <p>支持向量机(Support Vector Machine, SVM)是一种有监督的机器学习算法,主要用于分类和回归分析。它的基本思想是在高维空间中构建一个超平面，将不同类别的数据点分开,使得两类数据点到超平面的距离最大化。</p>
        <ul>
          <li>
            <p><strong>超平面</strong>
              <br />超平面是指 $n$ 维线性空间中维度为 $n-1$ 的子空间。该子空间可以把线性空间分割成不相交的两个部分，例如：二维空间的线和三维空间的面。其描述方程为
              $w^Tx+b=0$ ，记为超平面 $(w,b)$ ；而由于方程的乘法性质，对于任意的 $k$ 值， $(w,b)$ 和 $(kw,kb)$
              为同一超平面，因此下述用$(w,b)$表示。其中， $w=(w_1,w_2,...,w_{n})$ 为超平面的法向量； $b$ 为位移项，决定超平面与原点的距离。</p>
          </li>
          <li>
            <p><strong>函数间隔与支持向量</strong>
              <br />函数间隔 $y_i\times(wx_i+b)$ 表示样本点距离超平面的距离，其值越大，距离越远。而支持向量则表示满足函数 $y_i\times(wx_i+b)=k$
              的样本点。</p>
          </li>
          <li>
            <p><strong>工作原理</strong>
              <br /><strong>线性可分情况（硬间隔SVM）</strong>：对于线性可分的数据,SVM试图找到一个能将两类数据完全分开的超平面,并使两类数据点到超平面的距离最大化(即最大间隔)。这个最大间隔超平面由最靠近它的几个支持向量决定。
<strong>线性不可分情况（软间隔SVM）</strong>：对于线性不可分的数据,SVM引入了软间隔,允许一些数据点位于间隔区域内或错分,从而使分类更加鲁棒。通过引入松弛变量和惩罚参数,SVM在最大化间隔和最小化误分类之间寻求平衡。
<strong>非线性情况</strong>：对于非线性数据,SVM使用核技巧将数据映射到高维特征空间,使得在高维空间中线性可分,从而实现非线性分类。常用的核函数包括线性核、多项式核、高斯核等。</p>
          </li>
        </ul>
        
<h3>（2）模型推导</h3>

        <ul>
          <li>
            <p><strong>线性可分</strong>
              <br />（a）<strong>函数间隔和支持向量</strong>
              <br />已知超平面 $(w,b)$ ，对于任一样本 $(x_i,y_i)$ $\in$ 样本集 $D$ ，都满足函数间隔 $y_i\times(wx_i+b)&gt;0$
              。若定义最小函数间隔 $\gamma$ 为 $\underset {i} {min} {(y_i\times (wx_i+b))}$ ，则所有正样本一定满足
              ${y_i\times (wx_i+b )} \geq \gamma &gt;0$ 。为了保证分类的鲁棒性，一定存在合适的超平面 $(w,b)$
              ，使得任一正样本 $(x_i,y_i)\in{D}$ 都满足函数间隔 ${y_i\times (wx_i+b )} \geq 1$ 。其中，函数间隔
              ${y_i\times (wx_i+b )} = 1$ 对应的样本点，称为支持向量。若 $y_i=+1$ ，则 $x_i$ 落在超平面 $H_1:wx+b=1$
              上；若 $y_i=-1$ ，则 $x_i$ 落在超平面 $H_1:wx+b=-1$ 上。如图所示，超平面 $H_1$ 和 $H_2$ 均与超平面
              $H_0$ 平行，且等距分布在两侧。其中，支持向量（超平面 $H_1$ ）到超平面 $H_0$ 的距离 $\frac{1}{\Vert w \Vert_2}$
              为最短间隔，而超平面 $H_1$ 到的超平面 $H_2$ 的距离 $\frac{2}{\Vert w \Vert_2}$ 为几何间隔。</p>
            <p>
              <img src="经典机器学习算法知识点_0608_SVM1.png" alt="image" />
            </p>
            <p>（b）<strong>硬间隔最大化</strong>
              <br />支持向量机通过最大化最短间隔和集合间隔，完成对训练样本的最佳线性分类，即<strong>硬间隔最大化</strong>。公式表达为 $\underset
              {(w,b)} {max}{\frac {1}{{\Vert w \Vert_2}}},s.t. {{y_i\times (wx_i+b )}
              \geq 1}$ 。而最大化 $\frac{1}{\Vert w \Vert_2}$ 和最小化 $\frac{1}{2}{\Vert w \Vert_2}$
              等价，因此硬间隔最大化可以重写成一个凸二次规划函数，即 $\underset {(w,b)} {min}{\frac {1}{2}{{\Vert
              w \Vert_2}}},s.t. {{y_i\times (wx_i+b )} \geq 1}$ 。若样本集线性可分，则该凸二次规划函数的解一定存在且唯一。</p>
            <p>（c）<strong>对偶求解</strong>
              <br />引入拉格朗日算子，即可写出凸二次规划函数的拉格朗日函数，如下：
              <br />$$L(w,b,\alpha) = \frac {1}{2}{\Vert w \Vert_2}- \sum_{i=1}^{n}{\alpha_i
              y_i(wx_i+b)+\sum_{i=1}^{n}\alpha_i}$$ 其中，$\alpha=(\alpha_1,\alpha_2,...\alpha_n)$
              是拉格朗日乘子。 根据朗格朗日的对偶性，将求解问题转化为一个极大极小问题 $\underset {\alpha} {max} \underset
              {(w,b)} {min} L(w,b,\alpha)$ 。解法如下： Step 1： 将拉格朗日函数其 $L(w,b,\alpha)$ 分别对
              $w,b$ 求偏导可得：</p>
          </li>
        </ul>
        <p>$$\frac {\partial L}{\partial w} = w - \sum_{i=1}^{n}\alpha_iy_ix_i=0$$
          $$\frac {\partial L}{\partial b} = - \sum_{i=1}^{n}\alpha_ix_i=0$$ Step
          2： 将拉格朗日函数化简为：
          <br />$$L(w,b,\alpha) = -\frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n}{\alpha_i\alpha_jy_iy_j(x_ix_j)}+\sum_{i=1}^{n}\alpha_i$$
          Step 3: 将极大极小问题化简为：
          <br />$$\underset {\alpha} {min} \frac {1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n}{\alpha_i\alpha_jy_iy_j(x_ix_j)}
          -\sum_{i=1}^{n}\alpha_i,$$ $$s.t. \sum_{i=1}^{n}\alpha_iy_i=0$$ Step 4：求解出最佳的超平面
          $(w,b)$：
          <br />$$w=\sum_{i=1}^{n}\alpha_iy_ix_i$$ $$b=y_i-x_j\sum_{i=1}^{n}\alpha_iy_ix_i$$
          $$s.t. y_j(wx_j+b)=1$$</p>
        <ul>
          <li>
            <p><strong>线性不可分</strong>
              <br />线性可分问题的支持向量机对线性不可分是不适用的，因此本节采用软间隔将支持向量机推广到线性不可分的情况。
              <br />（a）<strong>软间隔最大化</strong>
              <br /> <strong>假设条件</strong>：样本集 $D$ 不是线性可分的，但剔除特异点之后，剩下的大部分样本 $(x_i,y_i)$ 是线性可分的。
              <br
              />线性不可分意味着某些样本点 $(x_i,y_i)$ 不能满足函数间隔 ${y_i\times (wx_i+b )} \geq 1$ 的约束条件。因此，引入一个松弛变量
              $\xi_i \geq0$ ，使得函数间隔更加宽松 ${y_i\times (wx_i+b )} \geq 1-\xi_i$ 。同时，对每个松弛变量
              $\xi_i$ ，引入一个代价变量 $\xi_i$ 和惩罚参数 $C$ 。软间隔最大化的凸二次规划函数则可以转化为：
              <br />$${\frac {1}{2}{{\Vert w \Vert_2}}} + C \sum_{i=1}^{n}{\xi_i}$$ $$s.t.
              {y_i(wx_i+b) \geq 1-\xi_i},({ \xi_i\geq 0})$$ （2）<strong>对偶求解</strong>
              <br
              />引入拉格朗日乘子 $\alpha,\beta$ ，写出凸二次规划函数的对偶问题，如下：
              <br />$$\underset {\alpha, \beta}{max}{- \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i
              \alpha_j y_i y_j (x_ix_j)}+\sum_{i=1}^{n}\alpha_i$$ $$s.t. \sum_{i=1}^{n}\alpha_iy_i=0$$
              $$C-\alpha_i-\beta_i=0$$ $$\alpha_i \geq 0, \beta_i \geq 0$$ 如果 $0 &lt;
              \alpha_i &lt; C$ ，则 $C − \alpha_i = \beta_i &gt; 0$ ，可以求得对应的 $\xi_i=0$
              。因此，该条件下最终求解的最优超平面 $(w,b)$ 同线性可分类似，为 $$w=\sum_{i=1}^{n}\alpha_iy_ix_i$$
              $$b=y_j-x_j\sum_{i=1}^{n}\alpha_iy_ix_i$$ $$s.t. y_j(wx_j+b)=1$$ 在线性不可分的情况下，最优超平面
              $(w,b)$ 的法向量 $w$ 是唯一的，但是偏置 $b$ 不一定是唯一的。因此，采用多次求解后的均值作为偏置 $b$ 。 对于 $\alpha_i
              =C$ 来说，满足 $\xi_i &gt;0$ 都是特异点。特异点到所属边界超平面的距离为 $\frac {\xi_i}{\Vert w \Vert_2}$
              。如果 $0&lt;\xi_i&lt;1$ ，则位于超平面 $H_1,H_2$ 和 $H_0$ 之间，仍被分类成功：如果 $\xi_i=1$
              ，在超平面 $H_0$ 上，无法分类； $\xi_i&gt;1$ ，则被分类错误。</p>
            <p>
              <img src="经典机器学习算法知识点_0608_SVM2.png" alt="image" />
            </p>
          </li>
          <li>
            <p><strong>非线性</strong>
在非线性情况下，SVM通过某种事先选择的非线性映射（核函数）将输入变量映到一个高维特征空间，将其变成在高维空间线性可分，在这个高维空间中构造最优分类超平面。
              参考：<a href="https://blog.csdn.net/qq_45823424/article/details/113420320">支持向量机原理之线性SVM与非线性SVM</a>
            </p>
          </li>
        </ul>
        
<h3>（3）Python代码</h3>

        <p><code>from sklearn.svm import SVC</code>
        </p>
        <p>参考：<a href="https://blog.csdn.net/weixin_66845445/article/details/137054240">【ML】支持向量机SVM及Python实现（详细）_支持向量机代码（鸢尾花为案例）</a>
        </p>
        
<h3>（4）优缺点</h3>

        <ul>
          <li><strong>优点</strong>：线性/非线性分类，小样本，高维数据。</li>
          <li><strong>缺点</strong>： 对核函数和惩罚参数的选择十分敏感。</li>
        </ul>
        
<h2>9.什么是逻辑回归？</h2>

        
<h3>（1）核心思想</h3>

        <p>对样本集$X$中的样本 $x=(x_1,x_2...x_i)$ 的特征进行线性拟合，并采用Sigmoid函数将拟合的预测结果值映射到值域为(0，1)的概率空间。其中，当线性回归的输出值大于0，Sigmoid函数将输出大于0.5的值；当线性回归的输出值等于0，Sigmoid函数将输出等于0.5的值；当线性回归的输出值小于0，Sigmoid函数将输出小于0.5的值。因此，可以将大于等于0.5的情况视为正分类（
          $y_1$ ），而小于0.5的情况视为负分类（ $y_0$ ）。</p>
        <ul>
          <li><strong>Sigmoid函数</strong>
由于线性回归的结果范围为正负无穷，因此通过<strong>对数几率</strong>将线性回归的<strong>预测结果</strong>非线性映射到固定区间(0~1)之间，数学表达式为:
            $$S(x)=\frac{1}{1+e^{-x}}$$
            <blockquote>
              <p>
                <img src="api/images/Ha2JDbDfZNLH/LR-1.jpg" alt="输入图片描述" />
              </p>
            </blockquote>
          </li>
        </ul>
        
<h3>（2）逻辑回归</h3>

        <ul>
          <li>
            <p><strong>数学表达</strong>
$$P(y=1|x,\theta)=\frac{1}{1+e^{-\theta^Tx}}$$ $P(y=1|x,\theta)$
              表示给定样本 $x$ ，其预测标签为正分类的概率。 $\theta$ 表示样本特征 $x_i$ 的权重参数。这个表达式的核心思想可以通过2步来分解和理解：<strong>第一步：回归假设</strong>：
              $z = h_\theta(x)=\theta^Tx$ ；<strong>第二步：Sigmoid函数</strong>： $y =g(z)=\frac{1}{1+e^{-z}}$
              。当 $\theta^Tx≥0, h_\theta(x)≥0,g(z)≥0.5$ 为正分类，反之 $g(z)&lt;0.5$ 为负分类，因此逻辑回归算法的核心就在于求解权重
              $\theta$ 和回归假设的函数，即确定决策边界。</p>
          </li>
          <li>
            <p><strong>决策边界的定义</strong>
逻辑回归算法通常不拟合样本分布，而且通过权重 $\theta$ 和回归函数确定决策边界，将样本划分为2类。其中，决策边界包括线性决策边界和非线性决策边界。 <strong>线性决策边界</strong>：即第一步<strong>线性回归</strong>：
              $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2...+\theta_ix_i$ 。</p>
            <p> <strong>非线性决策边界</strong>：即将线性回归拓展成<strong>多项式回归</strong>： $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2^2...+\theta_ix_i^i$
              。</p>
          </li>
          <li>
            <p><strong>决策边界的确定</strong>
决策边界通过梯度下降法最小化损失函数得到。
<strong>损失函数</strong>： 损失函数通过衡量训练样本标签与预测标签之间的差异，确定最优的决策边界。其中，损失函数越小，决策边界越好。损失函数包括：<strong>均方误差损失(MSE)<strong>和</strong>对数损失函数</strong>。
              均方差误差： $$MSE=\frac{1}{m}\sum_{x\in{X}}({f(x)-y_{1/2})}$$ 对数损失函数： $$J(\theta)=-\frac{1}{m}[\sum_{x\in{X}}(y_{1/2}\log{h_\theta(x)+(1-y_{1/2})\log(1-h_\theta(x))}]$$
<strong>梯度下降</strong>：梯度下降法通过向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索，找到一个函数的局部极小值。该局部极小值对应的参数
              $\theta$ 即为最佳的参数 $\theta$ 。其公式为： $$J(\theta_1)=\theta_1-\alpha\frac{dJ(\theta_1)}{d\theta_1}$$</p>
          </li>
        </ul>
        
<h3>（3）算法正则化</h3>

        <p>在训练数据不够多，或者模型复杂又过度训练时，模型会陷入过拟合（Overfitting）状态。通过对损失函数添加正则化项，可以约束参数的搜索空间，从而缓解过拟合现象，以下是对对数损失函数添加L2正则化项的公式。其中，
          $m$ 为样本集 $X$ 的个数； $\lambda$ 为正则化系数， $\lambda$ 值越大， $J(\theta)$ 越大，越不容易发生过拟合现象。
          $$J(\theta)=\frac{1}{m}[\sum_{x\in{X}}(y_{1/2}\log{h_\theta(x)+(1-y_{1/2})\log(1-h_\theta(x))}]+\frac{\lambda}{2m}\sum_{j=1}^i{\theta_j^2}$$</p>
        
<h3>（4）Python代码</h3>

        <p>参见厦门大学数据实验室
<a href="https://dblab.xmu.edu.cn/blog/84/">Python实现逻辑回归(Logistic Regression in Python)_厦大数据库实验室博客 (xmu.edu.cn)</a>
        </p>
        
<h2>10.介绍一下机器学习中的Nucleus采样原理</h2>

        <p><strong>Nucleus 采样</strong>（Nucleus Sampling），也称为 <strong>Top-p 采样</strong>，是一种用于文本生成模型（如
          GPT 系列模型）的采样策略，特别用于生成质量更高、更具多样性的文本。它通过动态调整生成候选集的大小，控制输出的质量和随机性。</p>
        
<h3>1. <strong>采样问题背景</strong></h3>

        <p>在生成式任务中，如文本生成或对话系统，模型通常会在每个生成步骤从概率分布中选择一个单词。常见的策略有：</p>
        <ul>
          <li><strong>贪婪搜索</strong>：每一步选择概率最高的单词，容易导致重复和缺乏多样性。</li>
          <li><strong>随机采样</strong>：完全根据模型输出的概率分布随机选取，可能会导致生成无意义或语法不通的内容。</li>
          <li><strong>Top-k 采样</strong>：仅从概率分布前 k 个最有可能的词中选择，而忽略剩下的候选词。这增加了一定的随机性，但
            k 的值固定，可能忽略了一些罕见但有意义的词。</li>
        </ul>
        <p>为了在 <strong>质量</strong> 和 <strong>多样性</strong> 之间取得更好的平衡，<strong>Nucleus 采样</strong> 应运而生。</p>
        
<h3>2. <strong>Nucleus 采样原理</strong></h3>

        <p>Nucleus 采样并不是简单地选择前 k 个最可能的词，而是根据一个动态的概率阈值 $p$ 来决定候选集的大小。它的具体操作如下：</p>
        <ol>
          <li>
            <p><strong>计算概率分布</strong>：给定模型在当前时间步的输出概率分布。</p>
          </li>
          <li>
            <p><strong>排序并累加概率</strong>：将词按照模型给出的概率从高到低排序，然后从最高概率开始累积这些概率值，直到累积的概率达到设定的阈值
              $p$ 。</p>
          </li>
          <li>
            <p><strong>采样候选集</strong>：生成的候选集是累积概率超过 $p$ 的那一部分词。接下来，从这个候选集中根据概率分布随机采样。</p>
          </li>
        </ol>
        <p>因此，Nucleus 采样是基于累积概率的动态调整策略，它确保候选词集合足够灵活，包含那些对生成质量至关重要的高概率词，同时也保留了低概率但可能有创意的词。</p>
        
<h3>3. <strong>与 Top-k 采样的区别</strong></h3>

        <ul>
          <li>
            <p><strong>Top-k 采样</strong>：Top-k 采样是从前 k 个最有可能的词中采样，忽略了剩下的词。而无论总概率分布如何，k
              是一个固定的整数。它的局限性在于，k 的选择可能过小，限制了生成的多样性，或者过大，导致生成质量下降。</p>
          </li>
          <li>
            <p><strong>Nucleus 采样（Top-p 采样）</strong>：Nucleus 采样动态选择候选词的数量，通过一个累积概率阈值
              p。随着 p 的变化，候选词集合可以根据上下文自动扩展或收缩，因此比 Top-k 更灵活、更适应不同的生成情境。</p>
          </li>
        </ul>
        
<h3>4. <strong>p 值的影响</strong></h3>

        <ul>
          <li>
            <p><strong>较小的 p</strong>：如果设定的 $p$ 很小（接近 0.1 或 0.2），模型将只会从极少数的高概率词中选择，输出将趋于确定，类似贪婪搜索的效果。这会提高文本的连贯性和语法正确性，但可能缺乏多样性和创意。</p>
          </li>
          <li>
            <p><strong>较大的 p</strong>：如果设定的 $p$ 较大（如 0.9），候选词集合会更大，包含更多低概率词，增加生成的随机性和多样性。这可能使文本生成更具创意和新颖性，但也可能会增加生成不连贯或无意义内容的风险。</p>
          </li>
        </ul>
        
<h3>5. <strong>Nucleus 采样的优势</strong></h3>

        <ul>
          <li>
            <p><strong>灵活性</strong>：Nucleus 采样能够根据不同上下文动态调整候选集的大小，在保证生成质量的同时，增加文本的多样性。</p>
          </li>
          <li>
            <p><strong>避免冗余或无意义的选择</strong>：与 Top-k 不同，Nucleus 采样不会固定从 k 个词中采样，它会在保证语义连贯的前提下，选择最有意义的词进行采样。</p>
          </li>
          <li>
            <p><strong>平衡性</strong>：它提供了灵活的平衡机制，既能控制生成的连贯性和语法正确性，又能通过随机性保持生成内容的丰富性。</p>
          </li>
        </ul>
        
<h3>6. <strong>实际应用场景</strong></h3>

        <p>Nucleus 采样常用于生成式语言模型中，如 GPT-3、GPT-4 等，它可以生成对话、文本扩展等任务。通过调整 p 值，开发者可以控制模型输出的创造性和连贯性。比如：</p>
        <ul>
          <li>在<strong>写作辅助</strong>场景中，可以设置较大的 p 值，鼓励模型生成具有创意的内容。</li>
          <li>在<strong>对话系统</strong>中，较小的 p 值可能更适合，让模型的回答更加精确和连贯。</li>
        </ul>
        
<h3>7. <strong>示例代码</strong></h3>

        <p>以下是一个使用 PyTorch 的简单示例，展示如何实现 Nucleus 采样。</p>
<pre><code class="language-python">import torch

def nucleus_sampling(logits, p):
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
    
    # 选出累积概率大于 p 的部分
    cutoff_index = torch.where(cumulative_probs &gt; p)[0][0]
    filtered_logits = sorted_logits[:cutoff_index + 1]
    filtered_indices = sorted_indices[:cutoff_index + 1]
    
    # 在这些候选集中进行采样
    sampled_index = torch.multinomial(torch.softmax(filtered_logits, dim=-1), 1)
    return filtered_indices[sampled_index]

# 假设 logits 是一个表示词的概率的张量，下面的代码会从中进行 Nucleus 采样
logits = torch.tensor([0.1, 0.2, 0.05, 0.4, 0.15, 0.1])
p = 0.85  # 累积概率阈值
selected_word_index = nucleus_sampling(logits, p)
print(selected_word_index)
</code></pre>

        
<h2>11.介绍一下机器学习中不同聚类算法的性能特点</h2>

        <table>
          <thead>
            <tr>
              <th align="center">算法名称</th>
              <th align="center">可伸缩性</th>
              <th align="center">适合的数据类型</th>
              <th align="center">高维性</th>
              <th align="center">异常数据抗干扰性</th>
              <th align="center">聚类形状</th>
              <th align="center">算法效率</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center">WAVECLUSTER</td>
              <td align="center">很高</td>
              <td align="center">数值型</td>
              <td align="center">很高</td>
              <td align="center">较高</td>
              <td align="center">任意形状</td>
              <td align="center">很高</td>
            </tr>
            <tr>
              <td align="center">ROCK</td>
              <td align="center">很高</td>
              <td align="center">混合型</td>
              <td align="center">很高</td>
              <td align="center">很高</td>
              <td align="center">任意形状</td>
              <td align="center">一般</td>
            </tr>
            <tr>
              <td align="center">BIRCH</td>
              <td align="center">较高</td>
              <td align="center">数值型</td>
              <td align="center">较低</td>
              <td align="center">较低</td>
              <td align="center">球形</td>
              <td align="center">很高</td>
            </tr>
            <tr>
              <td align="center">CURE</td>
              <td align="center">较高</td>
              <td align="center">数值型</td>
              <td align="center">一般</td>
              <td align="center">很高</td>
              <td align="center">任意形状</td>
              <td align="center">较高</td>
            </tr>
            <tr>
              <td align="center">K-PROTOTYPES</td>
              <td align="center">一般</td>
              <td align="center">混合型</td>
              <td align="center">较低</td>
              <td align="center">较低</td>
              <td align="center">任意形状</td>
              <td align="center">一般</td>
            </tr>
            <tr>
              <td align="center">DENCLUE</td>
              <td align="center">较低</td>
              <td align="center">数值型</td>
              <td align="center">较高</td>
              <td align="center">一般</td>
              <td align="center">任意形状</td>
              <td align="center">较高</td>
            </tr>
            <tr>
              <td align="center">OPTIGRID</td>
              <td align="center">一般</td>
              <td align="center">数值型</td>
              <td align="center">较高</td>
              <td align="center">一般</td>
              <td align="center">任意形状</td>
              <td align="center">一般</td>
            </tr>
            <tr>
              <td align="center">CLIQUE</td>
              <td align="center">较高</td>
              <td align="center">数值型</td>
              <td align="center">较高</td>
              <td align="center">较高</td>
              <td align="center">任意形状</td>
              <td align="center">较低</td>
            </tr>
            <tr>
              <td align="center">DBSCAN</td>
              <td align="center">一般</td>
              <td align="center">数值型</td>
              <td align="center">较低</td>
              <td align="center">较高</td>
              <td align="center">任意形状</td>
              <td align="center">一般</td>
            </tr>
            <tr>
              <td align="center">CLARANS</td>
              <td align="center">较低</td>
              <td align="center">数值型</td>
              <td align="center">较低</td>
              <td align="center">较高</td>
              <td align="center">球形</td>
              <td align="center">较低</td>
            </tr>
          </tbody>
        </table>
        
<h2>12.介绍一下机器学习中的SVD（Singular Value Decomposition）技术</h2>

        <p><strong>SVD（Singular Value Decomposition，奇异值分解）</strong> 是一种常用的矩阵分解技术，在AIGC、传统深度学习以及自动驾驶中都有广泛的应用。SVD
          提供了一种将一个矩阵分解成多个分量的方法，有助于数据降维、特征提取、数据去噪、AI模型轻量化等任务。</p>
        
<h2><strong>1. SVD 的数学原理</strong></h2>

        <p>对于一个任意矩阵 $A$ （大小为 $m \times n$ ），SVD 将其分解为三个矩阵的乘积：</p>
        <p>$$ A = U \Sigma V^T $$</p>
        <p>其中：</p>
        <ul>
          <li>$U$ （大小 $m \times m$ ）：左奇异向量矩阵，列向量是 $A A^T$ 的特征向量。</li>
          <li>$\Sigma$ （大小 $m \times n$ ）：对角矩阵，包含 $A$ 的<strong>奇异值</strong>，按降序排列。奇异值是
            $A$ 的特征值的平方根。</li>
          <li>$V^T$ （大小 $n \times n$ ）：右奇异向量矩阵，行向量是 $A^T A$ 的特征向量的转置。</li>
        </ul>
        <p><strong>几何解释</strong>：</p>
        <ul>
          <li>SVD 将一个矩阵 $A$ 分解成三个部分：先通过 $V$ 进行旋转，再通过 $\Sigma$ 进行缩放，最后通过 $U$ 进行另一个旋转。</li>
          <li>$\Sigma$ 中的奇异值反映了矩阵 $A$ 在不同方向上的重要性。</li>
        </ul>
        
<h2><strong>2. SVD 的应用场景</strong></h2>

        
<h3><strong>2.1 数据降维与 PCA</strong></h3>

        <ul>
          <li><strong>PCA（主成分分析）</strong>：PCA 可以通过 SVD 实现，它将高维数据映射到一个低维空间，同时保留尽可能多的数据方差。</li>
          <li><strong>方法</strong>：
            <ol>
              <li>对数据矩阵 $A$ 进行中心化处理（减去均值）。</li>
              <li>对中心化矩阵执行 SVD 分解 $A = U \Sigma V^T$ 。</li>
              <li>选取 $\Sigma$ 中最大的 $k$ 个奇异值及对应的 $U$ 和 $V$ 向量，得到降维后的数据。</li>
            </ol>
          </li>
        </ul>
        
<h3><strong>2.2 推荐系统</strong></h3>

        <ul>
          <li>在推荐系统中，用户-物品评分矩阵通常是稀疏的。SVD 可以分解评分矩阵，提取出用户和物品的隐含特征表示。</li>
          <li><strong>步骤</strong>：
            <ol>
              <li>对评分矩阵 $R$ 进行 SVD 分解。</li>
              <li>使用分解后的矩阵 $U$ , $\Sigma$ , $V^T$ 预测缺失的评分。</li>
            </ol>
          </li>
        </ul>
        <p><strong>优势</strong>：SVD 可以有效提取用户和物品的特征，解决稀疏矩阵的问题。</p>
        
<h3><strong>2.3 去噪与数据压缩</strong></h3>

        <ul>
          <li><strong>图像去噪</strong>：SVD 可用于图像的压缩和去噪，通过保留主要的奇异值，丢弃次要的奇异值，实现信息的压缩和噪声的滤除。</li>
          <li><strong>原理</strong>：
            <ul>
              <li>图像通常被表示为矩阵形式，通过 SVD 分解后，保留前 $k$ 个奇异值及对应向量，重构近似图像。</li>
              <li>较小的奇异值通常对应噪声。</li>
            </ul>
          </li>
        </ul>
        
<h3><strong>2.4 解决线性系统与伪逆计算</strong></h3>

        <ul>
          <li>SVD 可以用于求解奇异或非方矩阵的线性系统。</li>
          <li><strong>矩阵伪逆</strong>：
            <ul>
              <li>对矩阵 $A$ 的伪逆可以通过 SVD 求解： $$ A^+ = V \Sigma^+ U^T $$ 其中 $\Sigma^+ $ 是 $\Sigma$
                的伪逆。</li>
            </ul>
          </li>
        </ul>
        
<h2><strong>3. SVD 的优点与缺点</strong></h2>

        
<h3><strong>优点</strong>：</h3>

        <ol>
          <li><strong>适用性广</strong>：适用于任意形状的矩阵（非方矩阵也适用）。</li>
          <li><strong>数值稳定性</strong>：SVD 是一种数值稳定的分解方法。</li>
          <li><strong>特征提取</strong>：可用于数据降维、压缩、去噪等任务。</li>
          <li><strong>适用于稀疏矩阵</strong>：在推荐系统等任务中表现优秀。</li>
        </ol>
        
<h3><strong>缺点</strong>：</h3>

        <ol>
          <li><strong>计算复杂度高</strong>：SVD 的时间复杂度为 $O(n^3)$ （对于 $n \times n$ 的矩阵），在大规模数据上计算成本较高。</li>
          <li><strong>不易解释</strong>：分解后的奇异值和奇异向量可能缺乏直观的物理意义。</li>
        </ol>
        
<h2><strong>4. SVD 与其他分解方法的对比</strong></h2>

        <table>
          <thead>
            <tr>
              <th>分解方法</th>
              <th>适用矩阵类型</th>
              <th>主要应用</th>
              <th>是否适用于降维</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>SVD</strong>
              </td>
              <td>任意矩阵</td>
              <td>降维、去噪、推荐系统</td>
              <td>是</td>
            </tr>
            <tr>
              <td><strong>PCA</strong>
              </td>
              <td>数据协方差矩阵</td>
              <td>主成分分析、特征提取</td>
              <td>是（基于 SVD）</td>
            </tr>
            <tr>
              <td><strong>LU 分解</strong>
              </td>
              <td>方阵</td>
              <td>线性方程求解</td>
              <td>否</td>
            </tr>
            <tr>
              <td><strong>QR 分解</strong>
              </td>
              <td>方阵</td>
              <td>正交化与回归分析</td>
              <td>否</td>
            </tr>
            <tr>
              <td><strong>Eigen 分解</strong>
              </td>
              <td>方阵，且对称正定</td>
              <td>计算特征值、特征向量</td>
              <td>否</td>
            </tr>
          </tbody>
        </table>
        
<h2>13.介绍一下机器学习中的聚类算法原理</h2>

        
<h4><strong>什么是聚类？</strong></h4>

        <ul>
          <li>聚类是一种<strong>无监督学习</strong>技术，它的目标是将数据集划分为若干个组（簇，clusters），使得：
            <ul>
              <li>同一簇中的数据点彼此之间的相似度尽可能高。</li>
              <li>不同簇之间的数据点相似度尽可能低。</li>
            </ul>
          </li>
          <li>聚类广泛应用于AIGC、传统深度学习以及自动驾驶等AI核心领域。</li>
        </ul>
        
<h3><strong>聚类算法的主要类别</strong></h3>

        <p>根据聚类方法的不同，可以将聚类算法分为以下几类：</p>
        <ol>
          <li><strong>划分式聚类（Partitioning Clustering）</strong>
          </li>
          <li><strong>层次聚类（Hierarchical Clustering）</strong>
          </li>
          <li><strong>基于密度的聚类（Density-Based Clustering）</strong>
          </li>
          <li><strong>基于网格的聚类（Grid-Based Clustering）</strong>
          </li>
          <li><strong>基于模型的聚类（Model-Based Clustering）</strong>
          </li>
        </ol>
        <p>下面Rocky详细介绍每种类别的代表算法。</p>
        
<h3><strong>1. 划分式聚类</strong></h3>

        
<h4><strong>概念</strong></h4>

        <ul>
          <li>将数据集划分为 $K$ 个簇，直接对簇进行优化。</li>
          <li>每个簇由一个中心点（质心）表示，数据点根据距离最近的质心分配到对应的簇。</li>
        </ul>
        
<h4><strong>代表算法：K-Means</strong></h4>

        <ul>
          <li>
            <p><strong>K-Means</strong> 是最常用的划分式聚类算法。</p>
          </li>
          <li>
            <p><strong>工作原理</strong>：</p>
            <ol>
              <li>随机选择 $K$ 个点作为初始质心。</li>
              <li>将每个数据点分配到最近的质心。</li>
              <li>重新计算每个簇的质心。</li>
              <li>重复步骤 2 和 3，直到质心不再变化或达到最大迭代次数。</li>
            </ol>
          </li>
          <li>
            <p><strong>优点</strong>：</p>
            <ul>
              <li>简单易实现。</li>
              <li>计算效率高，适合大规模数据。</li>
            </ul>
          </li>
          <li>
            <p><strong>缺点</strong>：</p>
            <ul>
              <li>需要预先指定 $K$ 。</li>
              <li>对初始值敏感，容易陷入局部最优。</li>
              <li>不能处理非球形数据和不同大小的簇。</li>
            </ul>
          </li>
          <li>
            <p><strong>改进算法</strong>：</p>
            <ul>
              <li><strong>K-Means++</strong>：改进初始化步骤，使质心的选择更加合理。</li>
              <li><strong>MiniBatch K-Means</strong>：用于大规模数据集，采用小批量数据优化。</li>
            </ul>
          </li>
        </ul>
        
<h3><strong>2. 层次聚类</strong></h3>

        
<h4><strong>概念</strong></h4>

        <ul>
          <li>通过构建层次树状结构（dendrogram）实现聚类。</li>
          <li>可以分为两种方法：
            <ol>
              <li><strong>凝聚层次聚类（Agglomerative Clustering）</strong>：
                <ul>
                  <li>自底向上，每个数据点开始是一个单独的簇，逐步合并簇。</li>
                </ul>
              </li>
              <li><strong>分裂层次聚类（Divisive Clustering）</strong>：
                <ul>
                  <li>自顶向下，所有数据点开始是一个大簇，逐步分裂成多个簇。</li>
                </ul>
              </li>
            </ol>
          </li>
        </ul>
        
<h4><strong>代表算法：凝聚层次聚类</strong></h4>

        <ul>
          <li>
            <p><strong>步骤</strong>：</p>
            <ol>
              <li>计算所有数据点之间的距离矩阵。</li>
              <li>找到最近的两个簇，合并它们。</li>
              <li>更新距离矩阵。</li>
              <li>重复步骤 2 和 3，直到所有数据点合并为一个簇或达到停止条件。</li>
            </ol>
          </li>
          <li>
            <p><strong>链接方法（距离度量方式）</strong>：</p>
            <ul>
              <li><strong>单链（Single Linkage）</strong>：两簇中最近的数据点之间的距离。</li>
              <li><strong>全链（Complete Linkage）</strong>：两簇中最远的数据点之间的距离。</li>
              <li><strong>平均链（Average Linkage）</strong>：两簇之间所有点的平均距离。</li>
            </ul>
          </li>
          <li>
            <p><strong>优点</strong>：</p>
            <ul>
              <li>不需要预定义簇的数量。</li>
              <li>适合小数据集，能够生成数据的层次结构。</li>
            </ul>
          </li>
          <li>
            <p><strong>缺点</strong>：</p>
            <ul>
              <li>计算复杂度较高，无法处理大规模数据。</li>
              <li>对噪声和异常值敏感。</li>
            </ul>
          </li>
        </ul>
        
<h3><strong>3. 基于密度的聚类</strong></h3>

        
<h4><strong>概念</strong></h4>

        <ul>
          <li>通过数据点的密度来定义簇，高密度区域的点归为一个簇，低密度区域被认为是噪声或边界点。</li>
        </ul>
        
<h4><strong>代表算法：DBSCAN</strong></h4>

        <ul>
          <li>
            <p><strong>工作原理</strong>：</p>
            <ol>
              <li>选择一个点，确定其 $\epsilon$ -邻域内的点。</li>
              <li>如果邻域内的点数大于某个阈值（MinPts），则将这些点归为一个簇。</li>
              <li>对邻域内的点重复步骤 2，直到没有新的点可以加入。</li>
              <li>标记低密度区域的点为噪声点。</li>
            </ol>
          </li>
          <li>
            <p><strong>优点</strong>：</p>
            <ul>
              <li>能够自动确定簇的数量。</li>
              <li>能处理任意形状的簇。</li>
              <li>对噪声点具有鲁棒性。</li>
            </ul>
          </li>
          <li>
            <p><strong>缺点</strong>：</p>
            <ul>
              <li>对 $\epsilon$ 和 MinPts 参数敏感。</li>
              <li>在高维数据中效果较差。</li>
            </ul>
          </li>
        </ul>
        
<h4><strong>改进算法</strong></h4>

        <ul>
          <li><strong>HDBSCAN</strong>：
            <ul>
              <li>在 DBSCAN 基础上，自动选择合适的密度阈值，适合更多场景。</li>
            </ul>
          </li>
        </ul>
        
<h3><strong>4. 基于网格的聚类</strong></h3>

        
<h4><strong>概念</strong></h4>

        <ul>
          <li>将数据空间划分为固定大小的网格单元，每个单元根据数据密度分配到簇。</li>
        </ul>
        
<h4><strong>代表算法：CLIQUE</strong></h4>

        <ul>
          <li>
            <p><strong>工作原理</strong>：</p>
            <ol>
              <li>将数据划分为固定网格。</li>
              <li>识别高密度网格单元。</li>
              <li>将相邻的高密度单元合并为一个簇。</li>
            </ol>
          </li>
          <li>
            <p><strong>优点</strong>：</p>
            <ul>
              <li>高效，适合大规模数据。</li>
              <li>对高维数据有效。</li>
            </ul>
          </li>
          <li>
            <p><strong>缺点</strong>：</p>
            <ul>
              <li>依赖网格划分的大小。</li>
              <li>可能丢失簇的边界细节。</li>
            </ul>
          </li>
        </ul>
        
<h3><strong>5. 基于模型的聚类</strong></h3>

        
<h4><strong>概念</strong></h4>

        <ul>
          <li>通过假设数据点服从某种分布（例如高斯分布），用概率模型来拟合数据。</li>
        </ul>
        
<h4><strong>代表算法：GMM（Gaussian Mixture Model）</strong></h4>

        <ul>
          <li>
            <p><strong>工作原理</strong>：</p>
            <ol>
              <li>假设数据点服从多个高斯分布。</li>
              <li>使用期望最大化算法（EM）优化每个高斯分布的参数（均值、协方差）。</li>
              <li>根据每个数据点属于各高斯分布的概率分配簇。</li>
            </ol>
          </li>
          <li>
            <p><strong>优点</strong>：</p>
            <ul>
              <li>能够生成概率输出，适合软聚类。</li>
              <li>能处理不同形状和大小的簇。</li>
            </ul>
          </li>
          <li>
            <p><strong>缺点</strong>：</p>
            <ul>
              <li>对初始值敏感。</li>
              <li>计算复杂度较高。</li>
            </ul>
          </li>
        </ul>
        
<h3><strong>聚类算法比较</strong></h3>

        <table>
          <thead>
            <tr>
              <th><strong>算法</strong>
              </th>
              <th><strong>适用场景</strong>
              </th>
              <th><strong>优点</strong>
              </th>
              <th><strong>缺点</strong>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>K-Means</td>
              <td>球形数据、规模较大的数据集</td>
              <td>简单高效，易实现</td>
              <td>对初始值敏感，无法处理非球形簇</td>
            </tr>
            <tr>
              <td>层次聚类</td>
              <td>小规模数据、生成层次结构</td>
              <td>不需要预定义簇数量，结果直观</td>
              <td>计算复杂度高，敏感于噪声</td>
            </tr>
            <tr>
              <td>DBSCAN</td>
              <td>任意形状的簇，含噪声的数据</td>
              <td>自动确定簇数，对噪声鲁棒</td>
              <td>参数敏感，高维数据效果差</td>
            </tr>
            <tr>
              <td>GMM</td>
              <td>不同形状和大小的簇</td>
              <td>能处理软聚类，适合概率建模</td>
              <td>对初始值敏感，计算复杂度高</td>
            </tr>
            <tr>
              <td>CLIQUE</td>
              <td>大规模高维数据</td>
              <td>高效，适合网格划分的场景</td>
              <td>依赖网格划分，可能丢失细节</td>
            </tr>
          </tbody>
        </table>
        
<h3><strong>如何选择聚类算法？</strong></h3>

        <ol>
          <li>
            <p><strong>数据规模</strong>：</p>
            <ul>
              <li>小数据集：层次聚类。</li>
              <li>大规模数据集：K-Means 或 DBSCAN。</li>
            </ul>
          </li>
          <li>
            <p><strong>数据分布</strong>：</p>
            <ul>
              <li>球形数据：K-Means。</li>
              <li>非球形数据：DBSCAN 或 GMM。</li>
            </ul>
          </li>
          <li>
            <p><strong>是否包含噪声</strong>：</p>
            <ul>
              <li>包含噪声：DBSCAN。</li>
              <li>不包含噪声：K-Means 或层次聚类。</li>
            </ul>
          </li>
          <li>
            <p><strong>结果要求</strong>：</p>
            <ul>
              <li>精确分组：GMM。</li>
              <li>快速近似：K-Means。</li>
            </ul>
          </li>
        </ol>
        
<h2>14.介绍一下颜色量化算法的原理</h2>

        <p><strong>颜色量化（Color Quantization）</strong> 是一种减少图像中颜色数量的技术，通常用于图像压缩、减少存储空间或简化图像处理。颜色量化的目标是将图像中的颜色从数百万种减少到较少的几种（通常是2到256种），同时尽量保持图像的视觉质量。</p>
        <p>其核心思想是通过聚类算法将颜色空间划分为若干个区域，并用每个区域的代表性颜色替换该区域中的所有颜色。常用的算法包括K-Means、K-Means++、中值切割和八叉树量化等。</p>
        
<h3>颜色量化的基本原理</h3>

        <p>颜色量化的核心思想是将图像中的颜色空间划分为若干个区域（称为“聚类”或“颜色桶”），然后用每个区域中的代表性颜色（通常是该区域的平均颜色）替换该区域中的所有颜色。这样，图像中的颜色数量就被减少到与聚类数量相同的数量。</p>
        
<h3>颜色量化的步骤</h3>

        <ol>
          <li>
            <p><strong>选择颜色空间</strong>：</p>
            <ul>
              <li>颜色量化通常在RGB颜色空间中进行，因为RGB是图像处理中最常用的颜色表示方式。每个像素的颜色由三个分量（R、G、B）表示，每个分量的取值范围是0到255。</li>
            </ul>
          </li>
          <li>
            <p><strong>确定聚类数量（k）</strong>：</p>
            <ul>
              <li>用户指定要减少到的颜色数量（k）。例如，如果k=16，图像将被量化为16种颜色。</li>
            </ul>
          </li>
          <li>
            <p><strong>初始化聚类中心</strong>：</p>
            <ul>
              <li>选择k个初始聚类中心。这些中心可以是随机选择的像素颜色，也可以通过其他方法（如K-Means++）选择。</li>
            </ul>
          </li>
          <li>
            <p><strong>分配像素到聚类</strong>：</p>
            <ul>
              <li>对于图像中的每个像素，计算它与每个聚类中心的距离（通常是欧几里得距离），并将其分配到距离最近的聚类中心。</li>
            </ul>
          </li>
          <li>
            <p><strong>更新聚类中心</strong>：</p>
            <ul>
              <li>对于每个聚类，计算该聚类中所有像素的平均颜色，并将该平均颜色作为新的聚类中心。</li>
            </ul>
          </li>
          <li>
            <p><strong>迭代</strong>：</p>
            <ul>
              <li>重复步骤4和步骤5，直到聚类中心不再显著变化（即误差小于某个阈值），或者达到最大迭代次数。</li>
            </ul>
          </li>
          <li>
            <p><strong>生成量化图像</strong>：</p>
            <ul>
              <li>将每个像素的颜色替换为其所属聚类的中心颜色，生成量化后的图像。</li>
            </ul>
          </li>
        </ol>
        
<h3>颜色量化的数学原理</h3>

        <p>颜色量化可以看作是一个<strong>聚类问题</strong>，其中每个像素的颜色是一个三维向量（R, G, B），聚类的目标是将这些向量划分为k个组，使得组内的颜色尽可能相似，组间的颜色尽可能不同。</p>
        
<h4>1. <strong>距离度量</strong>：</h4>

        <ul>
          <li>通常使用欧几里得距离来度量两个颜色之间的相似性。对于两个颜色向量 $C_1 = (R_1, G_1, B_1)$ 和 $C_2 = (R_2,
            G_2, B_2)$ ，它们之间的距离为：</li>
        </ul>
        <p>$$ d(C_1, C_2) = \sqrt{(R_1 - R_2)^2 + (G_1 - G_2)^2 + (B_1 - B_2)^2}
          $$</p>
        
<h4>2. <strong>聚类中心的更新</strong>：</h4>

        <ul>
          <li>对于每个聚类，新的聚类中心是该聚类中所有像素颜色的平均值。假设聚类 $C_j$ 包含 $n_j$ 个像素，其颜色向量为 $C_{j1}, C_{j2},
            \dots, C_{jn_j}$ ，则新的聚类中心 $\mu_j$ 为：</li>
        </ul>
        <p>$$ \mu_j = \frac{1}{n_j} \sum_{i=1}^{n_j} C_{ji} $$</p>
        
<h4>3. <strong>误差计算</strong>：</h4>

        <ul>
          <li>误差通常定义为所有像素与其所属聚类中心的距离之和。误差越小，表示聚类效果越好。误差的计算公式为：</li>
        </ul>
        <p>$$ E = \sum_{j=1}^{k} \sum_{i=1}^{n_j} d(C_{ji}, \mu_j) $$</p>
        <ul>
          <li>当误差不再显著减少时，算法停止。</li>
        </ul>
        
<h3>颜色量化的改进方法</h3>

        <ol>
          <li>
            <p><strong>K-Means++</strong>：</p>
            <ul>
              <li>K-Means++是一种改进的K-Means算法，它在选择初始聚类中心时，倾向于选择距离较远的点，从而避免陷入局部最优解。</li>
            </ul>
          </li>
          <li>
            <p><strong>中值切割（Median Cut）</strong>：</p>
            <ul>
              <li>中值切割算法通过递归地将颜色空间划分为更小的区域，直到达到所需的颜色数量。每个区域的颜色取该区域的中值或平均值。</li>
            </ul>
          </li>
          <li>
            <p><strong>八叉树量化（Octree Quantization）</strong>：</p>
            <ul>
              <li>八叉树量化通过构建一个八叉树数据结构来表示颜色空间，并在树中进行颜色合并，从而实现颜色量化。</li>
            </ul>
          </li>
        </ol>
        
<h3>颜色量化的应用</h3>

        <ol>
          <li>
            <p><strong>图像压缩</strong>：</p>
            <ul>
              <li>颜色量化可以显著减少图像的文件大小，特别是在GIF和PNG等格式中，这些格式支持有限的颜色数量。</li>
            </ul>
          </li>
          <li>
            <p><strong>图像处理</strong>：</p>
            <ul>
              <li>在图像处理中，颜色量化可以简化图像，使其更容易处理和分析。</li>
            </ul>
          </li>
          <li>
            <p><strong>艺术效果</strong>：</p>
            <ul>
              <li>颜色量化可以用于生成具有艺术效果的图像，例如将照片转换为类似海报或卡通的效果。</li>
            </ul>
          </li>
        </ol>
        
<h2>15.介绍一下K-Means++算法的原理</h2>

        <p><strong>K-Means++</strong> 是 K-Means 聚类算法的一种改进版本，主要用于解决 K-Means 算法对初始聚类中心敏感的问题。K-Means++
          通过一种更智能的方式选择初始聚类中心，从而减少算法陷入局部最优解的可能性，并提高聚类效果。</p>
        <p>其核心思想是选择距离较远的点作为初始聚类中心，从而使得初始聚类中心更加分散。</p>
        
<h3>K-Means++ 的核心思想</h3>

        <p>K-Means++ 的核心思想是：<strong>初始聚类中心的选择应该尽可能分散</strong>，即选择的初始中心点之间的距离应该尽可能大。这样可以避免初始中心点过于集中，导致聚类结果不理想。</p>
        
<h3>K-Means++ 的步骤</h3>

        <p>K-Means++ 的初始化过程分为以下几个步骤：</p>
        <ol>
          <li>
            <p><strong>随机选择第一个聚类中心</strong>：</p>
            <ul>
              <li>从数据集中随机选择一个点作为第一个聚类中心。</li>
            </ul>
          </li>
          <li>
            <p><strong>计算每个点到最近聚类中心的距离</strong>：</p>
            <ul>
              <li>对于数据集中的每个点，计算它与当前已选聚类中心的最短距离 $D(x)$ 。</li>
            </ul>
          </li>
          <li>
            <p><strong>根据距离选择下一个聚类中心</strong>：</p>
            <ul>
              <li>按照与已选聚类中心的距离的平方成比例的概率，选择下一个聚类中心。具体来说，每个点被选中的概率为：</li>
            </ul>
          </li>
        </ol>
        <p>$$ P(x) = \frac{D(x)^2}{\sum_{x' \in X} D(x')^2} $$</p>
        <ul>
          <li>这意味着距离已选聚类中心越远的点，被选中的概率越大。</li>
        </ul>
        <ol>
          <li>
            <p><strong>重复步骤2和步骤3</strong>：</p>
            <ul>
              <li>重复上述过程，直到选择出 k 个初始聚类中心。</li>
            </ul>
          </li>
          <li>
            <p><strong>运行标准的 K-Means 算法</strong>：</p>
            <ul>
              <li>使用 K-Means++ 选择的初始聚类中心，运行标准的 K-Means 算法进行聚类。</li>
            </ul>
          </li>
        </ol>
        
<h3>K-Means++ 的数学原理</h3>

        <p>K-Means++ 的数学原理主要基于概率选择和距离度量。</p>
        
<h4>1. <strong>距离度量</strong>：</h4>

        <ul>
          <li>对于数据集中的每个点 $x$ ，计算它与当前已选聚类中心的最短距离 $D(x)$ ：</li>
        </ul>
        <p>$$ D(x) = \min_{c \in C} d(x, c) $$</p>
        <ul>
          <li>其中， $C$ 是当前已选的聚类中心集合， $d(x, c)$ 是点 $x$ 与聚类中心 $c$ 之间的距离（通常是欧几里得距离）。</li>
        </ul>
        
<h4>2. <strong>概率选择</strong>：</h4>

        <ul>
          <li>根据每个点的 $D(x)^2$ 计算其被选为下一个聚类中心的概率：</li>
        </ul>
        <p>$$ P(x) = \frac{D(x)^2}{\sum_{x' \in X} D(x')^2} $$</p>
        <ul>
          <li>这意味着距离已选聚类中心越远的点，被选中的概率越大。</li>
        </ul>
        
<h4>3. <strong>选择下一个聚类中心</strong>：</h4>

        <ul>
          <li>根据计算出的概率分布，随机选择一个点作为下一个聚类中心。</li>
        </ul>
        
<h3>K-Means++ 的优势</h3>

        <ol>
          <li>
            <p><strong>更好的初始聚类中心</strong>：</p>
            <ul>
              <li>K-Means++ 通过选择距离较远的点作为初始聚类中心，避免了初始中心点过于集中的问题，从而提高了聚类效果。</li>
            </ul>
          </li>
          <li>
            <p><strong>减少陷入局部最优解的可能性</strong>：</p>
            <ul>
              <li>由于初始聚类中心的选择更加分散，K-Means++ 减少了算法陷入局部最优解的可能性，通常能够得到更好的聚类结果。</li>
            </ul>
          </li>
          <li>
            <p><strong>计算效率</strong>：</p>
            <ul>
              <li>虽然 K-Means++ 的初始化过程比随机选择初始中心点更复杂，但其计算复杂度仍然是可接受的，并且在大多数情况下，K-Means++ 能够显著减少
                K-Means 算法的迭代次数。</li>
            </ul>
          </li>
        </ol>
        
<h3>K-Means++ 的应用</h3>

        <p>K-Means++ 广泛应用于各种需要聚类的场景，特别是在以下领域：</p>
        <ol>
          <li>
            <p><strong>图像处理</strong>：</p>
            <ul>
              <li>用于图像分割、颜色量化等任务。</li>
            </ul>
          </li>
          <li>
            <p><strong>数据挖掘</strong>：</p>
            <ul>
              <li>用于客户细分、市场分析等。</li>
            </ul>
          </li>
          <li>
            <p><strong>机器学习</strong>：</p>
            <ul>
              <li>用于特征提取、降维等。</li>
            </ul>
          </li>
        </ol>
      </div>
    </div>
  </body>

</html>