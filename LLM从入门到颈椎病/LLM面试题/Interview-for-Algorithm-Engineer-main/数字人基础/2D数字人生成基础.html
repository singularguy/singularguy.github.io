<html>
  
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../../style.css">
    <base target="_parent">
    <title data-trilium-title>2D数字人生成基础</title>
  </head>
  
  <body>
    <div class="content">
       <h1 data-trilium-h1>2D数字人生成基础</h1>

      <div class="ck-content">
        <hr />
        
<h2>created: 2025-01-25T00:41
updated: 2025-01-25T13:23</h2>

        
<h2>目录</h2>

        
<h2>第一章 可控数字人生成</h2>

        <ul>
          <li><a href="#1.2D%E6%95%B0%E5%AD%97%E4%BA%BA%E7%94%9F%E6%88%90%E6%9C%89%E4%BB%80%E4%B9%88%E6%96%B9%E5%90%91?">1.2D数字人生成有什么方向?</a>
          </li>
          <li><a href="#2.%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8E%E4%B8%80%E4%B8%AA%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E6%89%A9%E5%B1%95%E5%88%B0%E8%A7%86%E9%A2%91?">2.如何基于一个图像生成模型扩展到视频?</a>
          </li>
          <li><a href="#3.%E4%BA%BA%E4%BD%93%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B?">3.人体驱动的方法有哪些?</a>
          </li>
          <li><a href="#4.%E5%8F%AF%E6%8E%A7%E4%BA%BA%E4%BD%93%E7%94%9F%E6%88%90%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E5%A6%82%E4%BD%95%E5%81%9A%E5%88%B0%E9%A9%B1%E5%8A%A8?">4.可控人体生成的目的是什么，如何做到驱动?</a>
          </li>
          <li><a href="#5.%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E4%BA%BA%E4%BD%93%E9%A9%B1%E5%8A%A8%E7%94%9F%E6%88%90%E4%B8%AD%E8%84%B8%E9%83%A8%E7%9A%84ID%E7%9B%B8%E4%BC%BC%E5%BA%A6?">5.如何提升人体驱动生成中脸部的ID相似度?</a>
          </li>
          <li><a href="#6.Animate-Anyone%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%92%8C%E5%8E%9F%E7%90%86">6.Animate-Anyone的模型结构和原理</a>
          </li>
          <li><a href="#7.ID%E4%BF%9D%E6%8C%81%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E5%92%8C%E6%8D%A2%E8%84%B8%E7%9A%84%E5%8C%BA%E5%88%AB">7.ID保持图像生成和换脸的区别</a>
          </li>
          <li><a href="#8.%E6%9C%89%E5%93%AA%E4%BA%9B%E4%B8%93%E6%B3%A8%E4%BA%BA%E5%83%8F%E7%94%9F%E6%88%90%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B?">8.有哪些专注人像生成的预训练模型?</a>
          </li>
        </ul>
        
<h2>第一章 可控数字人生成</h2>

        
<h2>1.2D数字人有什么方向?</h2>

        <p>目前，2D数字人生成的方向包括：</p>
        <ol>
          <li>可控人体生成</li>
        </ol>
        <ul>
          <li>‌<strong>人体驱动</strong> 
          </li>
          <li><strong>虚拟换衣</strong>
          </li>
        </ul>
        <ol>
          <li>可控人脸生成</li>
        </ol>
        <ul>
          <li><strong>人脸属性编辑</strong>
          </li>
          <li><strong>换脸</strong>
          </li>
          <li><strong>目标人脸引导的人脸驱动生成</strong>
          </li>
          <li><strong>音频引导的人脸驱动生成</strong>
          </li>
        </ul>
        <ol>
          <li>ID保持的人体图像/视频生成</li>
        </ol>
        <ul>
          <li><strong>视频写真</strong>
          </li>
        </ul>
        
<h2>2.如何基于一个图像生成模型扩展到视频?</h2>

        <p>基于GAN的方案构造视频数据集抽帧进行训练即可，无需添加额外的帧间一致性模块，测试时就可以达到不错的帧间稳定性。由于扩散模型方案建模的多样性强，如果直接逐帧进行推理会导致帧间一致性较差，目前常用的解决方式是采用SD1.5或者SDXL基底模型的基础上，第一阶段使用人脸或人体数据集将底模调整到对应的domain，第二阶段插入一个类似AnimateDiff中提出的Motion
          Module提升帧间一致性。</p>
        
<h2>3.人体驱动的方法有哪些?</h2>

        <table>
          <thead>
            <tr>
              <th></th>
              <th>T2V model</th>
              <th>Pose Condition</th>
              <th>Injection Type</th>
              <th>Others</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Magic Animate</td>
              <td>AnimateDiff</td>
              <td>DensePose</td>
              <td>ReferenceNet+ControlNet</td>
              <td>w/o. alignment</td>
            </tr>
            <tr>
              <td>Animate Anyone</td>
              <td>AnimateDiff</td>
              <td>DWPose</td>
              <td>ReferenceNet+Pose Encoder+CLIP</td>
              <td>w/o. alignment</td>
            </tr>
            <tr>
              <td>Moore-Animate Anyone (AA unofficial implementation)</td>
              <td>AnimateDiff</td>
              <td>DWPose</td>
              <td>ReferenceNet+Pose Encoder+CLIP</td>
              <td>w/o. alignment</td>
            </tr>
            <tr>
              <td>MusePose</td>
              <td>AnimateDiff</td>
              <td>DWPose</td>
              <td>ReferenceNet+Pose Encoder+CLIP</td>
              <td>w/. alignment (2d)</td>
            </tr>
            <tr>
              <td>Champ</td>
              <td>AnimateDiff</td>
              <td>DensePose/DWPose/Normal/Depth</td>
              <td>ReferenceNet+Pose Encoder+CLIP</td>
              <td>w/. alignment (2d)</td>
            </tr>
            <tr>
              <td>UniAnimate</td>
              <td>AnimateDiff</td>
              <td>DWPose</td>
              <td>Pose Encoder+CLIP</td>
              <td>w/. alignment (2d)</td>
            </tr>
            <tr>
              <td>ViVidPose</td>
              <td>Stable Video Diffusion</td>
              <td>DWPose/SMPLX-Shape</td>
              <td>ReferenceNet+Pose Encoder+CLIP+Face Encoder</td>
              <td>w/. alignment (3d)</td>
            </tr>
          </tbody>
        </table>
        
<h2>4.可控人体生成的目的是什么，如何做到驱动?</h2>

        <p>不管是文本生成、图像生成、视频生成，如果没有具备可控性，AI作为一个工具，本身能够带来的效能的提升就非常的有限。可控人体生成的目的就是希望通过输入一段目标的姿态序列和一张参考人像图片，能够保持参考人像的背景，人物特征的同时，生成其按照目标序列进行运动的人像视频。</p>
        
<h2>5.如何提升人体驱动生成中脸部的ID相似度?</h2>

        <p>人脸生成，是 AI 生成视频中最难的场景之一。首先是因为人类对人脸本身就很敏感。一个细微的肌肉表情，就能被解读出不同的含义。人们自拍经常要拍几十张相似的照片，才能挑到合适的角度。因此涉及到人脸的一些形变，很容易就会引起我们的注意。在早期的人体驱动工作中，研究者们并没有过多的采用一些额外的模块约束参考人像和生成人像的脸部ID一致性，仅采用ReferenceNet和CLIP
          Image Encoder来提取了参考人像信息。在此基础上，有几种方式可以提升脸部ID一致性：</p>
        <ol>
          <li>在训练过程中，计算生成人脸和参考人脸的ID Similarity，并加入ID Loss，</li>
          <li>对于参考人像的人脸区域，使用人脸识别网络提取对应的ID信息，在主干网络中注入</li>
        </ol>
        
<h2>6.Animate-Anyone的模型结构和原理</h2>

        <p>AnimateAnyone是一种能够将角色图像转换为所需姿势序列控制的动画视频的方法，继承了Stable Diffusion模型的网络设计和预训练权重，并在UNet中插入Motion
          Module以适应多帧输入。为了解决保持外观一致性的挑战，引入了ReferenceNet，专门设计为UNet结构来捕获参考图像的空间细节。</p>
        <p>
          <img src="api/images/rak2aNGmKiVu/animate_anyone.png" />
        </p>
        
<h2>7.ID保持图像生成和换脸的区别</h2>

        <p>ID保持图像生成和换脸都可以达到生成和参考人脸相似的人体图像。这两者区别在于，ID保持图像生成是在生成过程中保持了参考图像的ID信息，而换脸则是将目标图像的人脸替换为参考图像的人脸。ID保持图像生成的目的是生成一个新的图像，使其在视觉上与参考图像相似，但不是完全相同。而换脸则是将目标图像的人脸替换为参考图像的人脸，使得目标图像的人脸与参考图像的人脸完全一致。其中，换脸还需要保持目标图像的其他信息不变，如头发、衣服等，而ID保持图像生成则不需要保持这些信息。</p>
        
<h2>8.有哪些专注人像生成的预训练模型?</h2>

        <p>随着大规模预训练模型的发展，专注人像生成的预训练模型也在不断涌现。目前，一些专注人像生成的预训练模型包括：</p>
        <ul>
          <li>
            <p><strong>CosmicMan</strong>: 一个基于文本的高保真人物图像生成模型，能够产生与文本描述精确对齐的逼真人物图像。CosmicMan在图像质量和文本-图像对齐方面优于现有模型，如Stable
              Diffusion和Imagen。它在2D和3D人物生成任务中展现了实用性和潜力。</p>
          </li>
          <li>
            <p><strong>Arc2Face</strong>: 专注于使用人脸识别技术的核心特征来引导图像的生成，从而实现在各种任务中保持人脸身份的一致性。这意味着Arc2Face可以用于创建非常符合特定人物身份特征的人脸图像，为人脸识别、数字娱乐以及安全领域等提供了新的可能性。</p>
            <p>
              <img src="api/images/N0ci66S6dyOw/arc2face.png" />
            </p>
          </li>
        </ul>
      </div>
    </div>
  </body>

</html>