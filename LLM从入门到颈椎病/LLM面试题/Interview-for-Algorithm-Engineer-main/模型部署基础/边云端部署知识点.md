# 边云端部署知识点
* * *

created: 2025-01-25T00:41 updated: 2025-01-26T02:20
---------------------------------------------------

目录
--

*   [1.X86和ARM架构在深度学习侧的区别？](#user-content-1.x86%E5%92%8Carm%E6%9E%B6%E6%9E%84%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BE%A7%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F)
*   [2.为何在AI端侧设备一般不使用传统图像算法？](#user-content-2.%E4%B8%BA%E4%BD%95%E5%9C%A8ai%E7%AB%AF%E4%BE%A7%E8%AE%BE%E5%A4%87%E4%B8%80%E8%88%AC%E4%B8%8D%E4%BD%BF%E7%94%A8%E4%BC%A0%E7%BB%9F%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%EF%BC%9F)
*   [3.端侧部署时整个解决方案的核心指标？](#user-content-3.%E7%AB%AF%E4%BE%A7%E9%83%A8%E7%BD%B2%E6%97%B6%E6%95%B4%E4%B8%AA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8C%87%E6%A0%87%EF%BC%9F)
*   [4.主流AI端侧硬件平台有哪些？](#user-content-4.%E4%B8%BB%E6%B5%81ai%E7%AB%AF%E4%BE%A7%E7%A1%AC%E4%BB%B6%E5%B9%B3%E5%8F%B0%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F)
*   [5.主流AI端侧硬件平台一般包含哪些模块？](#user-content-5.%E4%B8%BB%E6%B5%81ai%E7%AB%AF%E4%BE%A7%E7%A1%AC%E4%BB%B6%E5%B9%B3%E5%8F%B0%E4%B8%80%E8%88%AC%E5%8C%85%E5%90%AB%E5%93%AA%E4%BA%9B%E6%A8%A1%E5%9D%97%EF%BC%9F)
*   [6.算法工程师该如何看待硬件侧知识？](#user-content-6.%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E8%AF%A5%E5%A6%82%E4%BD%95%E7%9C%8B%E5%BE%85%E7%A1%AC%E4%BB%B6%E4%BE%A7%E7%9F%A5%E8%AF%86%EF%BC%9F)
*   [7.优化模型端侧性能的一些方法](#user-content-7.%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E7%AB%AF%E4%BE%A7%E6%80%A7%E8%83%BD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%96%B9%E6%B3%95)
*   [8.目前主流的端侧算力芯片有哪些种类？](#user-content-8.%E7%9B%AE%E5%89%8D%E4%B8%BB%E6%B5%81%E7%9A%84%E7%AB%AF%E4%BE%A7%E7%AE%97%E5%8A%9B%E8%8A%AF%E7%89%87%E6%9C%89%E5%93%AA%E4%BA%9B%E7%A7%8D%E7%B1%BB%EF%BC%9F)
*   [9.介绍一下XPU、MLU、MUSA各自的特点](#user-content-9.%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8BXPU%E3%80%81MLU%E3%80%81MUSA%E5%90%84%E8%87%AA%E7%9A%84%E7%89%B9%E7%82%B9)

1.X86和ARM架构在深度学习侧的区别？
---------------------

AI服务器与PC端一般都是使用X86架构，因为其高性能；AI端侧设备（手机/端侧盒子等）一般使用ARM架构，因为需要低功耗。

X86指令集中的指令是复杂的，一条很长指令就可以很多功能；而ARM指令集的指令是很精简的，需要几条精简的短指令完成很多功能。

X86的方向是高性能方向，因为它追求一条指令完成很多功能；而ARM的方向是面向低功耗，要求指令尽可能精简。

2.为何在AI端侧设备一般不使用传统图像算法？
-----------------------

AI端侧设备多聚焦于深度学习算法模型的加速与赋能，而传统图像算法在没有加速算子赋能的情况下，在AI端侧设备无法发挥最优的性能。

3.端侧部署时整个解决方案的核心指标？
-------------------

1.  精度
2.  耗时
3.  内存占用
4.  功耗

4.主流AI端侧硬件平台有哪些？
----------------

1.  英伟达
2.  海思
3.  寒武纪
4.  比特大陆
5.  昇腾
6.  登临
7.  联咏
8.  安霸
9.  耐能
10.  爱芯
11.  瑞芯

5.主流AI端侧硬件平台一般包含哪些模块？
---------------------

1.  视频编解码模块
2.  CPU核心处理模块
3.  AI协处理器模块
4.  GPU模块
5.  DSP模块
6.  DDR内存模块
7.  数字图像处理模块

6.算法工程师该如何看待硬件侧知识？
------------------

GPU乃至硬件侧的整体逻辑，是CV算法工作中必不可少的组成部分，也是算法模型所依赖的重要物理载体。

### GPU的相关知识

现在AI行业有个共识，认为是数据的爆发和算力的突破开启了深度学习在计算机视觉领域的“乘风破浪”，而其中的算力，主要就是指以GPU为首的计算平台。

GPU（Graphical Processing Unit）从最初用来进行图形处理和渲染（玩游戏），到通过CUDA/OpenCL库以及相应的工程开发之后，成为深度学习模型在学术界和工业界的底层计算工具，其有以下的一些特征：

1.  异构计算：GPU能作为CPU的协处理器与CPU协同运算。
2.  单指令流多数据流（SIMD）架构：使得同一个指令（比如对图像数据的一些操作），可以同时在多个像素点上并行计算，从而得到比较大的吞吐量，深度学习中大量的矩阵操作，让GPU成为一个非常适合的计算平台。
3.  多计算核心。比如Nvidia的GTX980GPU中，在和i7-5960CPU差不多的芯片面积上，有其128倍的运算速度。GTX980中有16个流处理单元，每个流处理单元中包含着128个CUDA计算核心，共有2048个GPU运算单元，与此同时i7-5960CPU只有16个类似的计算单元。
4.  CUDA模块。作为GPU架构中的最小单元，它的设计和CPU有着非常类似的结构，其中包括了一个浮点运算单元，整型运算单元以及控制单元。一个流处理单元中的CUDA模块将执行同一个指令，但是会作用在不同的数据上。多CUDA模块意味着GPU有更加高的计算性能，但更重要的是在算法侧有没有高效地调度和使用。
5.  计算核心频率。即时钟频率，代表每一秒内能进行同步脉冲次数。就核心频率而言，CPU要高于GPU。由于GPU采用了多核逻辑，即使提高一些频率，其实对于总体性能影响不会特别大。
6.  内存架构。GPU的多层内存架构包括全局内存，2级缓存，和芯片上的存储（包括寄存器，和1级缓存共用的共享内存，只读/纹理缓存和常量缓存）。

![](边云端部署知识点_24cbb3b8-c530-4eec-a3.jpg)

在使用GPU时，在命令行输入nvidia-smi命令时会打印出一张表格，其中包含了GPU当时状态的所有参数信息。

![](边云端部署知识点_9772a00f-8006-4d93-ac.jpg)

CUDA/cuDNN/OpenCL科普小知识：

1.  CUDA是NVIDIA推出的用于GPU的并行计算框架。
2.  cuDNN是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。
3.  OpenCL是由苹果（Apple）公司发起，业界众多著名厂商共同制作的面向异构系统通用目的并行编程的开放式、免费标准，也是一个统一的编程环境。

### 深度学习的端侧设备

深度学习的端侧设备，又可以叫做边缘计算设备，深度学习特别是CV领域中，模型+端侧设备的组合能够加快业务的即时计算，决策和反馈能力，极大释放AI可能性。

![](边云端部署知识点_6863aa5d-a89b-4b08-a5.jpg)

深度学习的端侧设备主要由ARM架构的CPU+ GPU/TPU/NPU等协处理器 + 整体功耗 + 外围接口 + 工具链等部分组成，也是算法侧对端侧设备进行选型要考虑的维度。

在实际业务中，根据公司的不同，算法工程师涉及到的硬件侧范围也会不一样。如果公司里硬件和算法由两个部门分别负责，那么算法工程师最多接触到的硬件侧知识就是硬件性能评估，模型转换与模型硬件侧验证，一些硬件高层API接口的开发与使用；如果公司里没有这么细分的部门，那么算法工程师可能就会接触到端侧的视频编解码，模型推理加速，Opencv，FFmpeg，Tensor RT，工具链开发等角度的知识。

![](边云端部署知识点_580db620-75b6-4254-a7.jpg)

### 算法工程师该如何看待硬件侧

首先，整体上还是要将硬件侧工具化，把端侧设备当做算法模型的一个下游载体，会熟练的选型与性能评估更加重要。

端侧设备是算法产品整体解决方案中一个非常重要的模块，算法+硬件的范式将在未来的边缘计算与万物智能场景中持续发力。

在日常业务中，算法模型与端侧设备的适配性与兼容性是必须要考虑的问题，端侧设备是否兼容一些特殊的网络结构？算法模型转化并部署后，精度是否下降？功耗与耗时能否达标？等等都让算法工程师的模型设计逻辑有更多的抓手。

7.优化模型端侧性能的一些方法
---------------

1.  设计能最大限度挖掘AI协处理器性能的模型结构。
2.  多模型共享计算内存。
3.  减少模型分支结构，减少模型元素级操作。
4.  卷积层的输入和输出特征通道数相等时MAC最小，以提升模型Inference速度。

8.目前主流的端侧算力芯片有哪些种类？
-------------------

AI端侧算力设备（如NPU、TPU、VPU、FPGA等）目前正在快速发展，这些设备专门设计用于加速AIGC、传统深度学习、自动驾驶等领域任务。它们在性能和效率方面大大超过了传统的CPU和GPU。以下是Rocky对这些AI端侧算力的详细介绍：

### 1\. NPU（Neural Processing Unit）

NPU，即神经处理单元，是一种专门用于加速神经网络计算的处理器。NPU通常集成在移动设备、物联网设备和其他嵌入式系统中，以提升AI应用的性能。

#### 特点与优势：

*   **高效能耗比**：NPU在进行神经网络计算时具有高能效，适用于资源受限的设备。
*   **专用硬件设计**：为了优化矩阵运算和卷积操作，NPU设计了专门的硬件加速器。
*   **实时处理**：NPU能实现低延迟的实时AI推理，非常适合智能手机、摄像头等需要实时处理的设备。
*   **集成性强**：NPU常与其他处理单元（如CPU、GPU）集成在同一个芯片上（如SoC），以提供全面的计算能力。

### 2\. TPU（Tensor Processing Unit）

TPU，即张量处理单元，是Google开发的一种专用AI加速器，主要用于加速TensorFlow框架下的机器学习任务。

#### 特点与优势：

*   **高性能**：TPU能够提供极高的计算能力，特别是在处理大规模矩阵运算和深度学习模型训练时。
*   **定制化设计**：TPU为特定的AI工作负载（如矩阵乘法、卷积运算）进行了优化，显著提升了性能。
*   **大规模部署**：TPU被广泛部署在Google的数据中心，用于支持Google的各项AI服务，如搜索、广告、翻译等。

#### 版本和架构：

*   **TPU v1**：主要用于推理任务，每秒可执行92万亿次浮点运算（92 TFLOPS）。
*   **TPU v2**和**TPU v3**：增强了训练能力，分别具有每秒180 TFLOPS和420 TFLOPS的计算能力。
*   **TPU v4**：最新版本，进一步提升了性能和能效，适用于更大规模、更复杂的AI任务。

### 3\. VPU（Vision Processing Unit）

VPU，即视觉处理单元，是一种专门设计用于计算机视觉和人工智能任务的处理器。VPUs的主要目标是以高效的能耗比处理复杂的视觉计算任务，适用于各种嵌入式和边缘设备。

#### 特点与优势：

*   VPU专注于低功耗的计算机视觉任务，适用于嵌入式系统和边缘设备。
*   提供高效的图像处理和神经网络推理能力。

### 4\. FPGA（Field Programmable Gate Array）

FPGA，即现场可编程门阵列，是一种高度可编程的集成电路，可以根据特定应用的需求重新配置其硬件电路。FPGA在AI和机器学习中广泛应用于需要高灵活性和低延迟的任务。

#### 特点与优势：

*   FPGA具有高度灵活性，可根据需求重新配置电路结构。
*   提供较低的延迟和高效的能耗比，适用于特定AI任务的加速。
*   能够实现高度并行计算，适用于实时处理应用。

### 5\. Huawei Ascend

华为昇腾系列AI处理器包括适用于云端和边缘计算的多种型号，提供高性能的AI计算能力。

#### 特点与优势：

*   华为的昇腾系列AI芯片，包括适用于云端和边缘计算的不同版本，如Ascend 910（高性能）和Ascend 310（边缘计算）。
*   提供高度集成的AI计算能力，支持多种AI框架和模型。

### 6\. Graphcore IPU（Intelligence Processing Unit）

Graphcore IPU是一种专门设计用于机器智能任务的处理器，采用全新的计算架构，优化了计算和内存访问。

#### 特点与优势：

*   IPU专为机器智能任务设计，采用了全新的计算架构，优化了计算和内存访问。
*   能够高效处理稀疏计算和动态计算图，适用于复杂的AI模型。

### 总结

这些AI端侧设备显著提升了AIGC、传统深度学习、自动驾驶任务的性能和能效，推动了AI技术的快速发展和应用扩展。不同的加速器在设计上各有侧重，适用于不同的应用场景，满足了多样化的AI计算需求。

9.介绍一下XPU、MLU、MUSA各自的特点
-----------------------

**XPU**、**MLU** 和 **MUSA** 都是不同的硬件加速器架构，各自由不同的公司开发并用于加速 AI 和高性能计算任务。它们的主要功能是通过专门设计的硬件来加速大规模计算任务，特别是在AIGC、传统深度学习、自动驾驶等领域。以下是对每个加速器架构的详细介绍和它们的特点：

### 1\. **XPU（Cross Processing Unit）**

*   **背景**：**XPU** 通常用作 Intel 的新一代可扩展异构计算平台的统称，涵盖了不同的硬件架构，用于处理 AI、数据分析和高性能计算任务。Intel 的 XPU 通过整合 CPU、GPU、FPGA 和专用 AI 加速器（如 Nervana 或 Habana）等硬件，实现广泛的计算场景支持。
    
*   **特点**：
    
    *   **异构计算**：XPU 的主要特点是它能够支持异构计算，即在同一计算框架中结合 CPU、GPU、FPGA 和其他 AI 加速硬件来提高计算性能。
    *   **通用性强**：XPU 设计的核心是通用计算，能够支持从数据中心到边缘设备的各种计算需求。
    *   **软件兼容性**：借助 **oneAPI**，Intel 提供了一个统一的编程平台，使开发者能够使用单一代码库来开发支持不同硬件加速的应用程序，这大大简化了异构计算的开发复杂度。
*   **应用领域**：
    
    *   高性能计算（HPC）
    *   AI 加速（如训练和推理任务）
    *   数据分析和处理

### 2\. **MLU（Machine Learning Unit）**

*   **背景**：**MLU** 是由中国公司 **寒武纪科技**（Cambricon）开发的专用机器学习处理器，专门用于加速深度学习和 AI 相关的任务。MLU 处理器通常部署在数据中心、云计算平台和边缘计算设备中。
    
*   **特点**：
    
    *   **AI 加速优化**：MLU 是为 AI 任务优化的芯片，特别擅长深度学习模型的训练和推理任务。与传统的 GPU 不同，它是专门为处理神经网络计算所设计的。
    *   **自定义架构**：寒武纪在设计 MLU 架构时，采用了针对神经网络计算的定制硬件，使其在神经网络计算时比传统的通用处理器更高效。
    *   **高吞吐量与能效比**：MLU 的设计目标是提供高吞吐量，并在大规模并行计算中保持较高的能效比。这使得 MLU 特别适合在数据中心进行大规模深度学习任务的处理。
*   **应用领域**：
    
    *   数据中心 AI 加速
    *   边缘计算设备
    *   云端 AI 服务

### 3\. **MUSA（Machine Unified Smart Accelerator）**

*   **背景**：**MUSA** 是由 **摩尔线程**（Moore Threads）开发的一种 AI 加速架构。摩尔线程是一家中国的半导体公司，致力于开发用于高性能图形渲染和 AI 计算的图形处理器和加速器。
    
*   **特点**：
    
    *   **多功能性**：MUSA 是一种智能加速器，能够同时支持图形渲染和 AI 计算任务。与单纯的 AI 加速器不同，MUSA 兼具 AI 和图形处理能力，适合图形计算与 AI 场景结合的应用。
    *   **可编程性**：MUSA 支持丰富的编程模型，可以通过不同的 API 和框架来进行开发，适合需要自定义计算任务的用户。
    *   **AI 与图形渲染结合**：MUSA 特别适合在图形处理和 AI 推理需要同时进行的场景中发挥作用，比如在实时渲染和 AI 驱动的增强现实、虚拟现实（AR/VR）应用中。
*   **应用领域**：
    
    *   高性能图形渲染
    *   AI 推理加速
    *   混合场景应用，如游戏、虚拟现实、增强现实

### 三者的对比：

*   **XPU** 更倾向于异构计算，强调广泛的适用性，并结合了多种计算架构（CPU、GPU、FPGA 等）。
*   **MLU** 专注于 AI 计算任务，特别是AI模型的训练和推理，适用于大规模 AI 计算场景。
*   **MUSA** 则是一种结合了图形处理和 AI 计算的加速器，适合图形密集型任务和 AI 计算的混合场景，如游戏和虚拟现实中的 AI 增强效果。